{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80659e1e",
   "metadata": {},
   "source": [
    "\n",
    "# Data Mining for Space Habitats\n",
    "## *Analyzing ISS Environmental Telemetry for Safer Off-World Living*\n",
    "**Kenny Jenkins**  \n",
    "**Department of Computer Science**  \n",
    "**University of Colorado Boulder**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21834d1",
   "metadata": {},
   "source": [
    "### Section 1: Setup and Imports\n",
    "#### Purpose\n",
    "Initialize libraries, plotting, and configuration for a multi-mission, multi-modal pipeline that supports exploratory analysis, forecasting, anomaly detection, and biological linkage.\n",
    "#### What is implemented\n",
    "1. Core stack: NumPy, Pandas, SciPy, statsmodels, scikit-learn, TensorFlow/Keras.\n",
    "2. Visualization: Matplotlib, Seaborn, optional Plotly.\n",
    "3. Time-series utilities: STL, FFT, SARIMAX, VAR, Kalman Filter.\n",
    "4. Streamlit-ready configuration guard (runs cleanly in Jupyter or Streamlit).\n",
    "5. Central `Settings` dataclass (missions, paths, sampling, figure sizes, output dirs).\n",
    "6. Reproducibility: unified random seeds (Python/NumPy/TensorFlow) and TF deterministic ops.\n",
    "7. Clean logging and consistent Pandas/Matplotlib display options.\n",
    "#### Future Work\n",
    "- Integrate behavioral/event context (e.g., docking/EVA windows) into configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7203f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete.\n",
      "TensorFlow: 2.16.2\n",
      "Data roots: ['/Users/kennethjenkins/data/osdr_eda', '/Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/data/osdr_eda', '/Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/data']\n",
      "Omics roots: ['/Users/kennethjenkins/data/genelab', '/Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/data/genelab', '/Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/data/omics']\n",
      "Outputs root: /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 1: Setup & Imports ====\n",
    "\n",
    "import os, sys, random, warnings, io, json, time\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Optional visual QA\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Timeseries & stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Jupyter-friendly progress\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# ---- Streamlit guard (keep commented in Jupyter) ----\n",
    "# try:\n",
    "#     import streamlit as st\n",
    "#     if os.environ.get(\"STREAMLIT_SERVER_RUNNING\") == \"1\":\n",
    "#         st.set_page_config(page_title=\"ISS Environmental Telemetry & Omics Explorer\", layout=\"wide\")\n",
    "#         st.title(\"ISS Environmental Telemetry & Omics Explorer\")\n",
    "#         st.caption(\"Rodent Research: RR-1, RR-3, RR-6, RR-9, RR-12, RR-19 · CO₂, Temp, RH, Pressure, Radiation\")\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# ---- Global Settings object ----\n",
    "@dataclass\n",
    "class Settings:\n",
    "    missions: List[str] = field(default_factory=lambda: [\n",
    "        \"RR-1\",\"RR-3\",\"RR-4\",\"RR-5\",\"RR-6\",\"RR-8\",\"RR-9\",\"RR-12\",\"RR-17\",\"RR-19\"\n",
    "    ])\n",
    "    sampling: str = \"1min\"\n",
    "    seed: int = 42\n",
    "    fig_size: Tuple[int,int] = (12, 6)\n",
    "\n",
    "    # Data roots (prefer home-level; keep repo fallbacks)\n",
    "    data_roots: List[Path] = field(default_factory=lambda: [\n",
    "        Path(os.environ.get(\"SPACEHAB_DATA\", \"~/data/osdr_eda\")).expanduser(),\n",
    "        Path.cwd() / \"data\" / \"osdr_eda\",\n",
    "        Path.cwd() / \"data\"\n",
    "    ])\n",
    "    omics_roots: List[Path] = field(default_factory=lambda: [\n",
    "        Path(os.environ.get(\"SPACEHAB_OMICS\", \"~/data/genelab\")).expanduser(),\n",
    "        Path.cwd() / \"data\" / \"genelab\",\n",
    "        Path.cwd() / \"data\" / \"omics\"\n",
    "    ])\n",
    "\n",
    "    # Outputs live inside repo (avoid macOS protected folders)\n",
    "    outputs_root: Path = field(default_factory=lambda: Path.cwd() / \"outputs\")\n",
    "    preprocessed_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"preprocessed\")\n",
    "    pattern_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"pattern_analysis\")\n",
    "    relationships_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"relationships\")\n",
    "    anomalies_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"anomaly_forecast\")\n",
    "\n",
    "S = Settings()\n",
    "\n",
    "# Create output dirs\n",
    "for d in [S.outputs_root, S.preprocessed_dir, S.pattern_dir, S.pattern_dir / \"radiation\",\n",
    "          S.relationships_dir, S.anomalies_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(S.seed)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "random.seed(S.seed)\n",
    "np.random.seed(S.seed)\n",
    "tf.random.set_seed(S.seed)\n",
    "\n",
    "# ---- Plotting / Pandas display ----\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = S.fig_size\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "\n",
    "# ---- Clean warnings ----\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Data roots:\", [str(p) for p in S.data_roots])\n",
    "print(\"Omics roots:\", [str(p) for p in S.omics_roots])\n",
    "print(\"Outputs root:\", str(S.outputs_root))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f0419",
   "metadata": {},
   "source": [
    "### Section 2: Load and Preview ISS Environmental Telemetry\n",
    "#### Purpose\n",
    "Load Rodent Research telemetry/radiation datasets and GeneLab omics tables for later linkage; standardize and validate inputs for cross-mission analysis.\n",
    "#### What is implemented\n",
    "1. Flexible file discovery across home and repo roots (nested and flat layouts).\n",
    "2. Light schema checks and datetime parsing validation.\n",
    "3. Mission-level ingestion for summary, minute-level telemetry, and radiation logs.\n",
    "4. Omics ingestion (GLDS-98/99/104: differential expression, normalized counts, PCA/SampleTable).\n",
    "5. Availability matrix by mission (summary/telemetry/radiation) and unified `rr_data` handle.\n",
    "#### Future Work\n",
    "- Loader for event context (docking/EVA/crew) for downstream overlays.\n",
    "- Data availability matrix by mission and variable (dashboard display).\n",
    "- Placeholders/hooks for behavioral video and phenotypic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0061d7-1fa6-4719-af2a-6aa5acc41672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked RR-1\n",
      "Checked RR-3\n",
      "Checked RR-4\n",
      "Checked RR-5\n",
      "Checked RR-6\n",
      "Checked RR-8\n",
      "Checked RR-9\n",
      "Checked RR-12\n",
      "Checked RR-17\n",
      "Checked RR-19\n",
      "\n",
      "Dataset Availability by Mission:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RR_Mission</th>\n",
       "      <th>Summary Available</th>\n",
       "      <th>Telemetry Available</th>\n",
       "      <th>Radiation Available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RR-3</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RR-4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RR-5</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RR-6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RR-8</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RR-9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RR-12</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RR-17</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RR-19</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  RR_Mission  Summary Available  Telemetry Available  Radiation Available\n",
       "0       RR-1               True                 True                 True\n",
       "1       RR-3               True                 True                 True\n",
       "2       RR-4               True                False                 True\n",
       "3       RR-5               True                 True                False\n",
       "4       RR-6               True                 True                 True\n",
       "5       RR-8               True                False                 True\n",
       "6       RR-9               True                 True                 True\n",
       "7      RR-12               True                 True                 True\n",
       "8      RR-17               True                False                 True\n",
       "9      RR-19               True                 True                 True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemetry files loaded: 7\n",
      "Radiation files loaded: 9\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 2: Load & Preview ISS Environmental + Omics ====\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    display = print\n",
    "\n",
    "# --- Light validation ---\n",
    "def require_cols(df: pd.DataFrame, cols: List[str], name: str) -> None:\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"[SCHEMA] {name} missing columns: {miss}\")\n",
    "\n",
    "def assert_time_parse_ok(dt_series: pd.Series, name: str) -> None:\n",
    "    if dt_series.notna().sum() == 0:\n",
    "        raise ValueError(f\"[TIME] Failed to parse datetime for {name}\")\n",
    "\n",
    "# --- File helpers ---\n",
    "def find_file(rel_path: str, roots: List[Path]) -> Path | None:\n",
    "    for root in roots:\n",
    "        p = (root / rel_path).expanduser()\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def try_candidates(candidates: List[str], roots: List[Path]) -> Path | None:\n",
    "    for rel in candidates:\n",
    "        p = find_file(rel, roots)\n",
    "        if p:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def safe_read_csv(path: Path, **kwargs) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_csv(path, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(f\"[READ FAIL] {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Candidate layouts (nested + flat) ---\n",
    "def env_candidates(rr: str, kind: str) -> List[str]:\n",
    "    suffix = {\"summary\": \"_EDA_Summary_table.csv\",\n",
    "              \"telemetry\": \"_EDA_Telemetry_data.csv\",\n",
    "              \"radiation\": \"_EDA_Radiation_data.csv\"}[kind]\n",
    "    fname = f\"{rr}{suffix}\"\n",
    "    return [f\"EDA Rodent Research/{fname}\", fname]\n",
    "\n",
    "def omics_candidates(glds: str, kind: str) -> List[str]:\n",
    "    fname = {\n",
    "        \"diff\": f\"{glds}_rna_seq_differential_expression.csv\",\n",
    "        \"norm\": f\"{glds}_rna_seq_Normalized_Counts.csv\",\n",
    "        \"meta\": f\"{glds}_rna_seq_visualization_PCA_table.csv\" if glds == \"GLDS-104\"\n",
    "                else f\"{glds}_rna_seq_SampleTable.csv\",\n",
    "    }[kind]\n",
    "    return [f\"RR-1/{glds}/{fname}\", fname]\n",
    "\n",
    "telemetry_expected = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_expected = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "\n",
    "summary_data: Dict[str, pd.DataFrame] = {}\n",
    "telemetry_data: Dict[str, pd.DataFrame] = {}\n",
    "radiation_data: Dict[str, pd.DataFrame] = {}\n",
    "ok_summary, ok_telemetry, ok_radiation = set(), set(), set()\n",
    "\n",
    "# --- Load EDA per mission ---\n",
    "for rr in S.missions:\n",
    "    # summary\n",
    "    p = try_candidates(env_candidates(rr, \"summary\"), S.data_roots)\n",
    "    if p:\n",
    "        df = safe_read_csv(p)\n",
    "        if df is not None and not df.empty:\n",
    "            summary_data[rr] = df\n",
    "            ok_summary.add(rr)\n",
    "    else:\n",
    "        print(f\"[MISS] {rr} summary\")\n",
    "\n",
    "    # telemetry\n",
    "    p = try_candidates(env_candidates(rr, \"telemetry\"), S.data_roots)\n",
    "    if p:\n",
    "        df = safe_read_csv(p)\n",
    "        if df is not None and not df.empty:\n",
    "            if \"Controller_Time_GMT\" in df.columns:\n",
    "                dt = pd.to_datetime(df[\"Controller_Time_GMT\"], errors=\"coerce\", utc=True)\n",
    "                assert_time_parse_ok(dt, f\"{rr} telemetry time\")\n",
    "                df[\"Controller_Time_GMT\"] = dt\n",
    "            # soft schema check\n",
    "            missing = [c for c in telemetry_expected if c not in df.columns]\n",
    "            if missing:\n",
    "                print(f\"[WARN] {rr} telemetry missing: {missing}\")\n",
    "            telemetry_data[rr] = df\n",
    "            ok_telemetry.add(rr)\n",
    "    else:\n",
    "        print(f\"[MISS] {rr} telemetry\")\n",
    "\n",
    "    # radiation\n",
    "    p = try_candidates(env_candidates(rr, \"radiation\"), S.data_roots)\n",
    "    if p:\n",
    "        df = safe_read_csv(p)\n",
    "        if df is not None and not df.empty:\n",
    "            if \"Date\" in df.columns:\n",
    "                dt = pd.to_datetime(df[\"Date\"], errors=\"coerce\", utc=True)\n",
    "                assert_time_parse_ok(dt, f\"{rr} radiation time\")\n",
    "                df[\"Date\"] = dt\n",
    "            missing = [c for c in radiation_expected if c not in df.columns]\n",
    "            if missing:\n",
    "                print(f\"[WARN] {rr} radiation missing: {missing}\")\n",
    "            radiation_data[rr] = df\n",
    "            ok_radiation.add(rr)\n",
    "    else:\n",
    "        print(f\"[MISS] {rr} radiation\")\n",
    "\n",
    "    print(f\"Checked {rr}\")\n",
    "\n",
    "# --- Omics (RR-1-focused tables; extend later) ---\n",
    "omics_data: Dict[str, Dict[str, pd.DataFrame]] = {}\n",
    "for glds in [\"GLDS-98\", \"GLDS-99\", \"GLDS-104\"]:\n",
    "    omics_data[glds] = {}\n",
    "    for key in [\"diff\", \"norm\", \"meta\"]:\n",
    "        p = try_candidates(omics_candidates(glds, key), S.omics_roots)\n",
    "        if p:\n",
    "            omics_data[glds][key] = safe_read_csv(p)\n",
    "        else:\n",
    "            print(f\"[MISS] OMICS {glds}:{key}\")\n",
    "\n",
    "# --- Availability table ---\n",
    "availability_rows = []\n",
    "for rr in S.missions:\n",
    "    availability_rows.append({\n",
    "        \"RR_Mission\": rr,\n",
    "        \"Summary Available\": rr in ok_summary,\n",
    "        \"Telemetry Available\": rr in ok_telemetry,\n",
    "        \"Radiation Available\": rr in ok_radiation\n",
    "    })\n",
    "availability_df = pd.DataFrame(availability_rows)\n",
    "print(\"\\nDataset Availability by Mission:\")\n",
    "display(availability_df)\n",
    "\n",
    "# --- Unified handle for downstream sections ---\n",
    "rr_data: Dict[str, Dict[str, pd.DataFrame | None]] = {\n",
    "    rr: {\n",
    "        \"summary\": summary_data.get(rr),\n",
    "        \"telemetry\": telemetry_data.get(rr),\n",
    "        \"radiation\": radiation_data.get(rr),\n",
    "    }\n",
    "    for rr in S.missions\n",
    "}\n",
    "\n",
    "# quick counts\n",
    "print(\"Telemetry files loaded:\", len(telemetry_data))\n",
    "print(\"Radiation files loaded:\", len(radiation_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c9ed4",
   "metadata": {},
   "source": [
    "### Section 3: Data Preprocessing\n",
    "#### Purpose\n",
    "Create clean, aligned, feature-enriched one-minute time series for telemetry and radiation.\n",
    "#### What is implemented\n",
    "1. One-minute resampling with conservative interpolation of short gaps (≤5 min).\n",
    "2. Rolling features (mean/std at 5/30/180 min) per variable.\n",
    "3. Orbital day/night proxy from CO₂; crew-awake heuristic (UTC 06–22).\n",
    "4. Z-scores for each native telemetry/radiation column.\n",
    "5. Quality flags per column (orig/interp/missing) and CSV exports to `outputs/preprocessed/`.\n",
    "#### Future Work\n",
    "- Replace orbital proxy with explicit lighting if available from EDA.\n",
    "- Persist a unified mission time index to simplify joins with omics and events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d0c733-3fb3-4448-8d5f-3a13243a588b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Telemetry preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-1_cleaned_telemetry.csv\n",
      "[✓] Radiation preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-1_cleaned_radiation.csv\n",
      "[✓] Telemetry preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-3_cleaned_telemetry.csv\n",
      "[✓] Radiation preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-3_cleaned_radiation.csv\n",
      "[✓] Telemetry preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-6_cleaned_telemetry.csv\n",
      "[✓] Radiation preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-6_cleaned_radiation.csv\n",
      "[✓] Telemetry preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-9_cleaned_telemetry.csv\n",
      "[✓] Radiation preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-9_cleaned_radiation.csv\n",
      "[✓] Telemetry preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-12_cleaned_telemetry.csv\n",
      "[✓] Radiation preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-12_cleaned_radiation.csv\n",
      "[✓] Telemetry preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-19_cleaned_telemetry.csv\n",
      "[✓] Radiation preprocessed → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/preprocessed/RR-19_cleaned_radiation.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 3: Data Preprocessing ====\n",
    "\n",
    "def minute_index(df: pd.DataFrame, time_col: str) -> pd.DatetimeIndex:\n",
    "    idx = pd.to_datetime(df[time_col], errors=\"coerce\", utc=True).dropna()\n",
    "    return pd.date_range(idx.min().floor(\"T\"), idx.max().ceil(\"T\"), freq=S.sampling)\n",
    "\n",
    "def build_quality_flags(original: pd.Series, resampled: pd.Series, interp: pd.Series) -> pd.Series:\n",
    "    # original points that landed exactly on the minute bins\n",
    "    flag = pd.Series(index=resampled.index, data=\"missing\", dtype=\"object\")\n",
    "    # mark where we have a value after resample before interpolate\n",
    "    flag.loc[resampled.index[resampled.notna()]] = \"orig\"\n",
    "    # after interpolation, any previously missing-but-now-present becomes 'interp'\n",
    "    new_filled = interp.notna() & resampled.isna()\n",
    "    flag.loc[new_filled.index[new_filled]] = \"interp\"\n",
    "    return flag\n",
    "\n",
    "cleaned_telemetry, cleaned_radiation = {}, {}\n",
    "\n",
    "for rr in availability_df.query(\"`Telemetry Available` and `Radiation Available`\")[\"RR_Mission\"]:\n",
    "    tel = rr_data[rr][\"telemetry\"].copy()\n",
    "    rad = rr_data[rr][\"radiation\"].copy()\n",
    "\n",
    "    # --- TELEMETRY ---\n",
    "    require_cols(tel, [\"Controller_Time_GMT\"], f\"{rr} telemetry\")\n",
    "    tel = tel.set_index(pd.to_datetime(tel[\"Controller_Time_GMT\"], utc=True)).sort_index()\n",
    "    # keep numeric cols only\n",
    "    tel_num = tel.select_dtypes(include=[np.number])\n",
    "\n",
    "    # resample\n",
    "    tel_res = tel_num.resample(S.sampling).mean()\n",
    "    tel_pre = tel_res.copy()\n",
    "    # interpolate short gaps only (<=5 minutes)\n",
    "    tel_int = tel_res.interpolate(limit=5)\n",
    "\n",
    "    # rolling features\n",
    "    wins = [5, 30, 180]\n",
    "    for col in tel_num.columns:\n",
    "        for w in wins:\n",
    "            tel_int[f\"{col}_mean_{w}min\"] = tel_int[col].rolling(w, min_periods=1).mean()\n",
    "            tel_int[f\"{col}_std_{w}min\"]  = tel_int[col].rolling(w, min_periods=1).std()\n",
    "\n",
    "    # day/night proxy using CO2\n",
    "    if \"CO2_ppm_ISS\" in tel_int.columns:\n",
    "        thr = tel_int[\"CO2_ppm_ISS\"].median()\n",
    "        tel_int[\"Orbital_Day\"] = (tel_int[\"CO2_ppm_ISS\"] > thr).astype(int)\n",
    "\n",
    "    # crew awake heuristic (UTC 06–22)\n",
    "    tel_int[\"Hour\"] = tel_int.index.hour\n",
    "    tel_int[\"Crew_Awake\"] = ((tel_int[\"Hour\"] >= 6) & (tel_int[\"Hour\"] < 22)).astype(int)\n",
    "    tel_int.drop(columns=\"Hour\", inplace=True)\n",
    "\n",
    "    # z-scores for native telemetry columns\n",
    "    for col in tel_num.columns:\n",
    "        std = tel_int[col].std()\n",
    "        if pd.notna(std) and std > 0:\n",
    "            tel_int[f\"{col}_zscore\"] = (tel_int[col] - tel_int[col].mean()) / std\n",
    "\n",
    "    # quality flags per column (orig/interp/missing)\n",
    "    for col in tel_num.columns:\n",
    "        flags = build_quality_flags(\n",
    "            original=tel_num[col],\n",
    "            resampled=tel_pre[col],\n",
    "            interp=tel_int[col]\n",
    "        )\n",
    "        tel_int[f\"{col}_qflag\"] = flags.values\n",
    "\n",
    "    cleaned_telemetry[rr] = tel_int\n",
    "    tel_out = S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\"\n",
    "    tel_int.to_csv(tel_out)\n",
    "    print(f\"[✓] Telemetry preprocessed → {tel_out}\")\n",
    "\n",
    "    # --- RADIATION ---\n",
    "    require_cols(rad, [\"Date\"], f\"{rr} radiation\")\n",
    "    rad = rad.set_index(pd.to_datetime(rad[\"Date\"], utc=True)).sort_index()\n",
    "    rad_num = rad.select_dtypes(include=[np.number])\n",
    "\n",
    "    rad_res = rad_num.resample(S.sampling).ffill()\n",
    "    rad_pre = rad_res.copy()\n",
    "    rad_int = rad_res.interpolate(limit=5)\n",
    "\n",
    "    for col in rad_num.columns:\n",
    "        for w in wins:\n",
    "            rad_int[f\"{col}_mean_{w}min\"] = rad_int[col].rolling(w, min_periods=1).mean()\n",
    "            rad_int[f\"{col}_std_{w}min\"]  = rad_int[col].rolling(w, min_periods=1).std()\n",
    "\n",
    "    # z-scores\n",
    "    for col in rad_num.columns:\n",
    "        std = rad_int[col].std()\n",
    "        if pd.notna(std) and std > 0:\n",
    "            rad_int[f\"{col}_zscore\"] = (rad_int[col] - rad_int[col].mean()) / std\n",
    "\n",
    "    # quality flags\n",
    "    for col in rad_num.columns:\n",
    "        flags = build_quality_flags(\n",
    "            original=rad_num[col],\n",
    "            resampled=rad_pre[col],\n",
    "            interp=rad_int[col]\n",
    "        )\n",
    "        rad_int[f\"{col}_qflag\"] = flags.values\n",
    "\n",
    "    cleaned_radiation[rr] = rad_int\n",
    "    rad_out = S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\"\n",
    "    rad_int.to_csv(rad_out)\n",
    "    print(f\"[✓] Radiation preprocessed → {rad_out}\")\n",
    "\n",
    "preprocessed_telemetry = cleaned_telemetry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4653969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemetry files: 6\n",
      "Radiation files: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Telemetry files:\", len(list(S.preprocessed_dir.glob(\"*_cleaned_telemetry.csv\"))))\n",
    "print(\"Radiation files:\", len(list(S.preprocessed_dir.glob(\"*_cleaned_radiation.csv\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fa0d3",
   "metadata": {},
   "source": [
    "### Section 4: Pattern Extraction\n",
    "#### Purpose\n",
    "Characterize seasonal and frequency structure to establish baseline rhythms and mission-specific signatures at 1-minute resolution.\n",
    "#### What is implemented\n",
    "1. STL decomposition (robust) for telemetry and radiation; raw/STL plots.\n",
    "2. FFT spectrum with flat-signal safeguards; Welch PSD robustness check.\n",
    "3. Per-series diagnostics (range, N, variance) and saved STL seasonal indices.\n",
    "4. Cross-mission `seasonality_summary.csv` (seasonality strength, robust amplitude, dominant period).\n",
    "#### Future Work\n",
    "- Compare STL seasonality with SARIMAX seasonal terms for consistency.\n",
    "- Automated report that ranks variables by seasonal strength per mission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a394bb-729a-4e15-aa6c-5eb052139675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern extraction for missions: ['RR-1', 'RR-3', 'RR-6', 'RR-9', 'RR-12', 'RR-19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missions:   0%|                                           | 0/6 [00:00<?, ?it/s]\n",
      "RR-1 vars:   0%|                                          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "RR-1 vars:   0%|  | 0/7 [03:55<?, ?it/s, Temp_degC_ISS: ok · 60445 pts · 235.9s]\u001b[A\n",
      "RR-1 vars:  14%|▏| 1/7 [03:55<23:35, 235.94s/it, Temp_degC_ISS: ok · 60445 pts ·\u001b[A\n",
      "RR-1 vars:  14%|▏| 1/7 [08:05<23:35, 235.94s/it, RH_percent_ISS: ok · 60445 pts \u001b[A\n",
      "RR-1 vars:  29%|▎| 2/7 [08:05<20:14, 243.00s/it, RH_percent_ISS: ok · 60445 pts \u001b[A\n",
      "RR-1 vars:  29%|▎| 2/7 [11:54<20:14, 243.00s/it, CO2_ppm_ISS: ok · 60445 pts · 2\u001b[A\n",
      "RR-1 vars:  43%|▍| 3/7 [11:54<15:52, 238.08s/it, CO2_ppm_ISS: ok · 60445 pts · 2\u001b[A\n",
      "RR-1 vars:  43%|▍| 3/7 [15:21<15:52, 238.08s/it, GCR_Dose_mGy_d: ok · 54721 pts \u001b[A\n",
      "RR-1 vars:  57%|▌| 4/7 [15:21<11:29, 229.76s/it, GCR_Dose_mGy_d: ok · 54721 pts \u001b[A\n",
      "RR-1 vars:  57%|▌| 4/7 [18:48<11:29, 229.76s/it, SAA_Dose_mGy_d: ok · 54721 pts \u001b[A\n",
      "RR-1 vars:  71%|▋| 5/7 [18:48<07:29, 224.58s/it, SAA_Dose_mGy_d: ok · 54721 pts \u001b[A\n",
      "RR-1 vars:  71%|▋| 5/7 [22:16<07:29, 224.58s/it, Total_Dose_mGy_d: ok · 54721 pt\u001b[A\n",
      "RR-1 vars:  86%|▊| 6/7 [22:16<03:41, 221.47s/it, Total_Dose_mGy_d: ok · 54721 pt\u001b[A\n",
      "RR-1 vars:  86%|▊| 6/7 [25:43<03:41, 221.47s/it, Accumulated_Dose_mGy_d: ok · 54\u001b[A\n",
      "RR-1 vars: 100%|█| 7/7 [25:43<00:00, 219.15s/it, Accumulated_Dose_mGy_d: ok · 54\u001b[A\n",
      "Missions:  17%|███▌                 | 1/6 [25:43<2:08:38, 1543.74s/it, 7/7 vars]\u001b[A\n",
      "RR-3 vars:   0%|                                          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "RR-3 vars:   0%|  | 0/7 [04:36<?, ?it/s, Temp_degC_ISS: ok · 71968 pts · 276.8s]\u001b[A\n",
      "RR-3 vars:  14%|▏| 1/7 [04:36<27:41, 276.85s/it, Temp_degC_ISS: ok · 71968 pts ·\u001b[A\n",
      "RR-3 vars:  14%|▏| 1/7 [09:12<27:41, 276.85s/it, RH_percent_ISS: ok · 71968 pts \u001b[A\n",
      "RR-3 vars:  29%|▎| 2/7 [09:12<23:00, 276.02s/it, RH_percent_ISS: ok · 71968 pts \u001b[A\n",
      "RR-3 vars:  29%|▎| 2/7 [13:47<23:00, 276.02s/it, CO2_ppm_ISS: ok · 71968 pts · 2\u001b[A\n",
      "RR-3 vars:  43%|▍| 3/7 [13:47<18:22, 275.75s/it, CO2_ppm_ISS: ok · 71968 pts · 2\u001b[A\n",
      "RR-3 vars:  43%|▍| 3/7 [17:35<18:22, 275.75s/it, GCR_Dose_mGy_d: ok · 60481 pts \u001b[A\n",
      "RR-3 vars:  57%|▌| 4/7 [17:35<13:09, 263.01s/it, GCR_Dose_mGy_d: ok · 60481 pts \u001b[A\n",
      "RR-3 vars:  57%|▌| 4/7 [21:23<13:09, 263.01s/it, SAA_Dose_mGy_d: ok · 60481 pts \u001b[A\n",
      "RR-3 vars:  71%|▋| 5/7 [21:23<08:30, 255.26s/it, SAA_Dose_mGy_d: ok · 60481 pts \u001b[A\n",
      "RR-3 vars:  71%|▋| 5/7 [25:14<08:30, 255.26s/it, Total_Dose_mGy_d: ok · 60481 pt\u001b[A\n",
      "RR-3 vars:  86%|▊| 6/7 [25:14<04:10, 250.61s/it, Total_Dose_mGy_d: ok · 60481 pt\u001b[A\n",
      "RR-3 vars:  86%|▊| 6/7 [29:02<04:10, 250.61s/it, Accumulated_Dose_mGy_d: ok · 60\u001b[A\n",
      "RR-3 vars: 100%|█| 7/7 [29:02<00:00, 246.84s/it, Accumulated_Dose_mGy_d: ok · 60\u001b[A\n",
      "Missions:  33%|███████              | 2/6 [54:46<1:49:42, 1645.57s/it, 7/7 vars]\u001b[A\n",
      "RR-6 vars:   0%|                                          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "RR-6 vars:   0%|  | 0/7 [05:34<?, ?it/s, Temp_degC_ISS: ok · 87872 pts · 334.3s]\u001b[A\n",
      "RR-6 vars:  14%|▏| 1/7 [05:34<33:25, 334.27s/it, Temp_degC_ISS: ok · 87872 pts ·\u001b[A\n",
      "RR-6 vars:  14%|▏| 1/7 [11:06<33:25, 334.27s/it, RH_percent_ISS: ok · 87872 pts \u001b[A\n",
      "RR-6 vars:  29%|▎| 2/7 [11:06<27:46, 333.35s/it, RH_percent_ISS: ok · 87872 pts \u001b[A\n",
      "RR-6 vars:  29%|▎| 2/7 [16:38<27:46, 333.35s/it, CO2_ppm_ISS: ok · 87872 pts · 3\u001b[A\n",
      "RR-6 vars:  43%|▍| 3/7 [16:38<22:10, 332.74s/it, CO2_ppm_ISS: ok · 87872 pts · 3\u001b[A\n",
      "RR-6 vars:  43%|▍| 3/7 [20:59<22:10, 332.74s/it, GCR_Dose_mGy_d: ok · 69121 pts \u001b[A\n",
      "RR-6 vars:  57%|▌| 4/7 [20:59<15:40, 313.41s/it, GCR_Dose_mGy_d: ok · 69121 pts \u001b[A\n",
      "RR-6 vars:  57%|▌| 4/7 [25:21<15:40, 313.41s/it, SAA_Dose_mGy_d: ok · 69121 pts \u001b[A\n",
      "RR-6 vars:  71%|▋| 5/7 [25:21<10:04, 302.03s/it, SAA_Dose_mGy_d: ok · 69121 pts \u001b[A\n",
      "RR-6 vars:  71%|▋| 5/7 [29:42<10:04, 302.03s/it, Total_Dose_mGy_d: ok · 69121 pt\u001b[A\n",
      "RR-6 vars:  86%|▊| 6/7 [29:42<04:54, 294.36s/it, Total_Dose_mGy_d: ok · 69121 pt\u001b[A\n",
      "RR-6 vars:  86%|▊| 6/7 [34:04<04:54, 294.36s/it, Accumulated_Dose_mGy_d: ok · 69\u001b[A\n",
      "RR-6 vars: 100%|█| 7/7 [34:04<00:00, 288.96s/it, Accumulated_Dose_mGy_d: ok · 69\u001b[A\n",
      "Missions:  50%|█████████▌         | 3/6 [1:28:50<1:29:16, 1785.44s/it, 7/7 vars]\u001b[A\n",
      "RR-9 vars:   0%|                                          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "RR-9 vars:   0%|  | 0/7 [03:12<?, ?it/s, Temp_degC_ISS: ok · 50396 pts · 192.0s]\u001b[A\n",
      "RR-9 vars:  14%|▏| 1/7 [03:12<19:12, 192.02s/it, Temp_degC_ISS: ok · 50396 pts ·\u001b[A\n",
      "RR-9 vars:  14%|▏| 1/7 [06:22<19:12, 192.02s/it, RH_percent_ISS: ok · 50396 pts \u001b[A\n",
      "RR-9 vars:  29%|▎| 2/7 [06:22<15:56, 191.26s/it, RH_percent_ISS: ok · 50396 pts \u001b[A\n",
      "RR-9 vars:  29%|▎| 2/7 [09:35<15:56, 191.26s/it, CO2_ppm_ISS: ok · 50396 pts · 1\u001b[A\n",
      "RR-9 vars:  43%|▍| 3/7 [09:35<12:47, 191.97s/it, CO2_ppm_ISS: ok · 50396 pts · 1\u001b[A\n",
      "RR-9 vars:  43%|▍| 3/7 [12:24<12:47, 191.97s/it, GCR_Dose_mGy_d: ok · 44641 pts \u001b[A\n",
      "RR-9 vars:  57%|▌| 4/7 [12:24<09:16, 185.59s/it, GCR_Dose_mGy_d: ok · 44641 pts \u001b[A\n",
      "RR-9 vars:  57%|▌| 4/7 [15:12<09:16, 185.59s/it, SAA_Dose_mGy_d: ok · 44641 pts \u001b[A\n",
      "RR-9 vars:  71%|▋| 5/7 [15:12<06:03, 181.77s/it, SAA_Dose_mGy_d: ok · 44641 pts \u001b[A\n",
      "RR-9 vars:  71%|▋| 5/7 [18:02<06:03, 181.77s/it, Total_Dose_mGy_d: ok · 44641 pt\u001b[A\n",
      "RR-9 vars:  86%|▊| 6/7 [18:02<02:59, 179.60s/it, Total_Dose_mGy_d: ok · 44641 pt\u001b[A\n",
      "RR-9 vars:  86%|▊| 6/7 [20:52<02:59, 179.60s/it, Accumulated_Dose_mGy_d: ok · 44\u001b[A\n",
      "RR-9 vars: 100%|█| 7/7 [20:52<00:00, 177.95s/it, Accumulated_Dose_mGy_d: ok · 44\u001b[A\n",
      "Missions:  67%|██████████████       | 4/6 [1:49:42<54:43, 1641.75s/it, 7/7 vars]\u001b[A\n",
      "RR-12 vars:   0%|                                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "RR-12 vars:   0%| | 0/7 [03:45<?, ?it/s, Temp_degC_ISS: ok · 58940 pts · 225.4s]\u001b[A\n",
      "RR-12 vars:  14%|▏| 1/7 [03:45<22:32, 225.44s/it, Temp_degC_ISS: ok · 58940 pts \u001b[A\n",
      "RR-12 vars:  14%|▏| 1/7 [07:28<22:32, 225.44s/it, RH_percent_ISS: ok · 58940 pts\u001b[A\n",
      "RR-12 vars:  29%|▎| 2/7 [07:28<18:40, 224.00s/it, RH_percent_ISS: ok · 58940 pts\u001b[A\n",
      "RR-12 vars:  29%|▎| 2/7 [11:12<18:40, 224.00s/it, CO2_ppm_ISS: ok · 58940 pts · \u001b[A\n",
      "RR-12 vars:  43%|▍| 3/7 [11:12<14:56, 224.11s/it, CO2_ppm_ISS: ok · 58940 pts · \u001b[A\n",
      "RR-12 vars:  43%|▍| 3/7 [14:27<14:56, 224.11s/it, GCR_Dose_mGy_d: ok · 51841 pts\u001b[A\n",
      "RR-12 vars:  57%|▌| 4/7 [14:27<10:48, 216.32s/it, GCR_Dose_mGy_d: ok · 51841 pts\u001b[A\n",
      "RR-12 vars:  57%|▌| 4/7 [17:45<10:48, 216.32s/it, SAA_Dose_mGy_d: ok · 51841 pts\u001b[A\n",
      "RR-12 vars:  71%|▋| 5/7 [17:45<07:04, 212.25s/it, SAA_Dose_mGy_d: ok · 51841 pts\u001b[A\n",
      "RR-12 vars:  71%|▋| 5/7 [21:02<07:04, 212.25s/it, Total_Dose_mGy_d: ok · 51841 p\u001b[A\n",
      "RR-12 vars:  86%|▊| 6/7 [21:02<03:29, 209.30s/it, Total_Dose_mGy_d: ok · 51841 p\u001b[A\n",
      "RR-12 vars:  86%|▊| 6/7 [24:19<03:29, 209.30s/it, Accumulated_Dose_mGy_d: ok · 5\u001b[A\n",
      "RR-12 vars: 100%|█| 7/7 [24:19<00:00, 207.25s/it, Accumulated_Dose_mGy_d: ok · 5\u001b[A\n",
      "Missions:  83%|█████████████████▌   | 5/6 [2:14:02<26:41, 1601.38s/it, 7/7 vars]\u001b[A\n",
      "RR-19 vars:   0%|                                         | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      "RR-19 vars:   0%| | 0/7 [03:12<?, ?it/s, Temp_degC_ISS: ok · 50967 pts · 192.5s]\u001b[A\n",
      "RR-19 vars:  14%|▏| 1/7 [03:12<19:15, 192.51s/it, Temp_degC_ISS: ok · 50967 pts \u001b[A\n",
      "RR-19 vars:  14%|▏| 1/7 [06:25<19:15, 192.51s/it, RH_percent_ISS: ok · 50967 pts\u001b[A\n",
      "RR-19 vars:  29%|▎| 2/7 [06:25<16:03, 192.69s/it, RH_percent_ISS: ok · 50967 pts\u001b[A\n",
      "RR-19 vars:  29%|▎| 2/7 [09:40<16:03, 192.69s/it, CO2_ppm_ISS: ok · 50967 pts · \u001b[A\n",
      "RR-19 vars:  43%|▍| 3/7 [09:40<12:53, 193.38s/it, CO2_ppm_ISS: ok · 50967 pts · \u001b[A\n",
      "RR-19 vars:  43%|▍| 3/7 [12:44<12:53, 193.38s/it, GCR_Dose_mGy_d: ok · 48961 pts\u001b[A\n",
      "RR-19 vars:  57%|▌| 4/7 [12:44<09:32, 190.93s/it, GCR_Dose_mGy_d: ok · 48961 pts\u001b[A\n",
      "RR-19 vars:  57%|▌| 4/7 [15:48<09:32, 190.93s/it, SAA_Dose_mGy_d: ok · 48961 pts\u001b[A\n",
      "RR-19 vars:  71%|▋| 5/7 [15:48<06:19, 189.54s/it, SAA_Dose_mGy_d: ok · 48961 pts\u001b[A\n",
      "RR-19 vars:  71%|▋| 5/7 [18:55<06:19, 189.54s/it, Total_Dose_mGy_d: ok · 48961 p\u001b[A\n",
      "RR-19 vars:  86%|▊| 6/7 [18:55<03:08, 188.92s/it, Total_Dose_mGy_d: ok · 48961 p\u001b[A\n",
      "RR-19 vars:  86%|▊| 6/7 [21:59<03:08, 188.92s/it, Accumulated_Dose_mGy_d: ok · 4\u001b[A\n",
      "RR-19 vars: 100%|█| 7/7 [21:59<00:00, 188.21s/it, Accumulated_Dose_mGy_d: ok · 4\u001b[A\n",
      "Missions: 100%|█████████████████████| 6/6 [2:36:01<00:00, 1560.32s/it, 7/7 vars]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Wrote seasonality summary → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/seasonality_summary.csv\n",
      "[✓] Appended progress log → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/pattern_progress.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mission</th>\n",
       "      <th>Type</th>\n",
       "      <th>Variable</th>\n",
       "      <th>N</th>\n",
       "      <th>Std</th>\n",
       "      <th>Seasonality_Strength</th>\n",
       "      <th>Seasonal_Amplitude_P95_P05</th>\n",
       "      <th>Dominant_Freq_Hz</th>\n",
       "      <th>Dominant_Period_min</th>\n",
       "      <th>Seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RR-6</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Accumulated_Dose_mGy_d</td>\n",
       "      <td>69121</td>\n",
       "      <td>3.91</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>69121.00</td>\n",
       "      <td>261.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Accumulated_Dose_mGy_d</td>\n",
       "      <td>54721</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>54721.00</td>\n",
       "      <td>207.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RR-12</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Accumulated_Dose_mGy_d</td>\n",
       "      <td>51841</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>51841.00</td>\n",
       "      <td>196.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>RR-19</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Accumulated_Dose_mGy_d</td>\n",
       "      <td>48961</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>48961.00</td>\n",
       "      <td>184.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RR-3</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Accumulated_Dose_mGy_d</td>\n",
       "      <td>60481</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>60481.00</td>\n",
       "      <td>227.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RR-9</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Accumulated_Dose_mGy_d</td>\n",
       "      <td>44641</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>44641.00</td>\n",
       "      <td>169.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RR-3</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>Temp_degC_ISS</td>\n",
       "      <td>71968</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71968.00</td>\n",
       "      <td>276.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RR-19</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>CO2_ppm_ISS</td>\n",
       "      <td>50967</td>\n",
       "      <td>277.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>609.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1456.20</td>\n",
       "      <td>194.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RR-12</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>CO2_ppm_ISS</td>\n",
       "      <td>58940</td>\n",
       "      <td>434.31</td>\n",
       "      <td>0.54</td>\n",
       "      <td>631.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1437.56</td>\n",
       "      <td>224.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>RR-6</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>CO2_ppm_ISS</td>\n",
       "      <td>87872</td>\n",
       "      <td>817.94</td>\n",
       "      <td>0.51</td>\n",
       "      <td>596.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>87872.00</td>\n",
       "      <td>331.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RR-3</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>CO2_ppm_ISS</td>\n",
       "      <td>71968</td>\n",
       "      <td>1357.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>775.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71968.00</td>\n",
       "      <td>275.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>CO2_ppm_ISS</td>\n",
       "      <td>60445</td>\n",
       "      <td>561.97</td>\n",
       "      <td>0.31</td>\n",
       "      <td>436.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30222.50</td>\n",
       "      <td>228.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RR-6</td>\n",
       "      <td>radiation</td>\n",
       "      <td>GCR_Dose_mGy_d</td>\n",
       "      <td>69121</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>69121.00</td>\n",
       "      <td>261.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>RR-9</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>CO2_ppm_ISS</td>\n",
       "      <td>50396</td>\n",
       "      <td>590.93</td>\n",
       "      <td>0.26</td>\n",
       "      <td>356.93</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50396.00</td>\n",
       "      <td>193.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RR-19</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>Temp_degC_ISS</td>\n",
       "      <td>50967</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25483.50</td>\n",
       "      <td>192.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>GCR_Dose_mGy_d</td>\n",
       "      <td>54721</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18240.33</td>\n",
       "      <td>207.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>RR-6</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>Temp_degC_ISS</td>\n",
       "      <td>87872</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17574.40</td>\n",
       "      <td>334.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>RR-6</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>RH_percent_ISS</td>\n",
       "      <td>87872</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29290.67</td>\n",
       "      <td>332.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>RR-19</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>RH_percent_ISS</td>\n",
       "      <td>50967</td>\n",
       "      <td>3.55</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16989.00</td>\n",
       "      <td>192.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RR-12</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>RH_percent_ISS</td>\n",
       "      <td>58940</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11788.00</td>\n",
       "      <td>222.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Mission       Type                Variable      N     Std  Seasonality_Strength  Seasonal_Amplitude_P95_P05  Dominant_Freq_Hz  Dominant_Period_min  Seconds\n",
       "20    RR-6  radiation  Accumulated_Dose_mGy_d  69121    3.91                  1.00                        0.25              0.00             69121.00   261.82\n",
       "6     RR-1  radiation  Accumulated_Dose_mGy_d  54721    2.24                  1.00                        0.19              0.00             54721.00   207.47\n",
       "34   RR-12  radiation  Accumulated_Dose_mGy_d  51841    3.74                  1.00                        0.32              0.00             51841.00   196.96\n",
       "41   RR-19  radiation  Accumulated_Dose_mGy_d  48961    3.59                  1.00                        0.32              0.00             48961.00   184.60\n",
       "13    RR-3  radiation  Accumulated_Dose_mGy_d  60481    2.80                  1.00                        0.21              0.00             60481.00   227.87\n",
       "27    RR-9  radiation  Accumulated_Dose_mGy_d  44641    2.84                  0.97                        0.29              0.00             44641.00   169.60\n",
       "7     RR-3  telemetry           Temp_degC_ISS  71968    0.96                  0.66                        1.71              0.00             71968.00   276.85\n",
       "37   RR-19  telemetry             CO2_ppm_ISS  50967  277.67                  0.62                      609.03              0.00              1456.20   194.65\n",
       "30   RR-12  telemetry             CO2_ppm_ISS  58940  434.31                  0.54                      631.23              0.00              1437.56   224.30\n",
       "16    RR-6  telemetry             CO2_ppm_ISS  87872  817.94                  0.51                      596.05              0.00             87872.00   331.62\n",
       "9     RR-3  telemetry             CO2_ppm_ISS  71968 1357.71                  0.43                      775.61              0.00             71968.00   275.24\n",
       "2     RR-1  telemetry             CO2_ppm_ISS  60445  561.97                  0.31                      436.24              0.00             30222.50   228.96\n",
       "17    RR-6  radiation          GCR_Dose_mGy_d  69121    0.00                  0.29                        0.01              0.00             69121.00   261.00\n",
       "23    RR-9  telemetry             CO2_ppm_ISS  50396  590.93                  0.26                      356.93              0.00             50396.00   193.30\n",
       "35   RR-19  telemetry           Temp_degC_ISS  50967    0.80                  0.20                        0.58              0.00             25483.50   192.51\n",
       "3     RR-1  radiation          GCR_Dose_mGy_d  54721    0.00                  0.17                        0.00              0.00             18240.33   207.23\n",
       "14    RR-6  telemetry           Temp_degC_ISS  87872    0.47                  0.09                        0.35              0.00             17574.40   334.26\n",
       "15    RR-6  telemetry          RH_percent_ISS  87872    3.42                  0.07                        1.52              0.00             29290.67   332.47\n",
       "36   RR-19  telemetry          RH_percent_ISS  50967    3.55                  0.07                        1.27              0.00             16989.00   192.87\n",
       "29   RR-12  telemetry          RH_percent_ISS  58940    3.36                  0.06                        2.36              0.00             11788.00   222.64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Section 4: Pattern Extraction (detailed tqdm + progress log) ====\n",
    "\n",
    "import os, sys, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# --- CPU threading knobs (effective if set before NumPy load; harmless otherwise) ---\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", str(max(1, os.cpu_count() // 2)))\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", os.environ[\"OMP_NUM_THREADS\"])\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", os.environ[\"OMP_NUM_THREADS\"])\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", os.environ[\"OMP_NUM_THREADS\"])\n",
    "\n",
    "# --- Robust tqdm (no ipywidgets dependency) ---\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    def _tqdm(iterable=None, **kwargs):\n",
    "        # Text-mode bar with dynamic width and responsive refresh\n",
    "        return tqdm(iterable, dynamic_ncols=True, miniters=1, smoothing=0.05, **kwargs)\n",
    "    def _tqdm_write(msg):\n",
    "        print(msg, file=sys.stdout, flush=True)\n",
    "except Exception:\n",
    "    def _tqdm(it, **kwargs): return it\n",
    "    def _tqdm_write(msg): print(msg)\n",
    "\n",
    "# ---- Config ----\n",
    "telemetry_cols = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_cols = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "\n",
    "stl_period = 1440          # 24h @ 1-min sampling\n",
    "min_required_points = stl_period * 2\n",
    "fft_min_std = 1e-6\n",
    "save_seasonal_indices = True\n",
    "SKIP_IF_DONE = False       # set True to skip variables with all artifacts already written\n",
    "\n",
    "# paths\n",
    "(S.pattern_dir / \"radiation\").mkdir(parents=True, exist_ok=True)\n",
    "seasonal_dir = S.pattern_dir / \"seasonal_indices\"\n",
    "seasonal_dir.mkdir(parents=True, exist_ok=True)\n",
    "progress_csv = S.pattern_dir / \"pattern_progress.csv\"\n",
    "\n",
    "# --- IO helpers (prefer in-memory from Section 3; fallback to disk) ---\n",
    "def _load_preprocessed(rr: str, kind: str) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        if kind == \"telemetry\":\n",
    "            if 'preprocessed_telemetry' in globals() and rr in preprocessed_telemetry:\n",
    "                return preprocessed_telemetry[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\"\n",
    "        else:\n",
    "            if 'cleaned_radiation' in globals() and rr in cleaned_radiation:\n",
    "                return cleaned_radiation[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\"\n",
    "        if p.exists():\n",
    "            df = pd.read_csv(p, index_col=0, parse_dates=True).sort_index()\n",
    "            return df\n",
    "    except Exception as e:\n",
    "        _tqdm_write(f\"[LOAD FAIL] {rr} {kind}: {e}\")\n",
    "    return None\n",
    "\n",
    "# --- Metrics helpers ---\n",
    "def _seasonality_strength(seasonal: pd.Series, resid: pd.Series) -> float:\n",
    "    x = pd.Series(seasonal, dtype=float)\n",
    "    r = pd.Series(resid, dtype=float)\n",
    "    denom = np.nanvar(x + r)\n",
    "    if not np.isfinite(denom) or denom <= 0: return 0.0\n",
    "    val = 1.0 - (np.nanvar(r) / denom)\n",
    "    return float(np.clip(val, 0.0, 1.0))\n",
    "\n",
    "def _dominant_period_minutes_fft(y: np.ndarray, fs_hz: float) -> tuple[float, float]:\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    y = y - np.nanmean(y)\n",
    "    if np.nanstd(y) < fft_min_std: return (0.0, np.inf)\n",
    "    N = len(y)\n",
    "    freqs = np.fft.rfftfreq(N, d=1.0/fs_hz)  # Hz\n",
    "    spec = np.abs(np.fft.rfft(y))\n",
    "    if len(freqs) < 2: return (0.0, np.inf)\n",
    "    spec[0] = 0.0\n",
    "    k = int(np.nanargmax(spec))\n",
    "    f = float(freqs[k])\n",
    "    if f <= 0: return (0.0, np.inf)\n",
    "    period_min = (1.0 / f) / 60.0\n",
    "    return (f, period_min)\n",
    "\n",
    "def _safe_fft_plot(ts: pd.Series, title: str, out_path: Path, fs_hz: float):\n",
    "    y = ts.to_numpy(dtype=float)\n",
    "    y = y - np.nanmean(y)\n",
    "    if np.nanstd(y) < fft_min_std:\n",
    "        _tqdm_write(f\"[SKIP FFT] {title} too flat.\")\n",
    "        return\n",
    "    N = len(y)\n",
    "    freqs = np.fft.rfftfreq(N, d=1.0/fs_hz)\n",
    "    spec = np.abs(np.fft.rfft(y))\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(freqs, spec, color=\"black\")\n",
    "    plt.title(f\"{title} — Frequency Spectrum (rFFT)\")\n",
    "    plt.xlabel(\"Frequency (Hz)\"); plt.ylabel(\"Amplitude\")\n",
    "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
    "\n",
    "def _welch_plot(ts: pd.Series, title: str, out_path: Path, fs_hz: float):\n",
    "    y = ts.to_numpy(dtype=float)\n",
    "    y = y - np.nanmean(y)\n",
    "    if np.nanstd(y) < fft_min_std:\n",
    "        _tqdm_write(f\"[SKIP WELCH] {title} too flat.\")\n",
    "        return\n",
    "    nperseg = min(len(y), 4096)\n",
    "    if nperseg < 256:\n",
    "        _tqdm_write(f\"[SKIP WELCH] {title} too short (n={len(y)}).\")\n",
    "        return\n",
    "    f, Pxx = welch(y, fs=fs_hz, nperseg=nperseg)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.semilogy(f, Pxx, color=\"black\")\n",
    "    plt.title(f\"{title} — Welch PSD\")\n",
    "    plt.xlabel(\"Frequency (Hz)\"); plt.ylabel(\"Power Spectral Density\")\n",
    "    plt.tight_layout(); plt.savefig(out_path); plt.close()\n",
    "\n",
    "def _stl_decompose(ts: pd.Series, period: int):\n",
    "    return STL(ts, period=period, robust=True).fit()\n",
    "\n",
    "# --- Checkpointing (optional) ---\n",
    "def _artifacts_exist(rr: str, kind: str, col: str) -> bool:\n",
    "    if kind == \"telemetry\":\n",
    "        base = S.pattern_dir\n",
    "    else:\n",
    "        base = S.pattern_dir / \"radiation\"\n",
    "    need = [\n",
    "        base / f\"{col}_{rr}_Raw.png\",\n",
    "        base / f\"{col}_{rr}_STL.png\",\n",
    "        base / f\"{col}_{rr}_FFT.png\",\n",
    "        base / f\"{col}_{rr}_WELCH.png\",\n",
    "    ]\n",
    "    if save_seasonal_indices:\n",
    "        need.append(seasonal_dir / f\"{rr}_{col}_seasonal.csv\")\n",
    "    return all(p.exists() for p in need)\n",
    "\n",
    "# ----- Run analysis -----\n",
    "summary_rows = []\n",
    "fs_hz = 1.0 / 60.0  # one sample every 60 seconds\n",
    "\n",
    "missions_to_run = []\n",
    "for rr in S.missions:\n",
    "    tel_ok = (S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\").exists() or \\\n",
    "             ('preprocessed_telemetry' in globals() and rr in preprocessed_telemetry)\n",
    "    rad_ok = (S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\").exists() or \\\n",
    "             ('cleaned_radiation' in globals() and rr in cleaned_radiation)\n",
    "    if tel_ok or rad_ok:\n",
    "        missions_to_run.append(rr)\n",
    "\n",
    "_tqdm_write(f\"Pattern extraction for missions: {missions_to_run}\")\n",
    "\n",
    "mission_bar = _tqdm(missions_to_run, desc=\"Missions\", total=len(missions_to_run))\n",
    "for rr in mission_bar:\n",
    "    # prepare lists of variables present\n",
    "    df_tel = _load_preprocessed(rr, \"telemetry\")\n",
    "    df_rad = _load_preprocessed(rr, \"radiation\")\n",
    "    tel_vars = [c for c in telemetry_cols if (df_tel is not None and c in df_tel.columns)]\n",
    "    rad_vars = [c for c in radiation_cols  if (df_rad is not None and c in df_rad.columns)]\n",
    "    total_vars = len(tel_vars) + len(rad_vars)\n",
    "\n",
    "    var_bar = _tqdm(range(total_vars), desc=f\"{rr} vars\", total=total_vars, position=1, leave=False)\n",
    "    processed = 0\n",
    "\n",
    "    # ---- Telemetry ----\n",
    "    if df_tel is not None and len(df_tel) >= min_required_points:\n",
    "        for col in tel_vars:\n",
    "            t0 = time.time()\n",
    "            if SKIP_IF_DONE and _artifacts_exist(rr, \"telemetry\", col):\n",
    "                var_bar.set_postfix_str(f\"{col}: skip (exists)\")\n",
    "                var_bar.update(1); processed += 1\n",
    "                continue\n",
    "            try:\n",
    "                ts = df_tel[col].dropna().resample(\"1min\").mean().interpolate(limit=5)\n",
    "                ts = ts[np.isfinite(ts)]\n",
    "                if len(ts) < min_required_points or ts.std() < fft_min_std:\n",
    "                    var_bar.set_postfix_str(f\"{col}: flat/short\")\n",
    "                    var_bar.update(1); processed += 1\n",
    "                    continue\n",
    "\n",
    "                # Raw\n",
    "                plt.figure(figsize=(12,4))\n",
    "                plt.plot(ts.index, ts.values, color=\"black\")\n",
    "                plt.title(f\"{col} Raw Signal ({rr})\")\n",
    "                plt.xlabel(\"Time\"); plt.ylabel(col)\n",
    "                plt.tight_layout(); plt.savefig(S.pattern_dir / f\"{col}_{rr}_Raw.png\"); plt.close()\n",
    "\n",
    "                # STL\n",
    "                res = _stl_decompose(ts, period=stl_period)\n",
    "                fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
    "                axes[0].plot(ts.index, res.observed.values, color='black'); axes[0].set_title(f\"{col} — Original ({rr})\")\n",
    "                axes[1].plot(res.trend.index, res.trend.values,   color='blue');  axes[1].set_title(\"Trend\")\n",
    "                axes[2].plot(res.seasonal.index, res.seasonal.values, color='green'); axes[2].set_title(\"Seasonal\")\n",
    "                axes[3].plot(res.resid.index, res.resid.values,   color='red');   axes[3].set_title(\"Residual\")\n",
    "                plt.tight_layout(); plt.savefig(S.pattern_dir / f\"{col}_{rr}_STL.png\"); plt.close()\n",
    "\n",
    "                # Save seasonal index\n",
    "                if save_seasonal_indices:\n",
    "                    pd.DataFrame({\"timestamp\": res.seasonal.index, \"seasonal\": res.seasonal.values}) \\\n",
    "                      .to_csv(seasonal_dir / f\"{rr}_{col}_seasonal.csv\", index=False)\n",
    "\n",
    "                # FFT + Welch\n",
    "                _safe_fft_plot(ts, f\"{col} ({rr})\", S.pattern_dir / f\"{col}_{rr}_FFT.png\", fs_hz)\n",
    "                _welch_plot(ts, f\"{col} ({rr})\", S.pattern_dir / f\"{col}_{rr}_WELCH.png\", fs_hz)\n",
    "                dom_f_hz, dom_period_min = _dominant_period_minutes_fft(ts.values, fs_hz)\n",
    "\n",
    "                strength = _seasonality_strength(res.seasonal, res.resid)\n",
    "                amp_robust = float(np.nanpercentile(res.seasonal, 95) - np.nanpercentile(res.seasonal, 5))\n",
    "                summary_rows.append({\n",
    "                    \"Mission\": rr, \"Type\": \"telemetry\", \"Variable\": col,\n",
    "                    \"N\": len(ts), \"Std\": float(ts.std()),\n",
    "                    \"Seasonality_Strength\": strength,\n",
    "                    \"Seasonal_Amplitude_P95_P05\": amp_robust,\n",
    "                    \"Dominant_Freq_Hz\": dom_f_hz,\n",
    "                    \"Dominant_Period_min\": dom_period_min,\n",
    "                    \"Seconds\": round(time.time() - t0, 2)\n",
    "                })\n",
    "\n",
    "                var_bar.set_postfix_str(f\"{col}: ok · {len(ts)} pts · {time.time()-t0:.1f}s\")\n",
    "            except Exception as e:\n",
    "                var_bar.set_postfix_str(f\"{col}: ERROR\")\n",
    "                _tqdm_write(f\"[ERROR] {rr} {col}: {e}\")\n",
    "            finally:\n",
    "                var_bar.update(1); processed += 1\n",
    "\n",
    "    # ---- Radiation ----\n",
    "    if df_rad is not None and len(df_rad) >= min_required_points:\n",
    "        for col in rad_vars:\n",
    "            t0 = time.time()\n",
    "            if SKIP_IF_DONE and _artifacts_exist(rr, \"radiation\", col):\n",
    "                var_bar.set_postfix_str(f\"{col}: skip (exists)\")\n",
    "                var_bar.update(1); processed += 1\n",
    "                continue\n",
    "            try:\n",
    "                ts = df_rad[col].dropna().resample(\"1min\").interpolate(limit=5)\n",
    "                ts = ts[np.isfinite(ts)]\n",
    "                if len(ts) < min_required_points or ts.std() < fft_min_std:\n",
    "                    var_bar.set_postfix_str(f\"{col}: flat/short\")\n",
    "                    var_bar.update(1); processed += 1\n",
    "                    continue\n",
    "\n",
    "                # Raw\n",
    "                plt.figure(figsize=(12,4))\n",
    "                plt.plot(ts.index, ts.values, color=\"black\")\n",
    "                plt.title(f\"{col} Raw Signal ({rr})\")\n",
    "                plt.xlabel(\"Time\"); plt.ylabel(col)\n",
    "                plt.tight_layout(); plt.savefig(S.pattern_dir / \"radiation\" / f\"{col}_{rr}_Raw.png\"); plt.close()\n",
    "\n",
    "                # STL\n",
    "                res = _stl_decompose(ts, period=stl_period)\n",
    "                fig, axes = plt.subplots(4, 1, figsize=(14, 10), sharex=True)\n",
    "                axes[0].plot(ts.index, res.observed.values, color='black'); axes[0].set_title(f\"{col} — Original ({rr})\")\n",
    "                axes[1].plot(res.trend.index, res.trend.values,   color='blue');  axes[1].set_title(\"Trend\")\n",
    "                axes[2].plot(res.seasonal.index, res.seasonal.values, color='green'); axes[2].set_title(\"Seasonal\")\n",
    "                axes[3].plot(res.resid.index, res.resid.values,   color='red');   axes[3].set_title(\"Residual\")\n",
    "                plt.tight_layout(); plt.savefig(S.pattern_dir / \"radiation\" / f\"{col}_{rr}_STL.png\"); plt.close()\n",
    "\n",
    "                if save_seasonal_indices:\n",
    "                    pd.DataFrame({\"timestamp\": res.seasonal.index, \"seasonal\": res.seasonal.values}) \\\n",
    "                      .to_csv(seasonal_dir / f\"{rr}_{col}_seasonal.csv\", index=False)\n",
    "\n",
    "                _safe_fft_plot(ts, f\"{col} ({rr})\", S.pattern_dir / \"radiation\" / f\"{col}_{rr}_FFT.png\", fs_hz)\n",
    "                _welch_plot(ts, f\"{col} ({rr})\", S.pattern_dir / \"radiation\" / f\"{col}_{rr}_WELCH.png\", fs_hz)\n",
    "                dom_f_hz, dom_period_min = _dominant_period_minutes_fft(ts.values, fs_hz)\n",
    "\n",
    "                strength = _seasonality_strength(res.seasonal, res.resid)\n",
    "                amp_robust = float(np.nanpercentile(res.seasonal, 95) - np.nanpercentile(res.seasonal, 5))\n",
    "                summary_rows.append({\n",
    "                    \"Mission\": rr, \"Type\": \"radiation\", \"Variable\": col,\n",
    "                    \"N\": len(ts), \"Std\": float(ts.std()),\n",
    "                    \"Seasonality_Strength\": strength,\n",
    "                    \"Seasonal_Amplitude_P95_P05\": amp_robust,\n",
    "                    \"Dominant_Freq_Hz\": dom_f_hz,\n",
    "                    \"Dominant_Period_min\": dom_period_min,\n",
    "                    \"Seconds\": round(time.time() - t0, 2)\n",
    "                })\n",
    "\n",
    "                var_bar.set_postfix_str(f\"{col}: ok · {len(ts)} pts · {time.time()-t0:.1f}s\")\n",
    "            except Exception as e:\n",
    "                var_bar.set_postfix_str(f\"{col}: ERROR\")\n",
    "                _tqdm_write(f\"[ERROR] {rr} {col}: {e}\")\n",
    "            finally:\n",
    "                var_bar.update(1); processed += 1\n",
    "\n",
    "    var_bar.close()\n",
    "    mission_bar.set_postfix_str(f\"{processed}/{total_vars} vars\")\n",
    "\n",
    "# ---- Save cross-mission summary + append progress log ----\n",
    "seasonality_summary = pd.DataFrame(summary_rows)\n",
    "out_csv = S.pattern_dir / \"seasonality_summary.csv\"\n",
    "seasonality_summary.to_csv(out_csv, index=False)\n",
    "_tqdm_write(f\"[✓] Wrote seasonality summary → {out_csv}\")\n",
    "\n",
    "# keep a lightweight cumulative progress log\n",
    "try:\n",
    "    mode = \"a\" if progress_csv.exists() else \"w\"\n",
    "    header = not progress_csv.exists()\n",
    "    seasonality_summary.assign(ts=pd.Timestamp.utcnow()).to_csv(progress_csv, index=False, mode=mode, header=header)\n",
    "    _tqdm_write(f\"[✓] Appended progress log → {progress_csv}\")\n",
    "except Exception as e:\n",
    "    _tqdm_write(f\"[LOG WARN] Could not append progress log: {e}\")\n",
    "\n",
    "# Optional: quick peek at strongest seasonality\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(seasonality_summary.sort_values(\n",
    "        [\"Seasonality_Strength\", \"Seasonal_Amplitude_P95_P05\"], ascending=[False, False]\n",
    "    ).head(20))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c99a7",
   "metadata": {},
   "source": [
    "### Section 5: Relationship Mapping\n",
    "#### Purpose\n",
    "Quantify associations and temporal dependencies across telemetry and radiation; prepare for biological/behavioral linkage.\n",
    "#### What is implemented\n",
    "1. Pearson and Spearman correlation matrices with heatmap exports.\n",
    "2. Conditional Granger causality via VAR: AIC-based lag selection, optional de-seasonalization using Section 4 indices, stationarity checks (ADF/diff), and BH FDR control with directed heatmaps and tidy CSV.\n",
    "3. Correlation network graphs for strong edges (thresholded).\n",
    "#### Future Work\n",
    "- Partial correlations controlling for time of day and orbital phase.\n",
    "- Non-linear directionality (transfer entropy or directed information).\n",
    "- Extend analyses to additional tissues (skin) and behavioral/phenotypic signals when available.\n",
    "- Formal interpretability (e.g., SHAP) once multivariate models are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1843ef6b-91b3-44fe-9998-41f927ceb7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship mapping for missions: ['RR-1', 'RR-3', 'RR-6', 'RR-9', 'RR-12', 'RR-19']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missions:  50%|█████████████████▌                 | 3/6 [00:18<00:18,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SKIP] RR-9: insufficient rows (0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missions: 100%|███████████████████████████████████| 6/6 [00:27<00:00,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Wrote VAR-Granger results → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/relationships/granger_var_results.csv (rows=210)\n",
      "[✓] Wrote correlations tidy → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/relationships/correlations_tidy.csv (rows=315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 5: Relationship Mapping (VAR Granger + Partial Corr + optional MI) ====\n",
    "import os, warnings, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# tqdm fallback\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "    def tprint(*a, **k): tqdm.write(*a, **k)\n",
    "except Exception:\n",
    "    def tqdm(x, **k): return x\n",
    "    def tprint(*a, **k): print(*a, **k)\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import networkx as nx\n",
    "    HAS_NX = True\n",
    "except Exception:\n",
    "    HAS_NX = False\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    HAS_MI = True\n",
    "except Exception:\n",
    "    HAS_MI = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---- Config ----\n",
    "telemetry_metrics = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_metrics  = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "\n",
    "resample_freq = \"5min\"\n",
    "maxlags_minutes = 60\n",
    "min_rows = 1500\n",
    "pearson_abs_threshold = 0.6\n",
    "alpha_fdr = 0.05\n",
    "RUN_NONLINEAR = False  # set True to scan lagged MI (nonlinear lead–lag)\n",
    "\n",
    "REL_DIR = S.relationships_dir\n",
    "PLOTS_DIR = REL_DIR / \"plots\"\n",
    "REL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seasonal_dir = S.pattern_dir / \"seasonal_indices\"  # from Section 4\n",
    "\n",
    "# ---- Helpers ----\n",
    "def _load_preprocessed(rr: str, kind: str) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        if kind == \"telemetry\":\n",
    "            if \"preprocessed_telemetry\" in globals() and rr in preprocessed_telemetry:\n",
    "                return preprocessed_telemetry[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\"\n",
    "        else:\n",
    "            if \"cleaned_radiation\" in globals() and rr in cleaned_radiation:\n",
    "                return cleaned_radiation[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\"\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p, index_col=0, parse_dates=True).sort_index()\n",
    "    except Exception as e:\n",
    "        tprint(f\"[LOAD FAIL] {rr} {kind}: {e}\")\n",
    "    return None\n",
    "\n",
    "def _missions_with_both() -> list[str]:\n",
    "    ms = []\n",
    "    for rr in S.missions:\n",
    "        tel_ok = (S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\").exists() or \\\n",
    "                 (\"preprocessed_telemetry\" in globals() and rr in preprocessed_telemetry)\n",
    "        rad_ok = (S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\").exists() or \\\n",
    "                 (\"cleaned_radiation\" in globals() and rr in cleaned_radiation)\n",
    "        if tel_ok and rad_ok:\n",
    "            ms.append(rr)\n",
    "    return ms\n",
    "\n",
    "def _maybe_deseason(rr: str, col: str, ts: pd.Series) -> pd.Series:\n",
    "    f = seasonal_dir / f\"{rr}_{col}_seasonal.csv\"\n",
    "    x = ts.resample(resample_freq).mean().interpolate(limit=5)\n",
    "    if f.exists():\n",
    "        try:\n",
    "            s = pd.read_csv(f, parse_dates=[\"timestamp\"]).set_index(\"timestamp\")[\"seasonal\"]\n",
    "            s = s.resample(resample_freq).mean().interpolate(limit=5)\n",
    "            return (x.reindex(s.index) - s).dropna()\n",
    "        except Exception:\n",
    "            return x.dropna()\n",
    "    return x.dropna()\n",
    "\n",
    "def _adf_stationarize(s: pd.Series) -> pd.Series:\n",
    "    s = s.dropna()\n",
    "    if len(s) < 200: return s\n",
    "    try:\n",
    "        p = adfuller(s, autolag=\"AIC\")[1]\n",
    "        return s if p <= 0.05 else s.diff().dropna()\n",
    "    except Exception:\n",
    "        return s\n",
    "\n",
    "def _bh_fdr(pvals: pd.Series, alpha=0.05) -> pd.Series:\n",
    "    pv = pvals.astype(float).copy()\n",
    "    m = pv.notna().sum()\n",
    "    if m == 0: return pv\n",
    "    order = pv.sort_values().index\n",
    "    ranks = pd.Series(range(1, m+1), index=order, dtype=float)\n",
    "    q = pv.copy()\n",
    "    prev = 1.0\n",
    "    for idx in reversed(order):\n",
    "        rank = ranks[idx]\n",
    "        q[idx] = min(prev, pv[idx] * m / rank)\n",
    "        prev = q[idx]\n",
    "    return q\n",
    "\n",
    "def _build_covariates(idx: pd.DatetimeIndex, tel_df: pd.DataFrame | None) -> pd.DataFrame:\n",
    "    h = idx.hour + idx.minute/60.0\n",
    "    rad = 2*np.pi*h/24.0\n",
    "    cov = pd.DataFrame({\n",
    "        \"const\": 1.0,\n",
    "        \"tod_sin\": np.sin(rad),\n",
    "        \"tod_cos\": np.cos(rad)\n",
    "    }, index=idx)\n",
    "    # add mission-derived proxies if present\n",
    "    if tel_df is not None:\n",
    "        for c in [\"Orbital_Day\", \"Crew_Awake\"]:\n",
    "            if c in tel_df.columns:\n",
    "                z = tel_df[c].resample(resample_freq).median()\n",
    "                cov[c] = z.reindex(idx).interpolate(limit=5)\n",
    "    return cov.dropna()\n",
    "\n",
    "def _residualize_matrix(df: pd.DataFrame, cov: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = cov.values\n",
    "    XtX_inv = None\n",
    "    try:\n",
    "        XtX_inv = np.linalg.inv(X.T @ X)\n",
    "    except Exception:\n",
    "        XtX_inv = np.linalg.pinv(X.T @ X)\n",
    "    beta = XtX_inv @ X.T @ df.values\n",
    "    resid = df.values - X @ beta\n",
    "    return pd.DataFrame(resid, index=df.index, columns=df.columns)\n",
    "\n",
    "def _save_heatmap(mat: pd.DataFrame, title: str, out_png: Path, annot=False):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(mat, cmap=\"coolwarm\", annot=annot, fmt=\".2f\", vmin=-1, vmax=1)\n",
    "    plt.title(title); plt.tight_layout(); plt.savefig(out_png); plt.close()\n",
    "\n",
    "def _network_from_corr(corr: pd.DataFrame, out_png: Path, thr=0.6):\n",
    "    if not HAS_NX: return\n",
    "    G = nx.Graph()\n",
    "    cols = list(corr.columns)\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            w = float(corr.iloc[i,j])\n",
    "            if abs(w) >= thr:\n",
    "                G.add_edge(cols[i], cols[j], weight=round(w,2))\n",
    "    if G.number_of_edges() == 0: return\n",
    "    plt.figure(figsize=(8,6))\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw(G, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"gray\",\n",
    "            node_size=2000, font_size=9, width=1.5)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=nx.get_edge_attributes(G, \"weight\"))\n",
    "    plt.title(out_png.stem); plt.tight_layout(); plt.savefig(out_png); plt.close()\n",
    "\n",
    "# ---- Main ----\n",
    "missions = _missions_with_both()\n",
    "tprint(f\"Relationship mapping for missions: {missions}\")\n",
    "\n",
    "all_granger = []\n",
    "all_corr_tidy = []\n",
    "all_mi = []\n",
    "\n",
    "for rr in tqdm(missions, desc=\"Missions\"):\n",
    "    tel = _load_preprocessed(rr, \"telemetry\")\n",
    "    rad = _load_preprocessed(rr, \"radiation\")\n",
    "    if tel is None or rad is None:\n",
    "        tprint(f\"[SKIP] {rr}: missing data\")\n",
    "        continue\n",
    "\n",
    "    cols = [c for c in telemetry_metrics if c in tel.columns] + \\\n",
    "           [c for c in radiation_metrics if c in rad.columns]\n",
    "    if len(cols) < 3:\n",
    "        tprint(f\"[SKIP] {rr}: too few variables\")\n",
    "        continue\n",
    "\n",
    "    df = tel[telemetry_metrics].join(rad[radiation_metrics], how=\"inner\")\n",
    "    df = df[[c for c in cols]]\n",
    "    df = df.resample(resample_freq).mean().interpolate(limit=5).dropna()\n",
    "    if len(df) < min_rows:\n",
    "        tprint(f\"[SKIP] {rr}: insufficient rows ({len(df)})\")\n",
    "        continue\n",
    "\n",
    "    # deseason + stationarity\n",
    "    for c in list(df.columns):\n",
    "        df[c] = _maybe_deseason(rr, c, df[c])\n",
    "        df[c] = _adf_stationarize(df[c])\n",
    "    df = df.dropna()\n",
    "    if len(df) < min_rows:\n",
    "        tprint(f\"[SKIP] {rr}: insufficient rows after transforms ({len(df)})\")\n",
    "        continue\n",
    "\n",
    "    # correlations\n",
    "    pearson_corr = df.corr(method=\"pearson\")\n",
    "    scorr, _ = spearmanr(df.values)\n",
    "    spearman_corr = pd.DataFrame(scorr, index=df.columns, columns=df.columns)\n",
    "\n",
    "    # partial correlations (control for time-of-day + proxies)\n",
    "    cov = _build_covariates(df.index, tel)\n",
    "    df_aligned = df.reindex(cov.index).dropna()\n",
    "    cov = cov.reindex(df_aligned.index).dropna()\n",
    "    df_aligned = df_aligned.loc[cov.index]\n",
    "    resid = _residualize_matrix(df_aligned, cov)\n",
    "    partial_corr = resid.corr(method=\"pearson\")\n",
    "\n",
    "    # save corr outputs\n",
    "    pearson_corr.to_csv(REL_DIR / f\"{rr}_pearson_corr.csv\")\n",
    "    spearman_corr.to_csv(REL_DIR / f\"{rr}_spearman_corr.csv\")\n",
    "    partial_corr.to_csv(REL_DIR / f\"{rr}_partial_corr.csv\")\n",
    "    _save_heatmap(pearson_corr, f\"Pearson — {rr}\", PLOTS_DIR / f\"{rr}_pearson.png\")\n",
    "    _save_heatmap(spearman_corr, f\"Spearman — {rr}\", PLOTS_DIR / f\"{rr}_spearman.png\")\n",
    "    _save_heatmap(partial_corr, f\"Partial (controls: TOD/Orbital/Crew) — {rr}\", PLOTS_DIR / f\"{rr}_partial.png\")\n",
    "    _network_from_corr(pearson_corr, PLOTS_DIR / f\"{rr}_network.png\", thr=pearson_abs_threshold)\n",
    "\n",
    "    # tidy corr for dashboard\n",
    "    for mname, mat in [(\"pearson\", pearson_corr), (\"spearman\", spearman_corr), (\"partial\", partial_corr)]:\n",
    "        tri = mat.where(np.triu(np.ones(mat.shape), k=1).astype(bool))\n",
    "        tidy = tri.stack().reset_index()\n",
    "        tidy.columns = [\"Var1\", \"Var2\", \"Value\"]\n",
    "        tidy[\"Mission\"] = rr\n",
    "        tidy[\"Method\"]  = mname\n",
    "        all_corr_tidy.append(tidy)\n",
    "\n",
    "    # VAR Granger (conditional)\n",
    "    step = int(resample_freq.replace(\"min\",\"\"))\n",
    "    maxlags = max(1, int(maxlags_minutes/step))\n",
    "    try:\n",
    "        sel = VAR(df).select_order(maxlags=maxlags)\n",
    "        k_ar = int(sel.aic or max(1, sel.selected_orders.get(\"aic\", 1)))\n",
    "        k_ar = max(1, min(k_ar, maxlags))\n",
    "        model = VAR(df).fit(k_ar)\n",
    "    except Exception as e:\n",
    "        tprint(f\"[VAR FAIL] {rr}: {e}\")\n",
    "        continue\n",
    "\n",
    "    pairs = [(y, x) for y in df.columns for x in df.columns if x != y]\n",
    "    rows = []\n",
    "    for (target, source) in pairs:\n",
    "        try:\n",
    "            test = model.test_causality(caused=target, causing=[source], kind=\"f\")\n",
    "            rows.append({\n",
    "                \"Mission\": rr, \"Lag\": k_ar,\n",
    "                \"Target\": target, \"Source\": source,\n",
    "                \"F\": float(getattr(test, \"test_statistic\", np.nan)),\n",
    "                \"pvalue\": float(getattr(test, \"pvalue\", np.nan))\n",
    "            })\n",
    "        except Exception:\n",
    "            rows.append({\"Mission\": rr, \"Lag\": k_ar, \"Target\": target, \"Source\": source, \"F\": np.nan, \"pvalue\": np.nan})\n",
    "\n",
    "    res_df = pd.DataFrame(rows)\n",
    "    res_df[\"qvalue\"] = _bh_fdr(res_df[\"pvalue\"], alpha=alpha_fdr)\n",
    "    res_df[\"Significant\"] = res_df[\"qvalue\"] <= alpha_fdr\n",
    "    res_df[\"MinusLog10_q\"] = -np.log10(res_df[\"qvalue\"].clip(lower=1e-300))\n",
    "    all_granger.append(res_df)\n",
    "\n",
    "    mat = res_df.pivot(index=\"Target\", columns=\"Source\", values=\"MinusLog10_q\").reindex(index=df.columns, columns=df.columns)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(mat, cmap=\"viridis\")\n",
    "    plt.title(f\"Directed Granger (−log10 q) — {rr} [lag={k_ar}]\")\n",
    "    plt.tight_layout(); plt.savefig(PLOTS_DIR / f\"{rr}_granger_directed.png\"); plt.close()\n",
    "\n",
    "    # optional: nonlinear lag scan (mutual information)\n",
    "    if RUN_NONLINEAR and HAS_MI:\n",
    "        maxlag_steps = maxlags\n",
    "        mi_rows = []\n",
    "        for tgt in df.columns:\n",
    "            y = df[tgt].values.reshape(-1, 1)\n",
    "            for src in df.columns:\n",
    "                if src == tgt: continue\n",
    "                x = df[src].values\n",
    "                best_mi, best_lag = 0.0, 0\n",
    "                for L in range(1, maxlag_steps+1):\n",
    "                    Xlag = x[:-L].reshape(-1, 1)\n",
    "                    Yt   = y[L:]\n",
    "                    mi = float(mutual_info_regression(Xlag, Yt.ravel(), random_state=S.seed, n_neighbors=3))\n",
    "                    if mi > best_mi:\n",
    "                        best_mi, best_lag = mi, L\n",
    "                mi_rows.append({\"Mission\": rr, \"Target\": tgt, \"Source\": src, \"BestLagSteps\": best_lag, \"BestLagMin\": best_lag*step, \"MI\": best_mi})\n",
    "        mi_df = pd.DataFrame(mi_rows)\n",
    "        mi_df.to_csv(REL_DIR / f\"{rr}_lagged_mi.csv\", index=False)\n",
    "        all_mi.append(mi_df)\n",
    "        # heatmap of best MI\n",
    "        H = mi_df.pivot(index=\"Target\", columns=\"Source\", values=\"MI\").reindex(index=df.columns, columns=df.columns)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(H, cmap=\"magma\", vmin=0)\n",
    "        plt.title(f\"Lagged MI (best over ≤{maxlags_minutes} min) — {rr}\")\n",
    "        plt.tight_layout(); plt.savefig(PLOTS_DIR / f\"{rr}_lagged_mi.png\"); plt.close()\n",
    "\n",
    "# ---- Save combined outputs ----\n",
    "if len(all_granger):\n",
    "    gr_all = pd.concat(all_granger, ignore_index=True)\n",
    "    gr_all.to_csv(REL_DIR / \"granger_var_results.csv\", index=False)\n",
    "    tprint(f\"[✓] Wrote VAR-Granger results → {REL_DIR/'granger_var_results.csv'} (rows={len(gr_all)})\")\n",
    "else:\n",
    "    tprint(\"[!] No VAR-Granger results to save.\")\n",
    "\n",
    "if len(all_corr_tidy):\n",
    "    corr_all = pd.concat(all_corr_tidy, ignore_index=True)\n",
    "    corr_all.to_csv(REL_DIR / \"correlations_tidy.csv\", index=False)\n",
    "    tprint(f\"[✓] Wrote correlations tidy → {REL_DIR/'correlations_tidy.csv'} (rows={len(corr_all)})\")\n",
    "\n",
    "if RUN_NONLINEAR and len(all_mi):\n",
    "    mi_all = pd.concat(all_mi, ignore_index=True)\n",
    "    mi_all.to_csv(REL_DIR / \"lagged_mi_all.csv\", index=False)\n",
    "    tprint(f\"[✓] Wrote lagged MI (all) → {REL_DIR/'lagged_mi_all.csv'} (rows={len(mi_all)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1119f20",
   "metadata": {},
   "source": [
    "### Section 6: Anomaly Detection and Forecasting\n",
    "#### Purpose\n",
    "Detect environmental deviations and produce short-term forecasts useful for life-support planning.\n",
    "#### What is implemented\n",
    "1. Statistical detectors: Z-score and EWMA with plots per variable.\n",
    "2. LSTM autoencoder (reconstruction error) and one-step LSTM forecaster with split evaluation.\n",
    "3. macOS GPU/MPS awareness; metrics CSVs and consolidated scoreboard.\n",
    "#### Future Work\n",
    "- Compact SARIMAX grid and mission-level multivariate VAR.\n",
    "- Broader SARIMAX grid search with AIC selection and saved diagnostics.\n",
    "- Residual diagnostics and Ljung–Box tests for each baseline model.\n",
    "- Conformal prediction intervals or quantile regression for uncertainty bands.\n",
    "- Roll out the LSTM/anomaly suite beyond RR-1 (RR-3/RR-6/RR-9/RR-12/RR-19) using same evaluation.\n",
    "- Path to multivariate forecasting (e.g., multivariate LSTM/attention) where feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b98db6-175c-4ae1-b091-38e95f9b9d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sec6 bootstrap] Using fallback Settings (S).\n",
      "[2025-09-13 16:04:40 UTC] [Sec6] TF 2.16.2 · Darwin 24.5.0 · GPUs=1 (no CUDA) → FP32\n",
      "[2025-09-13 16:04:41 UTC] [Sec6] Jobs queued: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks:   0%|      | 0/7 [00:00<?, ?it/s, RR-1:telemetry:Temp_degC_ISS]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 16:04:41 UTC] [START] RR-1 telemetry:Temp_degC_ISS  (60455 pts)\n",
      "[2025-09-13 16:04:42 UTC] [RR-1 telemetry:Temp_degC_ISS] detectors done in 1.4s\n",
      "[2025-09-13 16:55:12 UTC] AE RR-1:Temp_degC_ISS epoch 1: loss=0.00518 val_loss=0.00003\n",
      "[2025-09-13 17:46:06 UTC] AE RR-1:Temp_degC_ISS epoch 2: loss=0.00850 val_loss=0.00019\n",
      "[2025-09-13 17:47:13 UTC] [RR-1 telemetry:Temp_degC_ISS] AE done in 6150.5s\n",
      "[2025-09-13 18:10:14 UTC] FC RR-1:Temp_degC_ISS epoch 1: loss=0.00507 val_loss=nan\n",
      "[2025-09-13 18:33:22 UTC] FC RR-1:Temp_degC_ISS epoch 2: loss=0.00027 val_loss=nan\n",
      "[2025-09-13 18:56:29 UTC] FC RR-1:Temp_degC_ISS epoch 3: loss=0.00024 val_loss=nan\n",
      "[2025-09-13 18:57:18 UTC] [RR-1 telemetry:Temp_degC_ISS] forecaster done in 4204.6s\n",
      "[2025-09-13 18:57:18 UTC] [RR-1 telemetry:Temp_degC_ISS] SARIMAX skipped (disabled) | total 10356.7s\n",
      "[2025-09-13 18:57:18 UTC] [DONE ] RR-1 telemetry:Temp_degC_ISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks:  14%|▏| 1/7 [2:52:37<17:15:43, 10357.20s/it, RR-1:telemetry:RH_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 18:57:19 UTC] [START] RR-1 telemetry:RH_percent_ISS  (60455 pts)\n",
      "[2025-09-13 18:57:20 UTC] [RR-1 telemetry:RH_percent_ISS] detectors done in 1.2s\n",
      "[2025-09-13 19:48:17 UTC] AE RR-1:RH_percent_ISS epoch 1: loss=0.05514 val_loss=0.00134\n",
      "[2025-09-13 20:38:42 UTC] AE RR-1:RH_percent_ISS epoch 2: loss=0.00203 val_loss=0.00056\n",
      "[2025-09-13 20:39:49 UTC] [RR-1 telemetry:RH_percent_ISS] AE done in 6149.0s\n",
      "[2025-09-13 21:02:55 UTC] FC RR-1:RH_percent_ISS epoch 1: loss=0.00330 val_loss=nan\n",
      "[2025-09-13 21:25:57 UTC] FC RR-1:RH_percent_ISS epoch 2: loss=0.00061 val_loss=nan\n",
      "[2025-09-13 21:48:58 UTC] FC RR-1:RH_percent_ISS epoch 3: loss=0.00060 val_loss=nan\n",
      "[2025-09-13 21:49:46 UTC] [RR-1 telemetry:RH_percent_ISS] forecaster done in 4197.4s\n",
      "[2025-09-13 21:49:46 UTC] [RR-1 telemetry:RH_percent_ISS] SARIMAX skipped (disabled) | total 10347.7s\n",
      "[2025-09-13 21:49:46 UTC] [DONE ] RR-1 telemetry:RH_percent_ISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks:  29%|▎| 2/7 [5:45:05<14:22:43, 10352.77s/it, RR-1:telemetry:CO2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-13 21:49:47 UTC] [START] RR-1 telemetry:CO2_ppm_ISS  (60455 pts)\n",
      "[2025-09-13 21:49:48 UTC] [RR-1 telemetry:CO2_ppm_ISS] detectors done in 1.1s\n",
      "[2025-09-13 22:40:36 UTC] AE RR-1:CO2_ppm_ISS epoch 1: loss=0.00361 val_loss=0.00126\n",
      "[2025-09-13 23:31:07 UTC] AE RR-1:CO2_ppm_ISS epoch 2: loss=0.00107 val_loss=0.00204\n",
      "[2025-09-13 23:32:14 UTC] [RR-1 telemetry:CO2_ppm_ISS] AE done in 6145.2s\n",
      "[2025-09-13 23:55:15 UTC] FC RR-1:CO2_ppm_ISS epoch 1: loss=0.00085 val_loss=nan\n",
      "[2025-09-14 00:18:14 UTC] FC RR-1:CO2_ppm_ISS epoch 2: loss=0.00031 val_loss=nan\n",
      "[2025-09-14 00:41:13 UTC] FC RR-1:CO2_ppm_ISS epoch 3: loss=0.00029 val_loss=nan\n",
      "[2025-09-14 00:42:01 UTC] [RR-1 telemetry:CO2_ppm_ISS] forecaster done in 4187.7s\n",
      "[2025-09-14 00:42:01 UTC] [RR-1 telemetry:CO2_ppm_ISS] SARIMAX skipped (disabled) | total 10334.1s\n",
      "[2025-09-14 00:42:01 UTC] [DONE ] RR-1 telemetry:CO2_ppm_ISS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks:  43%|▍| 3/7 [8:37:20<11:29:46, 10346.60s/it, RR-1:radiation:GCR"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 00:42:02 UTC] [START] RR-1 radiation:GCR_Dose_mGy_d  (54721 pts)\n",
      "[2025-09-14 00:42:03 UTC] [RR-1 radiation:GCR_Dose_mGy_d] detectors done in 1.4s\n",
      "[2025-09-14 01:27:47 UTC] AE RR-1:GCR_Dose_mGy_d epoch 1: loss=0.00304 val_loss=0.01268\n",
      "[2025-09-14 02:13:41 UTC] AE RR-1:GCR_Dose_mGy_d epoch 2: loss=0.00186 val_loss=0.02328\n",
      "[2025-09-14 02:14:42 UTC] [RR-1 radiation:GCR_Dose_mGy_d] AE done in 5558.6s\n",
      "[2025-09-14 02:35:31 UTC] FC RR-1:GCR_Dose_mGy_d epoch 1: loss=0.00873 val_loss=nan\n",
      "[2025-09-14 02:56:20 UTC] FC RR-1:GCR_Dose_mGy_d epoch 2: loss=0.00131 val_loss=nan\n",
      "[2025-09-14 03:17:11 UTC] FC RR-1:GCR_Dose_mGy_d epoch 3: loss=0.00087 val_loss=nan\n",
      "[2025-09-14 03:17:55 UTC] [RR-1 radiation:GCR_Dose_mGy_d] forecaster done in 3792.8s\n",
      "[2025-09-14 03:17:55 UTC] [RR-1 radiation:GCR_Dose_mGy_d] SARIMAX skipped (disabled) | total 9352.9s\n",
      "[2025-09-14 03:17:55 UTC] [DONE ] RR-1 radiation:GCR_Dose_mGy_d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks:  57%|▌| 4/7 [11:13:14<8:23:56, 10078.82s/it, RR-1:radiation:SAA"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 03:17:55 UTC] [START] RR-1 radiation:SAA_Dose_mGy_d  (54721 pts)\n",
      "[2025-09-14 03:17:56 UTC] [RR-1 radiation:SAA_Dose_mGy_d] detectors done in 1.5s\n",
      "[2025-09-14 04:05:18 UTC] AE RR-1:SAA_Dose_mGy_d epoch 1: loss=0.00564 val_loss=0.00627\n",
      "[2025-09-14 04:51:10 UTC] AE RR-1:SAA_Dose_mGy_d epoch 2: loss=0.00140 val_loss=0.00036\n",
      "[2025-09-14 04:52:11 UTC] [RR-1 radiation:SAA_Dose_mGy_d] AE done in 5654.8s\n",
      "[2025-09-14 05:13:04 UTC] FC RR-1:SAA_Dose_mGy_d epoch 1: loss=0.00088 val_loss=nan\n",
      "[2025-09-14 05:33:55 UTC] FC RR-1:SAA_Dose_mGy_d epoch 2: loss=0.00036 val_loss=nan\n",
      "[2025-09-14 05:54:50 UTC] FC RR-1:SAA_Dose_mGy_d epoch 3: loss=0.00019 val_loss=nan\n",
      "[2025-09-14 05:55:35 UTC] [RR-1 radiation:SAA_Dose_mGy_d] forecaster done in 3803.4s\n",
      "[2025-09-14 05:55:35 UTC] [RR-1 radiation:SAA_Dose_mGy_d] SARIMAX skipped (disabled) | total 9459.7s\n",
      "[2025-09-14 05:55:35 UTC] [DONE ] RR-1 radiation:SAA_Dose_mGy_d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks:  71%|▋| 5/7 [13:50:54<5:31:24, 9942.05s/it, RR-1:radiation:Tota"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 05:55:35 UTC] [START] RR-1 radiation:Total_Dose_mGy_d  (54721 pts)\n",
      "[2025-09-14 05:55:36 UTC] [RR-1 radiation:Total_Dose_mGy_d] detectors done in 1.4s\n",
      "[2025-09-14 06:41:43 UTC] AE RR-1:Total_Dose_mGy_d epoch 1: loss=0.00684 val_loss=0.01177\n",
      "[2025-09-14 07:27:24 UTC] AE RR-1:Total_Dose_mGy_d epoch 2: loss=0.00193 val_loss=0.00747\n",
      "[2025-09-14 07:28:25 UTC] [RR-1 radiation:Total_Dose_mGy_d] AE done in 5568.4s\n",
      "[2025-09-14 07:49:17 UTC] FC RR-1:Total_Dose_mGy_d epoch 1: loss=0.00169 val_loss=nan\n",
      "[2025-09-14 08:10:11 UTC] FC RR-1:Total_Dose_mGy_d epoch 2: loss=0.00068 val_loss=nan\n",
      "[2025-09-14 08:31:03 UTC] FC RR-1:Total_Dose_mGy_d epoch 3: loss=0.00047 val_loss=nan\n",
      "[2025-09-14 08:31:47 UTC] [RR-1 radiation:Total_Dose_mGy_d] forecaster done in 3801.6s\n",
      "[2025-09-14 08:31:47 UTC] [RR-1 radiation:Total_Dose_mGy_d] SARIMAX skipped (disabled) | total 9371.5s\n",
      "[2025-09-14 08:31:47 UTC] [DONE ] RR-1 radiation:Total_Dose_mGy_d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks:  86%|▊| 6/7 [16:27:05<2:43:54, 9834.43s/it, RR-1:radiation:Accu"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 08:31:47 UTC] [START] RR-1 radiation:Accumulated_Dose_mGy_d  (54721 pts)\n",
      "[2025-09-14 08:31:49 UTC] [RR-1 radiation:Accumulated_Dose_mGy_d] detectors done in 2.0s\n",
      "[2025-09-14 09:18:04 UTC] AE RR-1:Accumulated_Dose_mGy_d epoch 1: loss=0.00000 val_loss=0.00169\n",
      "[2025-09-14 10:03:56 UTC] AE RR-1:Accumulated_Dose_mGy_d epoch 2: loss=0.00087 val_loss=0.03770\n",
      "[2025-09-14 10:04:56 UTC] [RR-1 radiation:Accumulated_Dose_mGy_d] AE done in 5587.5s\n",
      "[2025-09-14 10:25:43 UTC] FC RR-1:Accumulated_Dose_mGy_d epoch 1: loss=0.00000 val_loss=nan\n",
      "[2025-09-14 10:46:30 UTC] FC RR-1:Accumulated_Dose_mGy_d epoch 2: loss=0.00009 val_loss=nan\n",
      "[2025-09-14 11:07:20 UTC] FC RR-1:Accumulated_Dose_mGy_d epoch 3: loss=0.00003 val_loss=nan\n",
      "[2025-09-14 11:08:04 UTC] [RR-1 radiation:Accumulated_Dose_mGy_d] forecaster done in 3787.8s\n",
      "[2025-09-14 11:08:04 UTC] [RR-1 radiation:Accumulated_Dose_mGy_d] SARIMAX skipped (disabled) | total 9377.4s\n",
      "[2025-09-14 11:08:04 UTC] [DONE ] RR-1 radiation:Accumulated_Dose_mGy_d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Section 6 tasks: 100%|█| 7/7 [19:03:23<00:00, 9800.52s/it, RR-1:radiation:Accumu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 11:08:06 UTC] [START] VAR baseline RR-1 on 54700 rows, 7 vars\n",
      "[2025-09-14 11:08:24 UTC] [VAR BASELINE ERR] RR-1: 'VARResults' object has no attribute 'y'\n",
      "[2025-09-14 11:08:24 UTC] [DONE ] VAR baseline RR-1\n",
      "[2025-09-14 11:08:24 UTC] [✓] Saved scoreboard → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-1_forecast_scoreboard.csv (10 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RR</th>\n",
       "      <th>Type</th>\n",
       "      <th>Variable</th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>AE_Threshold</th>\n",
       "      <th>AE_MeanMSE</th>\n",
       "      <th>AE_Time_s</th>\n",
       "      <th>SeqLen</th>\n",
       "      <th>Resample</th>\n",
       "      <th>Conformal_q_inv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Accumulated_Dose_mGy_d</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.642128</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>1.286641</td>\n",
       "      <td>0.316788</td>\n",
       "      <td>9375.284692</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.329161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>CO2_ppm_ISS</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>57.470621</td>\n",
       "      <td>45.486809</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>10332.888385</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>110.456475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>GCR_Dose_mGy_d</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.024436</td>\n",
       "      <td>0.024295</td>\n",
       "      <td>9351.403144</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>RH_percent_ISS</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>1.025136</td>\n",
       "      <td>0.843621</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.001615</td>\n",
       "      <td>10346.441202</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>1.558475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>SAA_Dose_mGy_d</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.009078</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>9458.126925</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>SAA_Dose_mGy_d</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.009078</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>9458.126925</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.001238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>Temp_degC_ISS</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.073985</td>\n",
       "      <td>0.057713</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>10355.082372</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.093678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>telemetry</td>\n",
       "      <td>Temp_degC_ISS</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.073985</td>\n",
       "      <td>0.057713</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>10355.082372</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.093678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Total_Dose_mGy_d</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.022289</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>9370.028767</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.001471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RR-1</td>\n",
       "      <td>radiation</td>\n",
       "      <td>Total_Dose_mGy_d</td>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.022289</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>9370.028767</td>\n",
       "      <td>45</td>\n",
       "      <td>1min</td>\n",
       "      <td>0.001471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RR       Type                Variable Model       RMSE        MAE  \\\n",
       "13  RR-1  radiation  Accumulated_Dose_mGy_d  LSTM   0.642128   0.537537   \n",
       "9   RR-1  telemetry             CO2_ppm_ISS  LSTM  57.470621  45.486809   \n",
       "10  RR-1  radiation          GCR_Dose_mGy_d  LSTM   0.000059   0.000059   \n",
       "8   RR-1  telemetry          RH_percent_ISS  LSTM   1.025136   0.843621   \n",
       "4   RR-1  radiation          SAA_Dose_mGy_d  LSTM   0.001281   0.000980   \n",
       "11  RR-1  radiation          SAA_Dose_mGy_d  LSTM   0.001281   0.000980   \n",
       "0   RR-1  telemetry           Temp_degC_ISS  LSTM   0.073985   0.057713   \n",
       "7   RR-1  telemetry           Temp_degC_ISS  LSTM   0.073985   0.057713   \n",
       "5   RR-1  radiation        Total_Dose_mGy_d  LSTM   0.001362   0.001318   \n",
       "12  RR-1  radiation        Total_Dose_mGy_d  LSTM   0.001362   0.001318   \n",
       "\n",
       "    AE_Threshold  AE_MeanMSE     AE_Time_s  SeqLen Resample  Conformal_q_inv  \n",
       "13      1.286641    0.316788   9375.284692      45     1min         0.329161  \n",
       "9       0.003686    0.001429  10332.888385      45     1min       110.456475  \n",
       "10      0.024436    0.024295   9351.403144      45     1min         0.000069  \n",
       "8       0.003064    0.001615  10346.441202      45     1min         1.558475  \n",
       "4       0.009078    0.003982   9458.126925      45     1min         0.001238  \n",
       "11      0.009078    0.003982   9458.126925      45     1min         0.001238  \n",
       "0       0.000085    0.000020  10355.082372      45     1min         0.093678  \n",
       "7       0.000085    0.000020  10355.082372      45     1min         0.093678  \n",
       "5       0.022289    0.004682   9370.028767      45     1min         0.001471  \n",
       "12      0.022289    0.004682   9370.028767      45     1min         0.001471  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Section 6: Anomaly Detection & Forecasting (restart-safe, with progress logs) ====\n",
    "\n",
    "import os, time, warnings, platform\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "# ---------- tqdm (robust text mode) ----------\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    def _tqdm(iterable=None, **kwargs):\n",
    "        return tqdm(iterable, dynamic_ncols=True, miniters=1, smoothing=0.05, **kwargs)\n",
    "except Exception:\n",
    "    def _tqdm(it, **kwargs): return it\n",
    "\n",
    "# ---------- Minimal Settings fallback ----------\n",
    "if \"S\" not in globals():\n",
    "    @dataclass\n",
    "    class Settings:\n",
    "        missions: List[str] = field(default_factory=lambda: [\"RR-1\",\"RR-3\",\"RR-6\",\"RR-9\",\"RR-12\",\"RR-19\"])\n",
    "        sampling: str = \"1min\"\n",
    "        outputs_root: Path = field(default_factory=lambda: Path.cwd() / \"outputs\")\n",
    "        preprocessed_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"preprocessed\")\n",
    "        pattern_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"pattern_analysis\")\n",
    "        anomalies_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"anomaly_forecast\")\n",
    "        relationships_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"relationships\")\n",
    "        omics_roots: List[Path] = field(default_factory=lambda: [])\n",
    "        data_roots: List[Path] = field(default_factory=lambda: [])\n",
    "    S = Settings()\n",
    "    for d in [S.outputs_root, S.preprocessed_dir, S.pattern_dir, S.anomalies_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[Sec6 bootstrap] Using fallback Settings (S).\")\n",
    "\n",
    "# ---------- Lightweight logger (stdout + file) ----------\n",
    "OUTDIR = S.anomalies_dir\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "PROGRESS_LOG = OUTDIR / \"sec6_progress.log\"\n",
    "\n",
    "def log(msg: str):\n",
    "    ts = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts} UTC] {msg}\"\n",
    "    print(line, flush=True)\n",
    "    try:\n",
    "        with open(PROGRESS_LOG, \"a\") as f:\n",
    "            f.write(line + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---------- Device policy ----------\n",
    "def _has_cuda_gpu() -> bool:\n",
    "    try:\n",
    "        gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "        for g in gpus:\n",
    "            name = str(tf.config.experimental.get_device_details(g).get(\"device_name\", \"\")).lower()\n",
    "            if (\"nvidia\" in name) or (\"cuda\" in name):\n",
    "                return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "for d in gpus:\n",
    "    try: tf.config.experimental.set_memory_growth(d, True)\n",
    "    except Exception: pass\n",
    "\n",
    "if platform.system() == \"Darwin\" or not _has_cuda_gpu():\n",
    "    # CPU or Apple M-series (MPS): keep float32 for speed/stability\n",
    "    log(f\"[Sec6] TF {tf.__version__} · {platform.system()} {platform.release()} · GPUs={len(gpus)} (no CUDA) → FP32\")\n",
    "else:\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    log(f\"[Sec6] TF {tf.__version__} · CUDA GPU detected → policy {mixed_precision.global_policy()}\")\n",
    "\n",
    "# ---------- Config ----------\n",
    "# Choose which missions to process (you can widen to S.missions later)\n",
    "MISSIONS = [\"RR-1\"]\n",
    "\n",
    "telemetry_cols = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_cols  = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "\n",
    "# Runtime toggles\n",
    "ENABLE_LSTM     = True\n",
    "ENABLE_SARIMAX  = False   # <— default OFF to avoid long runs; flip to True when benchmarking\n",
    "ENABLE_VAR      = True\n",
    "RESUME_SAFE     = True    # skip series if artifacts already exist\n",
    "WRITE_AFTER_EACH_SERIES = True  # incrementally flush scoreboard\n",
    "\n",
    "# SARIMAX bounds (if/when enabled)\n",
    "SARIMAX_TIME_BUDGET_S = 600      # per-series wall time budget\n",
    "SARIMAX_GRID = {\n",
    "    \"p\": (0,1,2),\n",
    "    \"d\": (0,1),\n",
    "    \"q\": (0,1,2),\n",
    "    \"P\": (0,1),\n",
    "    \"D\": (0,1),\n",
    "    \"Q\": (0,1),\n",
    "}\n",
    "SARIMAX_ONLY_DOWNSAMPLE = \"5min\"  # fit SARIMAX on coarser grid to speed up\n",
    "\n",
    "resample_freq = S.sampling   # minute-level data for detectors/LSTM\n",
    "seq_len = 45                  # LSTM window length (minutes)\n",
    "epochs = 3                    # keep small for first pass; raise when ready\n",
    "batch_size = 32\n",
    "val_frac = 0.2\n",
    "threshold_z = 3.0             # z-score anomaly threshold\n",
    "ewma_span = 60                 # EWMA window (minutes)\n",
    "min_points = 1000             # minimal usable length\n",
    "min_std = 1e-6                # guard against flat signals\n",
    "alpha_pi = 0.1                # 90% conformal interval\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=1, restore_best_weights=True)\n",
    "\n",
    "# ---------- Keras mini-callback for concise epoch logs ----------\n",
    "class EpochLogger(Callback):\n",
    "    def __init__(self, tag: str): self.tag = tag\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        l = logs.get(\"loss\", np.nan); vl = logs.get(\"val_loss\", np.nan)\n",
    "        log(f\"{self.tag} epoch {epoch+1}: loss={l:.5f} val_loss={vl:.5f}\")\n",
    "\n",
    "# ---------- IO helpers (rely on Section 3 outputs; restart-safe) ----------\n",
    "seasonal_dir = S.pattern_dir / \"seasonal_indices\"\n",
    "\n",
    "def _load_preprocessed(rr: str, kind: str) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        if kind == \"telemetry\":\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\"\n",
    "        else:\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\"\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p, index_col=0, parse_dates=True).sort_index()\n",
    "    except Exception as e:\n",
    "        log(f\"[LOAD FAIL] {rr} {kind}: {e}\")\n",
    "    return None\n",
    "\n",
    "def _maybe_deseason(rr: str, col: str, ts: pd.Series) -> pd.Series:\n",
    "    \"\"\"Subtract STL seasonal index from Section 4 if present; otherwise just clean/resample.\"\"\"\n",
    "    f = seasonal_dir / f\"{rr}_{col}_seasonal.csv\"\n",
    "    x = ts.resample(resample_freq).mean().interpolate(limit=5)\n",
    "    if f.exists():\n",
    "        try:\n",
    "            s = pd.read_csv(f, parse_dates=[\"timestamp\"]).set_index(\"timestamp\")[\"seasonal\"]\n",
    "            s = s.resample(resample_freq).mean().interpolate(limit=5)\n",
    "            x = x.reindex(s.index).interpolate(limit=5)\n",
    "            return (x - s).dropna()\n",
    "        except Exception:\n",
    "            return x.dropna()\n",
    "    return x.dropna()\n",
    "\n",
    "def _adf_p(s: pd.Series) -> float:\n",
    "    try: return adfuller(s.dropna(), autolag=\"AIC\")[1]\n",
    "    except Exception: return 1.0\n",
    "\n",
    "def _lb_p(resid: np.ndarray, lags: int = 24) -> float:\n",
    "    try:\n",
    "        lb = acorr_ljungbox(resid, lags=[lags], return_df=True)\n",
    "        return float(lb[\"lb_pvalue\"].iloc[0])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _train_test_split_series(y: pd.Series, frac=0.2) -> Tuple[pd.Series, pd.Series]:\n",
    "    n = len(y); n_tr = max(int(n*(1-frac)), 10)\n",
    "    return y.iloc[:n_tr], y.iloc[n_tr:]\n",
    "\n",
    "def _create_sequences(arr: np.ndarray, L: int):\n",
    "    X, y = [], []\n",
    "    for i in range(len(arr)-L):\n",
    "        X.append(arr[i:i+L])\n",
    "        y.append(arr[i+L])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def _savefig(path: Path):\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=160); plt.close()\n",
    "\n",
    "# ---------- Plotters (dashboard-compatible filenames) ----------\n",
    "def _plot_z_anoms(ts: pd.Series, z_mask: pd.Series, title: str, outdir: Path):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(ts.index, ts.values, label=\"value\", color=\"black\")\n",
    "    if z_mask.any():\n",
    "        plt.scatter(ts.index[z_mask], ts.values[z_mask], s=8, color=\"red\", label=\"|z|>3\")\n",
    "    plt.title(title); plt.legend()\n",
    "    _savefig(outdir / \"zscore_anomalies.png\")       # dashboard\n",
    "    # legacy duplicate for backward compatibility\n",
    "    _savefig(outdir / \"anoms.png\")\n",
    "\n",
    "def _plot_ewma_anoms(ts: pd.Series, ewma: pd.Series, ew_mask: pd.Series, title: str, outdir: Path):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(ts.index, ts.values, label=\"value\", color=\"black\")\n",
    "    plt.plot(ewma.index, ewma.values, color=\"blue\", alpha=0.8, label=\"EWMA\")\n",
    "    if ew_mask.any():\n",
    "        plt.scatter(ts.index[ew_mask], ts.values[ew_mask], s=8, color=\"orange\", label=\"EWMA anom\")\n",
    "    plt.title(title); plt.legend()\n",
    "    _savefig(outdir / \"ewma_anomalies.png\")\n",
    "\n",
    "# ---------- Metrics writer ----------\n",
    "def _append_metrics_row(row: dict, scoreboard_path: Path):\n",
    "    df = pd.DataFrame([row])\n",
    "    if scoreboard_path.exists():\n",
    "        try:\n",
    "            df_existing = pd.read_csv(scoreboard_path)\n",
    "            df = pd.concat([df_existing, df], ignore_index=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    df.to_csv(scoreboard_path, index=False)\n",
    "\n",
    "# ---------- Models ----------\n",
    "def run_univariate(rr: str, signal_type: str, col: str, y: pd.Series, outdir: Path,\n",
    "                   all_metrics: list, scoreboard_path: Path):\n",
    "    t0 = time.time()\n",
    "    if len(y) < min_points or y.std() < min_std:\n",
    "        log(f\"[SKIP] {rr} {signal_type}:{col} — short/flat\"); return\n",
    "\n",
    "    # Resume-skip if artifacts already present\n",
    "    if RESUME_SAFE:\n",
    "        needed = [outdir/\"zscore_anomalies.png\", outdir/\"ewma_anomalies.png\",\n",
    "                  outdir/\"lstm_reconstruction_error.png\", outdir/\"lstm_forecast.png\"]\n",
    "        if all(p.exists() for p in needed):\n",
    "            log(f\"[SKIP] {rr} {signal_type}:{col} — artifacts already present\")\n",
    "            return\n",
    "\n",
    "    # 1) Statistical detectors (always)\n",
    "    z = (y - y.mean())/y.std()\n",
    "    z_mask = z.abs() > threshold_z\n",
    "    ew = y.ewm(span=ewma_span).mean()\n",
    "    resid = y - ew\n",
    "    ew_mask = resid.abs() > (2*resid.std())\n",
    "    _plot_z_anoms(y, z_mask, f\"{col} Z-score anomalies ({rr})\", outdir)\n",
    "    _plot_ewma_anoms(y, ew, ew_mask, f\"{col} EWMA anomalies ({rr})\", outdir)\n",
    "    log(f\"[{rr} {signal_type}:{col}] detectors done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # Scale (MinMax on the single variable)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(y.to_frame(\"v\").values).astype(np.float32)\n",
    "\n",
    "    # 2) LSTM Autoencoder (reconstruction error → anomaly score)\n",
    "    if ENABLE_LSTM:\n",
    "        X_ae, _ = _create_sequences(scaled, seq_len)\n",
    "        if len(X_ae) < 100:\n",
    "            log(f\"[SKIP] {rr} {signal_type}:{col} — too few AE seqs\")\n",
    "        else:\n",
    "            split = int(len(X_ae)*(1-val_frac))\n",
    "            Xtr, Xte = X_ae[:split], X_ae[split:]\n",
    "\n",
    "            inputs = Input(shape=(seq_len, 1))\n",
    "            enc = LSTM(16, activation=\"relu\", return_sequences=False)(inputs)\n",
    "            dec = RepeatVector(seq_len)(enc)\n",
    "            dec = LSTM(16, activation=\"relu\", return_sequences=True)(dec)\n",
    "            outs = TimeDistributed(Dense(1))(dec)\n",
    "            ae = Model(inputs, outs)\n",
    "            ae.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "            ta0 = time.time()\n",
    "            val_split = 0.1 if len(Xtr) > 20 else 0.0  # guard tiny splits\n",
    "            ae.fit(Xtr, Xtr, epochs=epochs, batch_size=batch_size, validation_split=val_split,\n",
    "                   shuffle=False, callbacks=[early_stop, EpochLogger(f\"AE {rr}:{col}\")], verbose=0)\n",
    "            Xte_pred = ae.predict(Xte, verbose=0)\n",
    "            mse = np.mean(np.square(Xte - Xte_pred), axis=(1,2))\n",
    "            ae_thr = float(np.percentile(mse, 95))\n",
    "\n",
    "            plt.figure(figsize=(12,4))\n",
    "            plt.plot(mse, color=\"black\"); plt.axhline(ae_thr, color=\"red\", ls=\"--\")\n",
    "            plt.title(f\"{col} AE reconstruction error ({rr})\")\n",
    "            _savefig(outdir / \"lstm_reconstruction_error.png\")  # dashboard name\n",
    "            _savefig(outdir / \"ae_error.png\")                   # legacy duplicate\n",
    "            log(f\"[{rr} {signal_type}:{col}] AE done in {time.time()-ta0:.1f}s\")\n",
    "    else:\n",
    "        mse, ae_thr = np.array([]), np.nan\n",
    "\n",
    "    # 3) LSTM forecaster (one-step) + conformal prediction interval\n",
    "    if ENABLE_LSTM:\n",
    "        tf0 = time.time()\n",
    "        Xf, yf = _create_sequences(scaled, seq_len)\n",
    "        split_f = int(len(Xf)*(1-val_frac))\n",
    "        Xtrf, ytrf = Xf[:split_f], yf[:split_f]\n",
    "        Xtef, ytef = Xf[split_f:], yf[split_f:]\n",
    "        fnet = Sequential([LSTM(16, activation=\"relu\", input_shape=(seq_len,1)), Dense(1)])\n",
    "        fnet.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        fnet.fit(Xtrf, ytrf, epochs=epochs, batch_size=batch_size, shuffle=False,\n",
    "                 callbacks=[early_stop, EpochLogger(f\"FC {rr}:{col}\")], verbose=0)\n",
    "\n",
    "        # calibration for conformal PI\n",
    "        n_cal = max(50, int(0.4*len(Xtef)))\n",
    "        n_cal = min(n_cal, len(Xtef)-1) if len(Xtef) > 1 else 0\n",
    "        y_cal_pred = fnet.predict(Xtef[:n_cal], verbose=0).reshape(-1,1) if n_cal > 0 else np.zeros((0,1))\n",
    "        y_cal_true = ytef[:n_cal].reshape(-1,1) if n_cal > 0 else np.zeros((0,1))\n",
    "        cal_resid = np.abs(y_cal_true - y_cal_pred)\n",
    "        q = float(np.quantile(cal_resid, 1 - alpha_pi)) if len(cal_resid) else 0.0\n",
    "\n",
    "        y_pred = fnet.predict(Xtef, verbose=0).reshape(-1,1)\n",
    "        y_true = ytef.reshape(-1,1)\n",
    "        # invert scale\n",
    "        y_true_inv = scaler.inverse_transform(y_true)\n",
    "        y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "        # scale-aware PI width\n",
    "        denom = (y_true.std() + 1e-9)\n",
    "        q_inv = float(q * (y_true_inv.std() / denom)) if denom > 0 else float(q)\n",
    "\n",
    "        rmse = float(np.sqrt(mean_squared_error(y_true_inv, y_pred_inv)))\n",
    "        mae  = float(mean_absolute_error(y_true_inv, y_pred_inv))\n",
    "\n",
    "        # plot forecast + conformal band (after calibration region)\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(y_true_inv, label=\"actual\", color=\"black\")\n",
    "        plt.plot(y_pred_inv, label=\"lstm\", color=\"green\")\n",
    "        lo = y_pred_inv - q_inv; hi = y_pred_inv + q_inv\n",
    "        if len(y_pred_inv) and n_cal < len(y_pred_inv):\n",
    "            xs = np.arange(n_cal, len(y_pred_inv))\n",
    "            plt.fill_between(xs, lo[n_cal:, 0], hi[n_cal:, 0], alpha=0.2, label=\"90% PI\")\n",
    "        plt.title(f\"{col} LSTM forecast ({rr})  RMSE={rmse:.2f}, MAE={mae:.2f}\")\n",
    "        plt.legend()\n",
    "        _savefig(outdir / \"lstm_forecast.png\")\n",
    "        log(f\"[{rr} {signal_type}:{col}] forecaster done in {time.time()-tf0:.1f}s\")\n",
    "\n",
    "        row = {\n",
    "            \"RR\": rr, \"Type\": signal_type, \"Variable\": col, \"Model\": \"LSTM\",\n",
    "            \"RMSE\": rmse, \"MAE\": mae,\n",
    "            \"AE_Threshold\": float(ae_thr) if np.isfinite(ae_thr) else np.nan,\n",
    "            \"AE_MeanMSE\": float(np.mean(mse)) if mse.size else np.nan,\n",
    "            \"AE_Time_s\": float(time.time()-ta0) if ENABLE_LSTM and 'ta0' in locals() else np.nan,\n",
    "            \"SeqLen\": seq_len, \"Resample\": resample_freq, \"Conformal_q_inv\": q_inv\n",
    "        }\n",
    "        all_metrics.append(row)\n",
    "        if WRITE_AFTER_EACH_SERIES:\n",
    "            _append_metrics_row(row, scoreboard_path)\n",
    "\n",
    "    # 4) SARIMAX baseline (bounded) — optional\n",
    "    if ENABLE_SARIMAX:\n",
    "        tb0 = time.time()\n",
    "        try:\n",
    "            # downsample only for SARIMAX to speed up\n",
    "            y_sar = y.asfreq(resample_freq).resample(SARIMAX_ONLY_DOWNSAMPLE).mean().interpolate(limit=3)\n",
    "            # daily season at the chosen grid\n",
    "            step_min = int(str(SARIMAX_ONLY_DOWNSAMPLE).replace(\"min\",\"\").replace(\"T\",\"\"))\n",
    "            s = max(2, int(1440 / max(1, step_min)))\n",
    "            train, test = _train_test_split_series(y_sar)\n",
    "            best = None\n",
    "\n",
    "            def _time_ok(): return (time.time()-tb0) < SARIMAX_TIME_BUDGET_S\n",
    "\n",
    "            for p in SARIMAX_GRID[\"p\"]:\n",
    "                if not _time_ok(): break\n",
    "                for d in SARIMAX_GRID[\"d\"]:\n",
    "                    if not _time_ok(): break\n",
    "                    for q_ in SARIMAX_GRID[\"q\"]:\n",
    "                        if not _time_ok(): break\n",
    "                        for P in SARIMAX_GRID[\"P\"]:\n",
    "                            if not _time_ok(): break\n",
    "                            for D in SARIMAX_GRID[\"D\"]:\n",
    "                                if not _time_ok(): break\n",
    "                                for Q in SARIMAX_GRID[\"Q\"]:\n",
    "                                    if not _time_ok(): break\n",
    "                                    order=(p,d,q_); seas=(P,D,Q,s)\n",
    "                                    try:\n",
    "                                        m = SARIMAX(train, order=order, seasonal_order=seas,\n",
    "                                                    enforce_stationarity=False, enforce_invertibility=False)\n",
    "                                        r = m.fit(disp=False)\n",
    "                                        aic = r.aic\n",
    "                                        if np.isfinite(aic) and (best is None or aic < best[0]):\n",
    "                                            best = (aic, order, seas, r)\n",
    "                                    except Exception:\n",
    "                                        continue\n",
    "\n",
    "            if best is not None:\n",
    "                _, order, seas, rfit = best\n",
    "                pred = rfit.get_forecast(steps=len(test))\n",
    "                mean = pred.predicted_mean\n",
    "                ci = pred.conf_int(alpha=alpha_pi)\n",
    "                rmse_s = float(np.sqrt(mean_squared_error(test.values, mean.values)))\n",
    "                mae_s  = float(mean_absolute_error(test.values, mean.values))\n",
    "                lbp = _lb_p(rfit.resid.dropna().values, lags=24)\n",
    "\n",
    "                plt.figure(figsize=(12,4))\n",
    "                plt.plot(train.index, train.values, color=\"gray\", alpha=0.6, label=\"train\")\n",
    "                plt.plot(test.index, test.values, color=\"black\", label=\"actual\")\n",
    "                plt.plot(test.index, mean.values, color=\"purple\", label=f\"SARIMAX{order}x{seas}\")\n",
    "                if isinstance(ci, pd.DataFrame):\n",
    "                    plt.fill_between(test.index, ci.iloc[:,0].values, ci.iloc[:,1].values, alpha=0.2, label=\"PI\")\n",
    "                ttl = f\"{col} SARIMAX ({rr})  RMSE={rmse_s:.2f}, MAE={mae_s:.2f} | Ljung-Box p={lbp:.3f}\"\n",
    "                plt.title(ttl); plt.legend()\n",
    "                _savefig(outdir / \"sarimax_forecast.png\")\n",
    "\n",
    "                row = {\n",
    "                    \"RR\": rr, \"Type\": signal_type, \"Variable\": col,\n",
    "                    \"Model\": f\"SARIMAX{order}x{seas}\", \"RMSE\": rmse_s, \"MAE\": mae_s, \"LB_p\": lbp,\n",
    "                    \"GridMin\": SARIMAX_ONLY_DOWNSAMPLE, \"TimeBudget_s\": SARIMAX_TIME_BUDGET_S\n",
    "                }\n",
    "                all_metrics.append(row)\n",
    "                if WRITE_AFTER_EACH_SERIES:\n",
    "                    _append_metrics_row(row, scoreboard_path)\n",
    "\n",
    "            took = time.time()-tb0\n",
    "            log(f\"[{rr} {signal_type}:{col}] SARIMAX done in {took:.1f}s | total {time.time()-t0:.1f}s\")\n",
    "        except Exception as e:\n",
    "            log(f\"[SARIMAX ERR] {rr} {signal_type}:{col}: {e}\")\n",
    "    else:\n",
    "        log(f\"[{rr} {signal_type}:{col}] SARIMAX skipped (disabled) | total {time.time()-t0:.1f}s\")\n",
    "\n",
    "def run_var_baseline(rr: str, df_multi: pd.DataFrame) -> pd.DataFrame | None:\n",
    "    \"\"\"Simple mission-level multivariate VAR baseline (forecast split).\"\"\"\n",
    "    try:\n",
    "        df = df_multi.copy().dropna()\n",
    "        if len(df) < 800: return None\n",
    "        try:\n",
    "            if _adf_p(df.iloc[:,0]) > 0.05:\n",
    "                df = df.diff().dropna()\n",
    "        except Exception:\n",
    "            pass\n",
    "        # choose lags up to ~30 minutes relative to S.sampling\n",
    "        step = int(str(S.sampling).replace(\"min\",\"\").replace(\"T\",\"\"))\n",
    "        maxlags = max(1, int(30/step))\n",
    "        sel = VAR(df).select_order(maxlags=maxlags)\n",
    "        # defensive: prefer AIC if available else 1\n",
    "        k = getattr(sel, \"aic\", None)\n",
    "        if k is None or (isinstance(k, float) and not np.isfinite(k)):\n",
    "            k = sel.selected_orders.get(\"aic\", 1) if hasattr(sel, \"selected_orders\") else 1\n",
    "        k = int(max(1, min(int(k), maxlags)))\n",
    "        n = len(df); n_tr = int(n*0.8)\n",
    "        model = VAR(df.iloc[:n_tr]).fit(k)\n",
    "        fc = model.forecast(model.y, steps=n-n_tr)\n",
    "        pred = pd.DataFrame(fc, index=df.index[n_tr:], columns=df.columns)\n",
    "        out = []\n",
    "        for c in df.columns:\n",
    "            yt = df[c].iloc[n_tr:]; yp = pred[c].reindex(yt.index)\n",
    "            out.append({\"RR\": rr, \"Type\": \"multivariate\", \"Variable\": c,\n",
    "                        \"Model\": f\"VAR(k={k})\", \"RMSE\": float(np.sqrt(mean_squared_error(yt, yp))),\n",
    "                        \"MAE\": float(mean_absolute_error(yt, yp))})\n",
    "        return pd.DataFrame(out)\n",
    "    except Exception as e:\n",
    "        log(f\"[VAR BASELINE ERR] {rr}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ---------- Build worklist ----------\n",
    "work = []\n",
    "for rr in MISSIONS:\n",
    "    tel = _load_preprocessed(rr, \"telemetry\")\n",
    "    rad = _load_preprocessed(rr, \"radiation\")\n",
    "    if tel is not None:\n",
    "        for c in telemetry_cols:\n",
    "            if c in tel.columns: work.append((rr, \"telemetry\", c))\n",
    "    if rad is not None:\n",
    "        for c in radiation_cols:\n",
    "            if c in rad.columns: work.append((rr, \"radiation\", c))\n",
    "\n",
    "log(f\"[Sec6] Jobs queued: {len(work)}\")\n",
    "\n",
    "# ---------- Main loop with progress ----------\n",
    "all_metrics = []\n",
    "scoreboard_path = OUTDIR / f\"{'-'.join(MISSIONS)}_forecast_scoreboard.csv\"\n",
    "\n",
    "pbar = _tqdm(work, desc=\"Section 6 tasks\")\n",
    "for (rr, kind, col) in pbar:\n",
    "    pbar.set_postfix_str(f\"{rr}:{kind}:{col}\")\n",
    "    df = _load_preprocessed(rr, \"telemetry\" if kind==\"telemetry\" else \"radiation\")\n",
    "    if df is None or col not in df.columns:\n",
    "        log(f\"[SKIP] {rr} {kind}:{col} — missing\")\n",
    "        continue\n",
    "    s_raw = df[col].dropna()\n",
    "    if len(s_raw) < min_points:\n",
    "        log(f\"[SKIP] {rr} {kind}:{col} — too short ({len(s_raw)} pts)\")\n",
    "        continue\n",
    "    y = _maybe_deseason(rr, col, s_raw)\n",
    "    y = y.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    if len(y) < min_points or y.std() < min_std:\n",
    "        log(f\"[SKIP] {rr} {kind}:{col} — post-clean short/flat\")\n",
    "        continue\n",
    "\n",
    "    outdir = OUTDIR / rr / f\"{kind}_{col}\"\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    log(f\"[START] {rr} {kind}:{col}  ({len(y)} pts)\")\n",
    "    run_univariate(rr, kind, col, y, outdir, all_metrics, scoreboard_path)\n",
    "    log(f\"[DONE ] {rr} {kind}:{col}\")\n",
    "\n",
    "# ---------- VAR baseline once per mission ----------\n",
    "if ENABLE_VAR:\n",
    "    for rr in MISSIONS:\n",
    "        tel = _load_preprocessed(rr, \"telemetry\")\n",
    "        rad = _load_preprocessed(rr, \"radiation\")\n",
    "        if (tel is None) or (rad is None):\n",
    "            continue\n",
    "        cols_multi = [c for c in telemetry_cols if c in tel.columns] + \\\n",
    "                     [c for c in radiation_cols if c in rad.columns]\n",
    "        if len(cols_multi) >= 2:\n",
    "            mdf = tel[telemetry_cols].join(rad[radiation_cols], how=\"inner\")\n",
    "            mdf = mdf[[c for c in cols_multi]]\n",
    "            for c in list(mdf.columns):\n",
    "                mdf[c] = _maybe_deseason(rr, c, mdf[c])\n",
    "            mdf = mdf.dropna().replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            log(f\"[START] VAR baseline {rr} on {len(mdf)} rows, {len(mdf.columns)} vars\")\n",
    "            var_metrics = run_var_baseline(rr, mdf)\n",
    "            if var_metrics is not None and len(var_metrics):\n",
    "                # append VAR rows to scoreboard immediately\n",
    "                for _, row in var_metrics.iterrows():\n",
    "                    _append_metrics_row(row.to_dict(), scoreboard_path)\n",
    "                all_metrics.extend(var_metrics.to_dict(\"records\"))\n",
    "            log(f\"[DONE ] VAR baseline {rr}\")\n",
    "\n",
    "# ---------- Save/flush scoreboard (final) ----------\n",
    "score = pd.DataFrame(all_metrics)\n",
    "# final de-dupe (just in case of restarts)\n",
    "if not score.empty:\n",
    "    if scoreboard_path.exists():\n",
    "        try:\n",
    "            prev = pd.read_csv(scoreboard_path)\n",
    "            score = pd.concat([prev, score], ignore_index=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # simple de-dup by subset of columns if present\n",
    "    keep_cols = [c for c in [\"RR\",\"Type\",\"Variable\",\"Model\",\"RMSE\",\"MAE\"] if c in score.columns]\n",
    "    if keep_cols:\n",
    "        score = score.drop_duplicates(subset=keep_cols, keep=\"last\")\n",
    "    score.to_csv(scoreboard_path, index=False)\n",
    "\n",
    "log(f\"[✓] Saved scoreboard → {scoreboard_path} ({len(score)} rows)\")\n",
    "\n",
    "# Optional quick peek (Jupyter)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(score.sort_values([\"RR\",\"Variable\",\"RMSE\"]).head(20))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc216c",
   "metadata": {},
   "source": [
    "### Section 7: Evaluation and Insights\n",
    "#### Purpose\n",
    "Aggregate results across missions, spotlight volatility/forecastability, and link environmental anomalies to biological findings where timing allows.\n",
    "#### What is implemented\n",
    "1. Aggregation of per-mission metrics; top forecastable variables and strongest anomaly intensities.\n",
    "2. Composite anomaly timeline for RR-1 from z-spikes across key telemetry/radiation.\n",
    "3. RR-1 omics linkage to GLDS-98, GLDS-99, and GLDS-104: DEGs and PCA summaries; exports to insights directory.\n",
    "4. Visual summaries: anomaly intensity by type and by mission.\n",
    "#### Future Work\n",
    "- Compare missions with pronounced environmental anomalies against relatively stable missions.\n",
    "- Anomaly calendar per mission with merged telemetry/radiation spike timestamps.\n",
    "- If event context is available, annotate anomaly calendars with docking/EVA intervals and compute overlap rates.\n",
    "- Pathway enrichment summaries per anomaly window and concise narrative per mission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c16f6596-eec4-442b-a907-be49f4421d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 16:53:05 UTC] [Sec7] Loaded metrics rows: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Volatility/Calendar: 100%|████████████████████████| 6/6 [00:08<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 16:53:13 UTC] [Sec7] No event context found; overlap skipped.\n",
      "[2025-09-14 16:53:17 UTC] [Sec7] OMICS loaded GLDS-98: diff_raw, diff, counts, sample, contrasts\n",
      "[2025-09-14 16:53:18 UTC] [Sec7] OMICS loaded GLDS-99: diff_raw, diff, counts, sample, contrasts\n",
      "[2025-09-14 16:53:18 UTC] [Sec7] OMICS loaded GLDS-104: diff_raw, diff, counts, sample, contrasts\n",
      "[2025-09-14 16:53:18 UTC] [OMICS] GLDS-98: PCA table -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/insights/GLDS-98_pca_table.csv\n",
      "[2025-09-14 16:53:18 UTC] [OMICS] GLDS-99: PCA table -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/insights/GLDS-99_pca_table.csv\n",
      "[2025-09-14 16:53:18 UTC] [OMICS] GLDS-104: PCA table -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/insights/GLDS-104_pca_table.csv\n",
      "[2025-09-14 16:53:18 UTC] [Sec7] No GMT pathway file found; enrichment skipped.\n",
      "\n",
      "[✓] Section 7 complete.\n",
      "Outputs → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/insights\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 7: Evaluation & Insights (restart-safe; explicit GLDS file map) ====\n",
    "# Purpose: Aggregate results across missions, spotlight volatility/forecastability,\n",
    "# and link environmental anomalies to biological findings where timing allows.\n",
    "\n",
    "import os, json, warnings, platform\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- tqdm (quiet if not available) ---\n",
    "try:\n",
    "    from tqdm import tqdm as _tqdm\n",
    "    def tqdm(it, **kw): return _tqdm(it, **kw)\n",
    "    def _tw(msg): print(msg)\n",
    "    tqdm.write = _tw\n",
    "except Exception:\n",
    "    def tqdm(it, **kw): return it\n",
    "    tqdm.write = print\n",
    "\n",
    "# ---------------- Settings bootstrap ----------------\n",
    "if \"S\" not in globals():\n",
    "    from dataclasses import dataclass, field\n",
    "    @dataclass\n",
    "    class Settings:\n",
    "        missions: List[str] = field(default_factory=lambda: [\"RR-1\",\"RR-3\",\"RR-6\",\"RR-9\",\"RR-12\",\"RR-19\"])\n",
    "        sampling: str = \"1min\"\n",
    "        outputs_root: Path = field(default_factory=lambda: Path.cwd() / \"outputs\")\n",
    "        preprocessed_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"preprocessed\")\n",
    "        pattern_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"pattern_analysis\")\n",
    "        anomalies_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"anomaly_forecast\")\n",
    "    S = Settings()\n",
    "    for d in [S.outputs_root, S.preprocessed_dir, S.pattern_dir, S.anomalies_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[Sec7 bootstrap] Using fallback Settings (S).\")\n",
    "\n",
    "# add optional attrs if absent\n",
    "if not hasattr(S, \"relationships_dir\"): S.relationships_dir = S.outputs_root / \"relationships\"\n",
    "if not hasattr(S, \"omics_roots\"):       S.omics_roots       = [S.outputs_root / \"omics\"]\n",
    "if not hasattr(S, \"data_roots\"):        S.data_roots        = [S.outputs_root]\n",
    "\n",
    "ANOM_DIR     = S.anomalies_dir\n",
    "PATTERN_DIR  = S.pattern_dir\n",
    "REL_DIR      = S.relationships_dir\n",
    "INSIGHTS_DIR = S.outputs_root / \"insights\"\n",
    "CAL_DIR      = INSIGHTS_DIR / \"anomaly_calendars\"\n",
    "PLOTS_DIR    = INSIGHTS_DIR / \"plots\"\n",
    "for d in [INSIGHTS_DIR, CAL_DIR, PLOTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- Your GLDS file paths (authoritative) ----------------\n",
    "# If a listed file is missing, code will try sensible GLbulkRNAseq fallbacks.\n",
    "GLDS_FILES: Dict[str, Dict[str, Optional[str]]] = {\n",
    "    \"GLDS-98\": {\n",
    "        \"contrasts\": \"/Users/kennethjenkins/data/genelab/GLDS-98_rna_seq_contrasts.csv\",\n",
    "        \"diff\":      \"/Users/kennethjenkins/data/genelab/GLDS-98_rna_seq_differential_expression.csv\",\n",
    "        \"counts\":    \"/Users/kennethjenkins/data/genelab/GLDS-98_rna_seq_Normalized_Counts.csv\",\n",
    "        \"sample\":    \"/Users/kennethjenkins/data/genelab/GLDS-98_rna_seq_SampleTable.csv\",\n",
    "    },\n",
    "    \"GLDS-99\": {\n",
    "        \"contrasts\": \"/Users/kennethjenkins/data/genelab/GLDS-99_rna_seq_contrasts.csv\",\n",
    "        \"diff\":      \"/Users/kennethjenkins/data/genelab/GLDS-99_rna_seq_differential_expression.csv\",\n",
    "        \"counts\":    \"/Users/kennethjenkins/data/genelab/GLDS-99_rna_seq_Normalized_Counts.csv\",\n",
    "        \"sample\":    \"/Users/kennethjenkins/data/genelab/GLDS-99_rna_seq_SampleTable.csv\",\n",
    "    },\n",
    "    \"GLDS-104\": {\n",
    "        \"contrasts\": \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_contrasts_GLbulkRNAseq.csv\",\n",
    "        \"diff\":      \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_differential_expression.csv\",\n",
    "        \"counts\":    \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_Normalized_Counts.csv\",\n",
    "        \"sample\":    \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_SampleTable_GLbulkRNAseq.csv\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Fallback filename patterns for GLDS-104 (post-2025 updates), used if the explicit path above is missing.\n",
    "GLDS_104_ALTS = {\n",
    "    \"diff\": [\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_differential_expression_GLbulkRNAseq.csv\",\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_differential_expression_rRNArm_GLbulkRNAseq.csv\",\n",
    "    ],\n",
    "    \"counts\": [\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_Normalized_Counts_GLbulkRNAseq.csv\",\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_Normalized_Counts_rRNArm_GLbulkRNAseq.csv\",\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_VST_Counts_GLbulkRNAseq.csv\",\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_VST_Counts_rRNArm_GLbulkRNAseq.csv\",\n",
    "    ],\n",
    "    \"sample\": [\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_SampleTable_GLbulkRNAseq.csv\",\n",
    "    ],\n",
    "    \"contrasts\": [\n",
    "        \"/Users/kennethjenkins/data/genelab/GLDS-104_rna_seq_contrasts_GLbulkRNAseq.csv\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ---------------- Utility: logging ----------------\n",
    "from datetime import datetime\n",
    "def log(msg: str):\n",
    "    ts = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{ts} UTC] {msg}\", flush=True)\n",
    "\n",
    "# ---------------- IO helpers for telemetry/radiation ----------------\n",
    "def _load_preprocessed(rr: str, kind: str) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        if kind == \"telemetry\":\n",
    "            if \"preprocessed_telemetry\" in globals() and rr in preprocessed_telemetry:\n",
    "                return preprocessed_telemetry[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\"\n",
    "        else:\n",
    "            if \"cleaned_radiation\" in globals() and rr in cleaned_radiation:\n",
    "                return cleaned_radiation[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\"\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p, index_col=0, parse_dates=True).sort_index()\n",
    "    except Exception as e:\n",
    "        log(f\"[LOAD FAIL] {rr} {kind}: {e}\")\n",
    "    return None\n",
    "\n",
    "# ---------------- Section 6 results collection ----------------\n",
    "scoreboards   = list(ANOM_DIR.glob(\"*_forecast_scoreboard.csv\"))\n",
    "rr_summaries  = list(ANOM_DIR.glob(\"**/*_forecast_summary.csv\"))\n",
    "\n",
    "dfs = []\n",
    "if scoreboards:\n",
    "    for p in scoreboards:\n",
    "        try:\n",
    "            dfs.append(pd.read_csv(p))\n",
    "        except Exception as e:\n",
    "            log(f\"[WARN] Could not read scoreboard {p.name}: {e}\")\n",
    "if rr_summaries:\n",
    "    for p in rr_summaries:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            if \"RR\" not in df.columns:\n",
    "                df[\"RR\"] = p.name.split(\"_\")[0]\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            log(f\"[WARN] Could not read summary {p.name}: {e}\")\n",
    "\n",
    "if not dfs:\n",
    "    raise RuntimeError(\"No Section 6 results found in outputs. Run Section 6 first.\")\n",
    "\n",
    "metrics_all = pd.concat(dfs, ignore_index=True)\n",
    "metrics_all[\"RR\"] = metrics_all[\"RR\"].astype(str)\n",
    "(INSIGHTS_DIR / \"all_missions_metrics_raw.csv\").parent.mkdir(parents=True, exist_ok=True)\n",
    "metrics_all.to_csv(INSIGHTS_DIR / \"all_missions_metrics_raw.csv\", index=False)\n",
    "log(f\"[Sec7] Loaded metrics rows: {len(metrics_all)}\")\n",
    "\n",
    "# ---------------- Seasonality (optional) ----------------\n",
    "seas_path = PATTERN_DIR / \"seasonality_summary.csv\"\n",
    "seasonality_df = pd.read_csv(seas_path) if seas_path.exists() else None\n",
    "\n",
    "telemetry_cols = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_cols = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "\n",
    "def _mission_volatility(rr: str) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for kind, cols in [(\"telemetry\", telemetry_cols), (\"radiation\", radiation_cols)]:\n",
    "        df = _load_preprocessed(rr, kind)\n",
    "        if df is None: \n",
    "            continue\n",
    "        for c in cols:\n",
    "            if c in df.columns:\n",
    "                s = df[c].dropna()\n",
    "                out.append({\"RR\": rr, \"Type\": kind, \"Variable\": c, \"Std\": float(s.std())})\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "def _anom_flags(series: pd.Series, z_thr=3.0, ewma_span=60) -> pd.DataFrame:\n",
    "    s = series.dropna()\n",
    "    if len(s) == 0 or s.std() == 0: \n",
    "        return pd.DataFrame(columns=[\"timestamp\",\"atype\"])\n",
    "    z = (s - s.mean())/s.std()\n",
    "    z_mask = z.abs() > z_thr\n",
    "    ew = s.ewm(span=ewma_span).mean()\n",
    "    resid = s - ew\n",
    "    ew_mask = (resid.abs() > (2*resid.std()))\n",
    "    out = []\n",
    "    if z_mask.any():   out.extend([(t, \"z\") for t in s.index[z_mask]])\n",
    "    if ew_mask.any():  out.extend([(t, \"ewma\") for t in s.index[ew_mask]])\n",
    "    if not out:\n",
    "        return pd.DataFrame(columns=[\"timestamp\",\"atype\"])\n",
    "    df = pd.DataFrame(out, columns=[\"timestamp\",\"atype\"]).drop_duplicates().sort_values(\"timestamp\")\n",
    "    return df\n",
    "\n",
    "def _mission_anomaly_calendar(rr: str) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for kind, cols in [(\"telemetry\", telemetry_cols), (\"radiation\", radiation_cols)]:\n",
    "        df = _load_preprocessed(rr, kind)\n",
    "        if df is None: \n",
    "            continue\n",
    "        for c in cols:\n",
    "            if c not in df.columns: \n",
    "                continue\n",
    "            y = df[c].dropna()\n",
    "            if len(y) == 0 or y.std() == 0:\n",
    "                continue\n",
    "            y = y.resample(S.sampling).mean()\n",
    "            flags = _anom_flags(y, z_thr=3.0, ewma_span=60)\n",
    "            if len(flags):\n",
    "                flags[\"RR\"] = rr\n",
    "                flags[\"Variable\"] = c\n",
    "                rows.append(flags)\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"timestamp\",\"atype\",\"RR\",\"Variable\"])\n",
    "    cal = pd.concat(rows, ignore_index=True).drop_duplicates().sort_values(\"timestamp\")\n",
    "    return cal\n",
    "\n",
    "# Build volatility & anomaly-calendars\n",
    "vol_tabs, calendars = [], []\n",
    "for rr in tqdm(S.missions, desc=\"Volatility/Calendar\"):\n",
    "    vt = _mission_volatility(rr)\n",
    "    if len(vt): vol_tabs.append(vt)\n",
    "    cal = _mission_anomaly_calendar(rr)\n",
    "    if len(cal): calendars.append(cal.to_dict(\"records\"))\n",
    "\n",
    "vol_df = pd.concat(vol_tabs, ignore_index=True) if vol_tabs else pd.DataFrame(columns=[\"RR\",\"Type\",\"Variable\",\"Std\"])\n",
    "cal_all = pd.DataFrame([r for mission in calendars for r in mission]) if calendars else pd.DataFrame(columns=[\"timestamp\",\"atype\",\"RR\",\"Variable\"])\n",
    "vol_df.to_csv(INSIGHTS_DIR / \"volatility_by_variable.csv\", index=False)\n",
    "cal_all.to_csv(INSIGHTS_DIR / \"anomaly_calendar_all_missions.csv\", index=False)\n",
    "\n",
    "# ---------------- Event overlap (optional, if events CSVs exist) ----------------\n",
    "def _scan_event_table(rr: str) -> Optional[pd.DataFrame]:\n",
    "    candidates = list((S.outputs_root / \"events\").glob(f\"*{rr}*events*.csv\"))\n",
    "    for root in S.data_roots:\n",
    "        root = Path(root)\n",
    "        candidates.extend(root.glob(f\"*{rr}*events*.csv\"))\n",
    "        candidates.extend(root.glob(\"events/*events*.csv\"))\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            start = df[cols.get(\"start\", next((c for c in df.columns if \"start\" in c.lower()), None))]\n",
    "            end   = df[cols.get(\"end\",   next((c for c in df.columns if \"end\"   in c.lower()), None))]\n",
    "            labc  = cols.get(\"label\", next((c for c in df.columns if \"label\" in c.lower() or \"event\" in c.lower()), None))\n",
    "            lab = df[labc] if labc else pd.Series([\"event\"]*len(df))\n",
    "            ev = pd.DataFrame({\n",
    "                \"start\": pd.to_datetime(start, utc=True, errors=\"coerce\"),\n",
    "                \"end\":   pd.to_datetime(end,   utc=True, errors=\"coerce\"),\n",
    "                \"label\": lab.astype(str)\n",
    "            }).dropna()\n",
    "            if len(ev):\n",
    "                return ev.sort_values(\"start\")\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "overlap_rows = []\n",
    "for rr in cal_all[\"RR\"].unique():\n",
    "    ev = _scan_event_table(rr)\n",
    "    if ev is None: \n",
    "        continue\n",
    "    c = cal_all[cal_all[\"RR\"] == rr].copy()\n",
    "    for _, row in c.iterrows():\n",
    "        t = pd.to_datetime(row[\"timestamp\"], utc=True)\n",
    "        hit = ev[(ev[\"start\"] <= t) & (t <= ev[\"end\"])]\n",
    "        if len(hit):\n",
    "            for _, h in hit.iterrows():\n",
    "                overlap_rows.append({\"RR\": rr, \"timestamp\": t, \"Variable\": row[\"Variable\"],\n",
    "                                     \"atype\": row[\"atype\"], \"event\": h[\"label\"]})\n",
    "if overlap_rows:\n",
    "    overlap_df = pd.DataFrame(overlap_rows).sort_values([\"RR\",\"timestamp\"])\n",
    "    overlap_df.to_csv(INSIGHTS_DIR / \"anomaly_event_overlaps.csv\", index=False)\n",
    "    log(f\"[Sec7] Event overlap rows: {len(overlap_df)}\")\n",
    "else:\n",
    "    log(\"[Sec7] No event context found; overlap skipped.\")\n",
    "\n",
    "# ---------------- Cross-mission summaries ----------------\n",
    "if len(cal_all):\n",
    "    rate = cal_all.groupby(\"RR\")[\"timestamp\"].nunique().rename(\"Anomaly_Timestamps\").reset_index()\n",
    "    norm_minutes = []\n",
    "    for rr in rate[\"RR\"]:\n",
    "        mins = 0\n",
    "        for kind in (\"telemetry\",\"radiation\"):\n",
    "            df = _load_preprocessed(rr, kind)\n",
    "            if df is None or len(df) == 0: \n",
    "                continue\n",
    "            i = df.index.sort_values()\n",
    "            mins = max(mins, int((i.max() - i.min()).total_seconds() // 60))\n",
    "        norm_minutes.append({\"RR\": rr, \"Minutes\": max(1, mins)})\n",
    "    norm_df = pd.DataFrame(norm_minutes)\n",
    "    rate = rate.merge(norm_df, on=\"RR\", how=\"left\")\n",
    "    rate[\"Anomaly_per_10kmin\"] = 1e4 * rate[\"Anomaly_Timestamps\"] / rate[\"Minutes\"].clip(lower=1)\n",
    "    q_hi = rate[\"Anomaly_per_10kmin\"].quantile(0.67)\n",
    "    q_lo = rate[\"Anomaly_per_10kmin\"].quantile(0.33)\n",
    "    def _lab(x):\n",
    "        if x >= q_hi: return \"anomalous\"\n",
    "        if x <= q_lo: return \"stable\"\n",
    "        return \"middle\"\n",
    "    rate[\"Group\"] = rate[\"Anomaly_per_10kmin\"].apply(_lab)\n",
    "    rate.to_csv(INSIGHTS_DIR / \"mission_anomaly_rates.csv\", index=False)\n",
    "\n",
    "    err_m = metrics_all.groupby([\"RR\",\"Variable\"], as_index=False)[[\"RMSE\",\"MAE\"]].mean()\n",
    "    vol_m = vol_df[[\"RR\",\"Variable\",\"Std\"]]\n",
    "    comp = (err_m.merge(vol_m, on=[\"RR\",\"Variable\"], how=\"left\")\n",
    "                  .merge(rate[[\"RR\",\"Group\"]], on=\"RR\", how=\"left\"))\n",
    "    comp.to_csv(INSIGHTS_DIR / \"group_comparison_metrics.csv\", index=False)\n",
    "else:\n",
    "    comp = pd.DataFrame()\n",
    "\n",
    "# ---- “Top” tables & plots ----\n",
    "top_forecast = (metrics_all.sort_values([\"RR\",\"RMSE\"])\n",
    "                .groupby(\"RR\", as_index=False).head(3))\n",
    "top_forecast.to_csv(INSIGHTS_DIR / \"top_forecastable_by_mission.csv\", index=False)\n",
    "\n",
    "if \"AE_MeanMSE\" in metrics_all.columns:\n",
    "    top_anom = (metrics_all.sort_values([\"RR\",\"AE_MeanMSE\"], ascending=[True, False])\n",
    "                .groupby(\"RR\", as_index=False).head(3))\n",
    "    top_anom.to_csv(INSIGHTS_DIR / \"top_anomalous_by_mission.csv\", index=False)\n",
    "\n",
    "if seasonality_df is not None and not seasonality_df.empty:\n",
    "    seas_keep = seasonality_df.rename(columns={\"Mission\":\"RR\",\n",
    "                                               \"Seasonality_Strength\":\"Seasonality_Strength\"})\n",
    "    best_seasonal = (seas_keep.sort_values([\"RR\",\"Seasonality_Strength\"], ascending=[True, False])\n",
    "                             .groupby(\"RR\", as_index=False).head(3))\n",
    "    best_seasonal.to_csv(INSIGHTS_DIR / \"top_seasonality_by_mission.csv\", index=False)\n",
    "\n",
    "# ---------------- OMICS helpers ----------------\n",
    "def _safe_read_csv(p: Optional[str]) -> Optional[pd.DataFrame]:\n",
    "    if p is None: return None\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=\"utf-8\", engine=\"python\")\n",
    "        except Exception as e:\n",
    "            log(f\"[OMICS WARN] Could not read {p}: {e}\")\n",
    "            return None\n",
    "\n",
    "def _first_existing(paths: List[str]) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if p and Path(p).exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _pick_contrast_suffix(cols: List[str], contrasts_df: Optional[pd.DataFrame]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    low = [c.lower() for c in cols]\n",
    "    def _find_suffix(keywords):\n",
    "        for c in cols:\n",
    "            cl = c.lower()\n",
    "            if all(k in cl for k in keywords):\n",
    "                if '.' in c:\n",
    "                    return c[c.rfind('.'):]  # \".FLT_vs_GC\"\n",
    "                return \"\"                   # already specific\n",
    "        return None\n",
    "    for kws in ([\"flight\",\"ground\"], [\"flt\",\"gc\"], [\"space\",\"ground\"]):\n",
    "        suf = _find_suffix(kws)\n",
    "        if suf is not None:\n",
    "            return suf, suf\n",
    "    if contrasts_df is not None and len(contrasts_df.columns):\n",
    "        suf = _find_suffix([\"flt\",\"gc\"]) or _find_suffix([\"flight\",\"ground\"])\n",
    "        return suf, suf\n",
    "    return None, None\n",
    "\n",
    "def _normalize_deg_columns(df: pd.DataFrame, contrasts_df: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    d = df.copy()\n",
    "    d.columns = (d.columns\n",
    "                   .str.strip()\n",
    "                   .str.replace(\"\\u200b\",\"\", regex=False)\n",
    "                   .str.replace(\"\\xa0\",\"\", regex=False))\n",
    "    cols = list(d.columns)\n",
    "\n",
    "    # gene column\n",
    "    gene_cands = [c for c in cols if c.lower() in (\"symbol\",\"gene\",\"genes\",\"gene_symbol\",\"gene name\",\"gene_id\",\"ensembl\",\"ensembl_gene_id\")]\n",
    "    gene_col = gene_cands[0] if gene_cands else cols[0]\n",
    "\n",
    "    # padj/lfc candidates\n",
    "    padj_like = [c for c in cols if (\"padj\" in c.lower()) or (\"fdr\" in c.lower()) or (\"qval\" in c.lower()) or (\"adj.p.value\" in c.lower()) or (\"fdr p-value\" in c.lower())]\n",
    "    lfc_like  = [c for c in cols if (\"log2\" in c.lower() and (\"fc\" in c.lower() or \"fold\" in c.lower())) or (\"logfc\" in c.lower()) or (\"estimate\" in c.lower())]\n",
    "\n",
    "    suf_p, suf_l = _pick_contrast_suffix(cols, contrasts_df)\n",
    "\n",
    "    padj_col = None\n",
    "    lfc_col  = None\n",
    "\n",
    "    if suf_p or suf_l:\n",
    "        for c in padj_like:\n",
    "            if suf_p and c.endswith(suf_p): padj_col = c; break\n",
    "        for c in lfc_like:\n",
    "            if suf_l and c.endswith(suf_l): lfc_col = c; break\n",
    "\n",
    "    def _prefer(cols_list, prefer_keywords):\n",
    "        ranked = []\n",
    "        for c in cols_list:\n",
    "            score = sum(k in c.lower() for k in prefer_keywords)\n",
    "            ranked.append((score, c))\n",
    "        ranked.sort(reverse=True)\n",
    "        return ranked[0][1] if ranked else None\n",
    "\n",
    "    if padj_col is None:\n",
    "        padj_col = _prefer(padj_like, [\"padj\",\"fdr\",\"qval\",\"adj\",\"fdr p-value\",\"adjusted\"])\n",
    "    if lfc_col is None:\n",
    "        lfc_col = _prefer(lfc_like, [\"log2foldchange\",\"log2fc\",\"logfc\",\"fold\",\"estimate\"])\n",
    "\n",
    "    keep = [gene_col] + [c for c in [padj_col, lfc_col] if c is not None]\n",
    "    d = d[keep].copy()\n",
    "\n",
    "    rename_map = {gene_col: \"gene\"}\n",
    "    if padj_col: rename_map[padj_col] = \"padj\"\n",
    "    if lfc_col:  rename_map[lfc_col]  = \"log2FoldChange\"\n",
    "    d = d.rename(columns=rename_map)\n",
    "\n",
    "    for c in [\"padj\",\"log2FoldChange\"]:\n",
    "        if c in d.columns:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "\n",
    "    if (\"log2FoldChange\" not in d.columns) and (\"padj\" not in d.columns):\n",
    "        return None\n",
    "    return d\n",
    "\n",
    "def _write_top_tables(glds: str, d: pd.DataFrame):\n",
    "    if \"log2FoldChange\" in d.columns:\n",
    "        if \"padj\" in d.columns:\n",
    "            sig = d[d[\"padj\"] < 0.05].copy()\n",
    "        else:\n",
    "            sig = d.assign(absfc=d[\"log2FoldChange\"].abs()).sort_values(\"absfc\", ascending=False).head(500).copy()\n",
    "        up   = sig.sort_values(\"log2FoldChange\", ascending=False).head(10)[[\"gene\",\"log2FoldChange\"]]\n",
    "        down = sig.sort_values(\"log2FoldChange\", ascending=True ).head(10)[[\"gene\",\"log2FoldChange\"]]\n",
    "        up.to_csv(INSIGHTS_DIR / f\"{glds}_top_upregulated.csv\", index=False)\n",
    "        down.to_csv(INSIGHTS_DIR / f\"{glds}_top_downregulated.csv\", index=False)\n",
    "    else:\n",
    "        best = d.sort_values(\"padj\").head(20)\n",
    "        best.to_csv(INSIGHTS_DIR / f\"{glds}_most_significant.csv\", index=False)\n",
    "\n",
    "def _best_sample_id_column(sample: pd.DataFrame, sample_names: List[str]) -> Optional[str]:\n",
    "    cand = None\n",
    "    best = -1\n",
    "    sset = set(map(str, sample_names))\n",
    "    for c in sample.columns:\n",
    "        vals = set(map(str, sample[c].astype(str).values))\n",
    "        score = len(sset & vals)\n",
    "        if score > best:\n",
    "            best = score; cand = c\n",
    "    return cand if best > 0 else None\n",
    "\n",
    "def _make_pca_table(glds: str, counts: pd.DataFrame, sample: Optional[pd.DataFrame]) -> Optional[pd.DataFrame]:\n",
    "    if counts is None or counts.empty:\n",
    "        return None\n",
    "    X = counts.copy()\n",
    "\n",
    "    # If a gene column survived, move it to index\n",
    "    if \"gene\" in X.columns:\n",
    "        X = X.set_index(\"gene\")\n",
    "\n",
    "    # Keep only numeric (samples should be columns)\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    if X.empty:\n",
    "        return None\n",
    "\n",
    "    # Heuristic: if columns >> rows, genes likely rows -> transpose to samples x genes\n",
    "    if X.shape[1] > X.shape[0]:\n",
    "        X = X.T\n",
    "\n",
    "    # Log1p if values large\n",
    "    if np.nanmedian(X.values) > 50:\n",
    "        X = np.log1p(X)\n",
    "\n",
    "    # Standardize features (genes)\n",
    "    X = (X - X.mean(axis=0)) / (X.std(axis=0) + 1e-8)\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    # PCA(2)\n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2, random_state=0)\n",
    "        coords = pca.fit_transform(X.values)  # samples x 2\n",
    "    except Exception:\n",
    "        # Fallback to SVD-based PCA (2 comps)\n",
    "        U, Ssvd, Vt = np.linalg.svd(X.values - X.values.mean(axis=0), full_matrices=False)\n",
    "        coords = U[:, :2] * Ssvd[:2]\n",
    "\n",
    "    out = pd.DataFrame({\"PC1\": coords[:,0], \"PC2\": coords[:,1], \"Sample\": X.index.astype(str)})\n",
    "\n",
    "    # Attach group/condition if possible\n",
    "    if sample is not None and not sample.empty:\n",
    "        s = sample.copy()\n",
    "        s.columns = s.columns.str.strip()\n",
    "        id_col = _best_sample_id_column(s, list(out[\"Sample\"]))\n",
    "        group_col = None\n",
    "        for cand in [\"Group\",\"Condition\",\"group\",\"condition\",\"SampleGroup\",\"Treatment\",\"Sample_Group\"]:\n",
    "            if cand in s.columns:\n",
    "                group_col = cand; break\n",
    "        if id_col and group_col:\n",
    "            meta = s[[id_col, group_col]].rename(columns={id_col: \"Sample\", group_col: \"Group\"})\n",
    "            out = out.merge(meta, on=\"Sample\", how=\"left\")\n",
    "\n",
    "    out.to_csv(INSIGHTS_DIR / f\"{glds}_pca_table.csv\", index=False)\n",
    "    log(f\"[OMICS] {glds}: PCA table -> {INSIGHTS_DIR / f'{glds}_pca_table.csv'}\")\n",
    "    return out\n",
    "\n",
    "# ---------------- OMICS ingest with fallbacks ----------------\n",
    "def _omics_ingest(gl_map: Dict[str, Dict[str, Optional[str]]]) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "    out: Dict[str, Dict[str, pd.DataFrame]] = {}\n",
    "    for glds, files in gl_map.items():\n",
    "        # Resolve paths (with GLDS-104 fallbacks if needed)\n",
    "        f_diff   = files.get(\"diff\")\n",
    "        f_counts = files.get(\"counts\")\n",
    "        f_sample = files.get(\"sample\")\n",
    "        f_contr  = files.get(\"contrasts\")\n",
    "\n",
    "        if glds == \"GLDS-104\":\n",
    "            if not (f_diff and Path(f_diff).exists()):\n",
    "                f_diff = _first_existing(GLDS_104_ALTS[\"diff\"])\n",
    "            if not (f_counts and Path(f_counts).exists()):\n",
    "                f_counts = _first_existing(GLDS_104_ALTS[\"counts\"])\n",
    "            if not (f_sample and Path(f_sample).exists()):\n",
    "                f_sample = _first_existing(GLDS_104_ALTS[\"sample\"])\n",
    "            if not (f_contr and Path(f_contr).exists()):\n",
    "                f_contr = _first_existing(GLDS_104_ALTS[\"contrasts\"])\n",
    "\n",
    "        diff   = _safe_read_csv(f_diff)\n",
    "        counts = _safe_read_csv(f_counts)\n",
    "        sample = _safe_read_csv(f_sample)\n",
    "        contr  = _safe_read_csv(f_contr)\n",
    "\n",
    "        pack: Dict[str, pd.DataFrame] = {}\n",
    "        if diff is not None:\n",
    "            pack[\"diff_raw\"] = diff.copy()\n",
    "            dnorm = _normalize_deg_columns(diff, contr)\n",
    "            if dnorm is not None:\n",
    "                pack[\"diff\"] = dnorm\n",
    "        if counts is not None:\n",
    "            c = counts.copy()\n",
    "            # promote a \"gene\" column to index if present\n",
    "            gcol = next((col for col in c.columns if col.lower() in {\"gene\",\"symbol\",\"genes\"}), None)\n",
    "            if gcol:\n",
    "                c = c.set_index(gcol)\n",
    "            pack[\"counts\"] = c\n",
    "        if sample is not None:\n",
    "            pack[\"sample\"] = sample\n",
    "        if contr is not None:\n",
    "            pack[\"contrasts\"] = contr\n",
    "\n",
    "        out[glds] = pack\n",
    "        have = \", \".join(pack.keys()) if pack else \"none\"\n",
    "        log(f\"[Sec7] OMICS loaded {glds}: {have}\")\n",
    "    return out\n",
    "\n",
    "omics_data = _omics_ingest(GLDS_FILES)  # global; later sections can reuse\n",
    "\n",
    "# ---------------- OMICS exports (tops + PCA) ----------------\n",
    "omics_out_rows = []\n",
    "for glds, dd in omics_data.items():\n",
    "    if \"diff\" not in dd: \n",
    "        log(f\"[OMICS] {glds}: Could not resolve DEG columns; check headers.\")\n",
    "    else:\n",
    "        d = dd[\"diff\"]\n",
    "        _write_top_tables(glds, d)\n",
    "        n_sig = int((d[\"padj\"] < 0.05).sum()) if \"padj\" in d.columns else int(len(d))\n",
    "        omics_out_rows.append({\"Dataset\": glds, \"Rows_in_DEG\": int(len(d)), \"DEGs_significant_q<0.05\": n_sig})\n",
    "\n",
    "    if \"counts\" in dd:\n",
    "        _make_pca_table(glds, dd[\"counts\"], dd.get(\"sample\"))\n",
    "\n",
    "if omics_out_rows:\n",
    "    pd.DataFrame(omics_out_rows).to_csv(INSIGHTS_DIR / \"omics_datasets_summary.csv\", index=False)\n",
    "\n",
    "# ---------------- Simple ORA if a GMT exists ----------------\n",
    "def _find_gmt() -> Optional[Path]:\n",
    "    candidates = []\n",
    "    for root in (list(S.omics_roots) + [S.outputs_root, S.outputs_root/\"pathways\"]):\n",
    "        root = Path(root)\n",
    "        for p in [root/\"gene_sets.gmt\", root/\"pathways.gmt\"]:\n",
    "            if p.exists(): candidates.append(p)\n",
    "        candidates.extend(list(root.glob(\"**/*.gmt\")))\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "def _load_gmt(p: Path) -> Dict[str, set]:\n",
    "    gs = {}\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) >= 3:\n",
    "                gs[parts[0]] = set([g for g in parts[2:] if g])\n",
    "    return gs\n",
    "\n",
    "def _ora(sig_genes: set, universe: set, gene_sets: Dict[str, set], topn=20) -> pd.DataFrame:\n",
    "    try:\n",
    "        from scipy.stats import fisher_exact\n",
    "        fisher_ok = True\n",
    "    except Exception:\n",
    "        fisher_ok = False\n",
    "    rows = []\n",
    "    for term, members in gene_sets.items():\n",
    "        a = len(sig_genes & members)\n",
    "        b = len(sig_genes - members)\n",
    "        c = len((universe & members) - sig_genes)\n",
    "        d = len(universe - members - sig_genes)\n",
    "        p = 1.0\n",
    "        if fisher_ok:\n",
    "            try:\n",
    "                _, p = __import__(\"scipy.stats\").stats.fisher_exact([[a, b],[c, d]], alternative=\"greater\")\n",
    "            except Exception:\n",
    "                p = 1.0/(1+a)\n",
    "        else:\n",
    "            p = 1.0/(1+a)\n",
    "        rows.append({\"term\": term, \"overlap\": a, \"pvalue\": p})\n",
    "    return pd.DataFrame(rows).sort_values(\"pvalue\").head(topn)\n",
    "\n",
    "gmt = _find_gmt()\n",
    "if gmt:\n",
    "    log(f\"[Sec7] Pathways GMT: {gmt}\")\n",
    "    gs = _load_gmt(gmt)\n",
    "    for glds, dd in omics_data.items():\n",
    "        d = dd.get(\"diff\")\n",
    "        if d is None or \"gene\" not in d.columns:\n",
    "            continue\n",
    "        if \"padj\" in d.columns:\n",
    "            sig = set(d.loc[d[\"padj\"] < 0.05, \"gene\"].dropna().astype(str))\n",
    "        else:\n",
    "            if \"log2FoldChange\" not in d.columns: \n",
    "                continue\n",
    "            sig = set(d.assign(absfc=d[\"log2FoldChange\"].abs())\n",
    "                        .sort_values(\"absfc\", ascending=False).head(200)[\"gene\"].astype(str))\n",
    "        universe = set(d[\"gene\"].dropna().astype(str))\n",
    "        ora = _ora(sig, universe, gs, topn=20)\n",
    "        ora.to_csv(INSIGHTS_DIR / f\"{glds}_pathway_enrichment.csv\", index=False)\n",
    "else:\n",
    "    log(\"[Sec7] No GMT pathway file found; enrichment skipped.\")\n",
    "\n",
    "# ---------------- Concise mission narratives ----------------\n",
    "def _mission_narrative(rr: str) -> str:\n",
    "    parts = [f\"# {rr} — Mission Insights\"]\n",
    "    mins = 0\n",
    "    for k in (\"telemetry\",\"radiation\"):\n",
    "        df = _load_preprocessed(rr, k)\n",
    "        if df is not None and len(df):\n",
    "            i = df.index.sort_values()\n",
    "            mins = max(mins, int((i.max() - i.min()).total_seconds() // 60))\n",
    "    parts.append(f\"- Duration (approx): {mins:,} minutes\")\n",
    "    tf = top_forecast[top_forecast[\"RR\"] == rr]\n",
    "    if len(tf):\n",
    "        v = tf.sort_values(\"RMSE\").iloc[0]\n",
    "        parts.append(f\"- Best forecasted variable: {v['Variable']} (RMSE={v['RMSE']:.2f}, MAE={v['MAE']:.2f})\")\n",
    "    if \"AE_MeanMSE\" in metrics_all.columns:\n",
    "        ta = metrics_all[metrics_all[\"RR\"] == rr]\n",
    "        if len(ta):\n",
    "            mv = ta.sort_values(\"AE_MeanMSE\", ascending=False).iloc[0]\n",
    "            parts.append(f\"- Strongest anomaly signal: {mv['Variable']} (AE_MeanMSE={mv['AE_MeanMSE']:.3f})\")\n",
    "    if seasonality_df is not None:\n",
    "        sd = seasonality_df[seasonality_df[\"Mission\"] == rr]\n",
    "        if len(sd):\n",
    "            srow = sd.sort_values(\"Seasonality_Strength\", ascending=False).iloc[0]\n",
    "            parts.append(f\"- Strongest seasonality: {srow['Variable']} (strength={srow['Seasonality_Strength']:.3f})\")\n",
    "    if 'rate' in locals() and rr in rate[\"RR\"].values:\n",
    "        g = rate.loc[rate[\"RR\"] == rr, \"Group\"].values[0]\n",
    "        apr = rate.loc[rate[\"RR\"] == rr, \"Anomaly_per_10kmin\"].values[0]\n",
    "        parts.append(f\"- Anomaly burden: {g} (≈{apr:.1f} timestamps per 10k minutes)\")\n",
    "    return \"\\n\".join(parts) + \"\\n\"\n",
    "\n",
    "for rr in S.missions:\n",
    "    (INSIGHTS_DIR / f\"{rr}_narrative.md\").write_text(_mission_narrative(rr))\n",
    "\n",
    "# ---------------- Dashboard manifest scaffold ----------------\n",
    "artifact = {\n",
    "    \"missions\": S.missions,\n",
    "    \"scoreboard_csv\": str((INSIGHTS_DIR / \"all_missions_metrics_raw.csv\").resolve()),\n",
    "    \"volatility_csv\": str((INSIGHTS_DIR / \"volatility_by_variable.csv\").resolve()),\n",
    "    \"anomaly_calendar_csv\": str((INSIGHTS_DIR / \"anomaly_calendar_all_missions.csv\").resolve()),\n",
    "    \"anomaly_rates_csv\": str((INSIGHTS_DIR / \"mission_anomaly_rates.csv\").resolve()) if len(cal_all) else None,\n",
    "    \"group_comparison_csv\": str((INSIGHTS_DIR / \"group_comparison_metrics.csv\").resolve()) if 'comp' in locals() and len(comp) else None,\n",
    "    \"top_forecast_csv\": str((INSIGHTS_DIR / \"top_forecastable_by_mission.csv\").resolve()),\n",
    "    \"top_anomalous_csv\": str((INSIGHTS_DIR / \"top_anomalous_by_mission.csv\").resolve()) if \"AE_MeanMSE\" in metrics_all.columns else None,\n",
    "}\n",
    "with open(INSIGHTS_DIR / \"insights_manifest.json\", \"w\") as f:\n",
    "    json.dump(artifact, f, indent=2)\n",
    "\n",
    "print(\"\\n[✓] Section 7 complete.\")\n",
    "print(f\"Outputs → {INSIGHTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e758ad3",
   "metadata": {},
   "source": [
    "### Section 8: Prescriptive Insights\n",
    "#### Purpose\n",
    "Translate analytics into decision aids for operations: alert thresholds, scheduling guidance, and sampling triggers.\n",
    "#### What is implemented\n",
    "1. Preliminary thresholds for CO₂/RH/Temp grounded in anomaly behavior and forecast accuracy.\n",
    "2. Radiation-aware scheduling guidance (SAA variability, cumulative dose).\n",
    "3. Triggers for omics/physiological sampling aligned to anomaly criteria.\n",
    "4. Policy export to JSON and optional compliance summaries for the dashboard.\n",
    "#### Future Work\n",
    "- Mark prescriptions as preliminary and revise after cross-mission Seasonal ARIMA benchmarking.\n",
    "- Add placeholders for LSDA physiological endpoints once access is available.\n",
    "- Calibrate thresholds using ROC curves once a labeled set exists.\n",
    "- Unit-tested utilities to compute rolling compliance against the policy per mission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813d170e-c6e5-401e-abd4-e0f8ee315505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Policy saved → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/policy/prescriptive_policy.json\n",
      "[✓] Policy preview → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/policy/policy_preview.csv\n",
      "[✓] Compliance summary rows: 24\n",
      "[i] No labels found (or not useful for ROC); kept percentile thresholds.\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 8: Prescriptive Insights ====\n",
    "import os, json, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Settings bootstrap ----------\n",
    "if \"S\" not in globals():\n",
    "    from dataclasses import dataclass, field\n",
    "    @dataclass\n",
    "    class Settings:\n",
    "        missions: list[str] = field(default_factory=lambda: [\"RR-1\",\"RR-3\",\"RR-6\",\"RR-9\",\"RR-12\",\"RR-19\"])\n",
    "        sampling: str = \"1min\"\n",
    "        outputs_root: Path = field(default_factory=lambda: Path.cwd() / \"outputs\")\n",
    "        preprocessed_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"preprocessed\")\n",
    "        anomalies_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"anomaly_forecast\")\n",
    "    S = Settings()\n",
    "    for d in [S.outputs_root, S.preprocessed_dir, S.anomalies_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[Sec8 bootstrap] Using fallback Settings (S).\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "POLICY_DIR = S.outputs_root / \"policy\"\n",
    "COMPLIANCE_DIR = POLICY_DIR / \"compliance\"\n",
    "PLOTS_DIR = POLICY_DIR / \"plots\"\n",
    "INSIGHTS_DIR = S.outputs_root / \"insights\"\n",
    "for d in [POLICY_DIR, COMPLIANCE_DIR, PLOTS_DIR, INSIGHTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "telemetry_cols = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_cols = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _load_mission(kind: str, rr: str) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        if kind == \"telemetry\":\n",
    "            if \"preprocessed_telemetry\" in globals() and rr in preprocessed_telemetry:\n",
    "                return preprocessed_telemetry[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_telemetry.csv\"\n",
    "        else:\n",
    "            if \"cleaned_radiation\" in globals() and rr in cleaned_radiation:\n",
    "                return cleaned_radiation[rr].copy()\n",
    "            p = S.preprocessed_dir / f\"{rr}_cleaned_radiation.csv\"\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p, index_col=0, parse_dates=True).sort_index()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _q(s: pd.Series, q): \n",
    "    s = s.dropna() if s is not None else pd.Series(dtype=float)\n",
    "    return float(np.nanpercentile(s.values, q)) if len(s) else np.nan\n",
    "\n",
    "def _hour_pref_list(co2_series: pd.Series) -> list[int]:\n",
    "    \"\"\"Return hours (UTC) when CO2 is characteristically low.\"\"\"\n",
    "    if co2_series is None or co2_series.empty:\n",
    "        return []\n",
    "    x = co2_series.dropna().resample(S.sampling).mean()\n",
    "    if x.empty: return []\n",
    "    h = x.groupby(x.index.hour).mean()\n",
    "    if h.empty: return []\n",
    "    thr = h.median() - 0.5*h.std(ddof=0)\n",
    "    hours = [int(k) for k, v in h.items() if v <= thr]\n",
    "    return sorted(hours)\n",
    "\n",
    "def _load_metrics_all() -> pd.DataFrame | None:\n",
    "    p = INSIGHTS_DIR / \"all_missions_metrics_raw.csv\"  # written by Section 7\n",
    "    if p.exists():\n",
    "        try: return pd.read_csv(p)\n",
    "        except Exception: pass\n",
    "    # fallback: look for per-mission summaries\n",
    "    rows = []\n",
    "    for rr in S.missions:\n",
    "        p2 = S.anomalies_dir / f\"{rr}_forecast_summary.csv\"\n",
    "        if p2.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(p2)\n",
    "                if \"RR\" not in df.columns: df[\"RR\"] = rr\n",
    "                rows.append(df)\n",
    "            except Exception: pass\n",
    "    return pd.concat(rows, ignore_index=True) if rows else None\n",
    "\n",
    "def _ae_ref_value(rr: str, var: str, default=0.95) -> float:\n",
    "    \"\"\"\n",
    "    Pull a reference anomaly score from metrics:\n",
    "    If AE_MeanMSE exists, use the 95th percentile of that metric for (rr,var),\n",
    "    else return a default quantile value for downstream logic.\n",
    "    \"\"\"\n",
    "    df = _load_metrics_all()\n",
    "    if df is None: return default\n",
    "    need = {\"RR\",\"Variable\",\"AE_MeanMSE\"}\n",
    "    if not need.issubset(df.columns): return default\n",
    "    sub = df[(df[\"RR\"].astype(str)==str(rr)) & (df[\"Variable\"].astype(str)==str(var))]\n",
    "    if not len(sub): return default\n",
    "    try:\n",
    "        return float(sub[\"AE_MeanMSE\"].quantile(0.95))\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "# ---------- Build per-mission policy ----------\n",
    "def _policy_for_mission(rr: str) -> dict:\n",
    "    tel = _load_mission(\"telemetry\", rr)\n",
    "    rad = _load_mission(\"radiation\", rr)\n",
    "    pol = {\"thresholds\": {}, \"radiation\": {}, \"sampling_triggers\": {}, \"forecasting\": {}, \"scheduling\": {}}\n",
    "\n",
    "    # Telemetry thresholds & scheduling prefs\n",
    "    if tel is not None and len(tel):\n",
    "        if \"CO2_ppm_ISS\" in tel.columns:\n",
    "            co2 = tel[\"CO2_ppm_ISS\"].resample(S.sampling).mean()\n",
    "            pol[\"thresholds\"][\"CO2_ppm_ISS\"] = {\"hi\": _q(co2, 95)}\n",
    "            pol[\"scheduling\"][\"prefer_hours_utc_low_CO2\"] = _hour_pref_list(co2)\n",
    "        if \"Temp_degC_ISS\" in tel.columns:\n",
    "            t = tel[\"Temp_degC_ISS\"].resample(S.sampling).mean()\n",
    "            pol[\"thresholds\"][\"Temp_degC_ISS\"] = {\"lo\": _q(t, 5), \"hi\": _q(t, 95)}\n",
    "        if \"RH_percent_ISS\" in tel.columns:\n",
    "            rh = tel[\"RH_percent_ISS\"].resample(S.sampling).mean()\n",
    "            pol[\"thresholds\"][\"RH_percent_ISS\"] = {\"lo\": _q(rh, 5), \"hi\": _q(rh, 95)}\n",
    "\n",
    "    # Radiation guidance (avoid high SAA windows; monitor cumulative increases)\n",
    "    if rad is not None and len(rad):\n",
    "        if \"SAA_Dose_mGy_d\" in rad.columns:\n",
    "            saa = rad[\"SAA_Dose_mGy_d\"].resample(S.sampling).ffill()\n",
    "            pol[\"radiation\"][\"avoid_when\"] = {\"SAA_Dose_mGy_d\": {\"hi\": _q(saa, 90), \"sustain_minutes\": 30}}\n",
    "        if \"Accumulated_Dose_mGy_d\" in rad.columns:\n",
    "            acc_delta = rad[\"Accumulated_Dose_mGy_d\"].resample(\"60min\").max().diff()\n",
    "            pol[\"radiation\"][\"dose_monitor\"] = {\"Accumulated_Dose_mGy_d_delta\": {\"window_min\": 1440, \"hi\": _q(acc_delta, 95)}}\n",
    "\n",
    "    # Sampling triggers (aligned to anomaly detection suite)\n",
    "    pol[\"sampling_triggers\"] = {\n",
    "        \"omics\": {\"any_anomaly_minutes\": 15, \"ae_error_ref\": 0.95},  # ae_error_ref replaced below per variable\n",
    "        \"physiology_placeholders\": [\"corticosterone\", \"bone_density\", \"muscle_mass\"]\n",
    "    }\n",
    "    pol[\"forecasting\"] = {\n",
    "        \"update_interval_min\": 60,\n",
    "        \"model_baselines\": [\"LSTM_univariate\", \"SARIMAX_seasonal\", \"VAR_multivariate\"]\n",
    "    }\n",
    "    return pol\n",
    "\n",
    "def build_policy() -> dict:\n",
    "    policy = {\n",
    "        \"meta\": {\n",
    "            \"version\": \"0.3.0\",\n",
    "            \"generated_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "            \"preliminary\": True,\n",
    "            \"revision_pending\": \"Cross-mission SARIMAX benchmarking\",\n",
    "            \"notes\": \"Percentile-derived bounds; tune with ROC once labeled events exist.\"\n",
    "        },\n",
    "        \"missions\": {}\n",
    "    }\n",
    "    for rr in S.missions:\n",
    "        pol_rr = _policy_for_mission(rr)\n",
    "        # Attach AE references per monitored var (used downstream by ops to set alert thresholds)\n",
    "        for v in telemetry_cols + radiation_cols:\n",
    "            pol_rr[\"sampling_triggers\"][f\"ae_ref_{v}\"] = _ae_ref_value(rr, v, default=0.95)\n",
    "        policy[\"missions\"][rr] = pol_rr\n",
    "    return policy\n",
    "\n",
    "def save_policy(policy: dict, path: Path) -> None:\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(policy, f, indent=2)\n",
    "\n",
    "# ---------- Compliance ----------\n",
    "def compliance_report(rr: str, policy: dict) -> pd.DataFrame:\n",
    "    tel = _load_mission(\"telemetry\", rr)\n",
    "    rad = _load_mission(\"radiation\", rr)\n",
    "    if tel is None and rad is None:\n",
    "        return pd.DataFrame(columns=[\"timestamp\",\"rule\",\"ok\"])\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Telemetry bound checks\n",
    "    if tel is not None:\n",
    "        for var, rule in policy[\"missions\"][rr].get(\"thresholds\", {}).items():\n",
    "            if var not in tel.columns: \n",
    "                continue\n",
    "            s = tel[var].resample(S.sampling).mean()\n",
    "            ok = pd.Series(True, index=s.index)\n",
    "            if \"hi\" in rule and np.isfinite(rule[\"hi\"]): ok &= (s <= rule[\"hi\"])\n",
    "            if \"lo\" in rule and np.isfinite(rule[\"lo\"]): ok &= (s >= rule[\"lo\"])\n",
    "            rows.append(pd.DataFrame({\"timestamp\": s.index, \"rule\": f\"{var}_bounds\", \"ok\": ok.astype(bool)}))\n",
    "\n",
    "    # Radiation sustained-high avoidance\n",
    "    if rad is not None:\n",
    "        avoid = policy[\"missions\"][rr].get(\"radiation\", {}).get(\"avoid_when\", {})\n",
    "        for var, rule in avoid.items():\n",
    "            if var not in rad.columns: continue\n",
    "            s = rad[var].resample(S.sampling).ffill()\n",
    "            hi = rule.get(\"hi\", np.inf)\n",
    "            sustain = int(rule.get(\"sustain_minutes\", 30))\n",
    "            above = (s > hi).astype(int)\n",
    "            # Time-based rolling window (requires DatetimeIndex)\n",
    "            sustained = above.rolling(f\"{sustain}T\").sum() >= sustain\n",
    "            rows.append(pd.DataFrame({\"timestamp\": s.index, \"rule\": f\"{var}_avoid_hi\", \"ok\": (~sustained).astype(bool)}))\n",
    "\n",
    "    rep = pd.concat(rows).sort_values(\"timestamp\") if rows else pd.DataFrame(columns=[\"timestamp\",\"rule\",\"ok\"])\n",
    "    rep.to_csv(COMPLIANCE_DIR / f\"{rr}_compliance.csv\", index=False)\n",
    "    return rep\n",
    "\n",
    "def compliance_summary(policy: dict) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for rr in policy[\"missions\"].keys():\n",
    "        rep = compliance_report(rr, policy)\n",
    "        if len(rep):\n",
    "            rate = rep.groupby(\"rule\")[\"ok\"].mean().reset_index()\n",
    "            rate[\"RR\"] = rr\n",
    "            out.append(rate)\n",
    "    summ = pd.concat(out, ignore_index=True) if out else pd.DataFrame(columns=[\"RR\",\"rule\",\"ok\"])\n",
    "    if len(summ):\n",
    "        summ.to_csv(POLICY_DIR / \"compliance_summary.csv\", index=False)\n",
    "    return summ\n",
    "\n",
    "# ---------- Optional ROC calibration if labels exist ----------\n",
    "def try_calibrate_with_labels(policy: dict) -> dict:\n",
    "    try:\n",
    "        from sklearn.metrics import roc_curve\n",
    "    except Exception:\n",
    "        return policy\n",
    "\n",
    "    updated = json.loads(json.dumps(policy))\n",
    "    for rr in S.missions:\n",
    "        lab_path = S.outputs_root / \"labels\" / f\"{rr}_labels.csv\"\n",
    "        if not lab_path.exists(): \n",
    "            continue\n",
    "        try:\n",
    "            lab = pd.read_csv(lab_path, parse_dates=[\"timestamp\"])\n",
    "            lab[\"timestamp\"] = pd.to_datetime(lab[\"timestamp\"], utc=True)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        tel = _load_mission(\"telemetry\", rr)\n",
    "        if tel is None or \"CO2_ppm_ISS\" not in tel.columns: \n",
    "            continue\n",
    "\n",
    "        s = tel[\"CO2_ppm_ISS\"].resample(S.sampling).mean().dropna()\n",
    "        df = s.to_frame(\"value\").reset_index().rename(columns={\"index\":\"timestamp\"})\n",
    "        # If labels include 'variable', align to CO2; otherwise assume single-task labels\n",
    "        if \"variable\" in lab.columns:\n",
    "            lab_sub = lab[lab[\"variable\"].astype(str) == \"CO2_ppm_ISS\"].copy()\n",
    "        else:\n",
    "            lab_sub = lab.copy()\n",
    "        merged = pd.merge_asof(df.sort_values(\"timestamp\"),\n",
    "                               lab_sub.sort_values(\"timestamp\"),\n",
    "                               on=\"timestamp\", tolerance=pd.Timedelta(\"2min\"), direction=\"nearest\")\n",
    "        if \"label\" not in merged.columns:\n",
    "            continue\n",
    "        y = merged[\"label\"].fillna(0).astype(int).values\n",
    "        x = merged[\"value\"].values\n",
    "        if y.sum() == 0 or len(np.unique(y)) == 1:\n",
    "            continue\n",
    "        fpr, tpr, thr = roc_curve(y, x)\n",
    "        j = tpr - fpr\n",
    "        best_thr = thr[int(np.argmax(j))]\n",
    "        updated[\"missions\"][rr].setdefault(\"thresholds\", {}).setdefault(\"CO2_ppm_ISS\", {})[\"hi\"] = float(best_thr)\n",
    "    return updated\n",
    "\n",
    "# ---------- Policy generation ----------\n",
    "policy = build_policy()\n",
    "policy_path = POLICY_DIR / \"prescriptive_policy.json\"\n",
    "save_policy(policy, policy_path)\n",
    "print(f\"[✓] Policy saved → {policy_path}\")\n",
    "\n",
    "# Quick human-readable preview table (one row per mission)\n",
    "prev_rows = []\n",
    "for rr, body in policy[\"missions\"].items():\n",
    "    th = body.get(\"thresholds\", {})\n",
    "    rad = body.get(\"radiation\", {}).get(\"avoid_when\", {})\n",
    "    prev_rows.append({\n",
    "        \"RR\": rr,\n",
    "        \"CO2_hi\": th.get(\"CO2_ppm_ISS\",{}).get(\"hi\"),\n",
    "        \"Temp_lo\": th.get(\"Temp_degC_ISS\",{}).get(\"lo\"),\n",
    "        \"Temp_hi\": th.get(\"Temp_degC_ISS\",{}).get(\"hi\"),\n",
    "        \"RH_lo\": th.get(\"RH_percent_ISS\",{}).get(\"lo\"),\n",
    "        \"RH_hi\": th.get(\"RH_percent_ISS\",{}).get(\"hi\"),\n",
    "        \"SAA_hi\": (list(rad.values())[0][\"hi\"] if isinstance(rad, dict) and len(rad) else np.nan),\n",
    "        \"Prefer_hours_low_CO2\": \",\".join(map(str, body.get(\"scheduling\",{}).get(\"prefer_hours_utc_low_CO2\",[])))\n",
    "    })\n",
    "preview_df = pd.DataFrame(prev_rows)\n",
    "preview_csv = POLICY_DIR / \"policy_preview.csv\"\n",
    "preview_df.to_csv(preview_csv, index=False)\n",
    "print(f\"[✓] Policy preview → {preview_csv}\")\n",
    "\n",
    "# Compliance (per-mission files + summary)\n",
    "summary = compliance_summary(policy)\n",
    "print(f\"[✓] Compliance summary rows: {len(summary)}\" if len(summary) else \"[i] No compliance rows (no data found)\")\n",
    "\n",
    "# Optional calibration via labels (if present)\n",
    "policy_cal = try_calibrate_with_labels(policy)\n",
    "if json.dumps(policy_cal, sort_keys=True) != json.dumps(policy, sort_keys=True):\n",
    "    policy_cal_path = POLICY_DIR / \"prescriptive_policy_calibrated.json\"\n",
    "    save_policy(policy_cal, policy_cal_path)\n",
    "    print(f\"[✓] Calibrated policy saved → {policy_cal_path}\")\n",
    "else:\n",
    "    print(\"[i] No labels found (or not useful for ROC); kept percentile thresholds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35dc99e",
   "metadata": {},
   "source": [
    "### Section 9: Global QA & Summary Statistics\n",
    "#### Purpose\n",
    "Validate data integrity across missions and summarize whole-pipeline behavior (seasonality, causality, anomalies, forecast accuracy) to support interpretation, publication figures, and policy tuning.\n",
    "\n",
    "#### What is implemented\n",
    "1. Coverage & quality: minute-level coverage per mission×variable (`coverage_summary.csv`) and quality composition (original/interpolated/missing; `quality_counts.csv`) with coverage heatmap and per-mission stacked bars.\n",
    "2. Seasonality distribution: histogram of `Seasonality_Strength` by type and export of top-15 seasonal variables (`seasonality_top15.csv`).\n",
    "3. Directed causality (VAR–Granger): significant edge counts by mission and direction (telemetry→radiation, radiation→telemetry, within-domain) with summary table and bar chart.\n",
    "4. Forecast performance distributions: RMSE boxplot by model (LSTM, SARIMAX where present) and best-model table per (mission, variable) (`forecast_best_by_variable.csv`).\n",
    "5. Mission-level anomaly rates: Z>3 anomaly rate per day, per variable, plus aggregated mission/type bar chart (`anomaly_rate_per_day.csv`).\n",
    "6. Omics snapshot (GLDS-98/99/104): counts of significant DEGs (q<0.05) with Up/Down bar chart and export (`omics_deg_counts.csv`).\n",
    "\n",
    "#### Future Work\n",
    "- None at this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2edd422b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Section 9] Key highlights\n",
      " - Avg coverage: 99.86 %\n",
      " - Median RMSE by model: {'LSTM': 0.038}\n",
      " - Granger edges (total): 50\n",
      " - Avg anomalies/day (Z>3): 15.539\n",
      " - Omics DEG datasets: [{'GLDS': 'GLDS-98', 'Total_DEG_q<0.05': 67, 'Up': 26, 'Down': 41}, {'GLDS': 'GLDS-99', 'Total_DEG_q<0.05': 2809, 'Up': 1564, 'Down': 1245}, {'GLDS': 'GLDS-104', 'Total_DEG_q<0.05': 4931, 'Up': 2360, 'Down': 2571}]\n",
      "\n",
      "[Section 9] Wrote summaries/figures → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/section9_global_qa\n",
      "[Section 9] Tables → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/section9_global_qa/tables\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 9: Global QA & Summary Statistics (with Key Findings) ====\n",
    "import os, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ---------- Settings bootstrap ----------\n",
    "if \"S\" not in globals():\n",
    "    from dataclasses import dataclass, field\n",
    "    @dataclass\n",
    "    class Settings:\n",
    "        missions: list[str] = field(default_factory=lambda: [\"RR-1\",\"RR-3\",\"RR-6\",\"RR-9\",\"RR-12\",\"RR-19\"])\n",
    "        sampling: str = \"1min\"\n",
    "        outputs_root: Path = field(default_factory=lambda: Path.cwd() / \"outputs\")\n",
    "        preprocessed_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"preprocessed\")\n",
    "        pattern_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"pattern_analysis\")\n",
    "        relationships_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"relationships\")\n",
    "        anomalies_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"anomaly_forecast\")\n",
    "        omics_roots: list[Path] = field(default_factory=lambda: [Path(\"/Users/kennethjenkins/data/genelab\"),\n",
    "                                                                 Path.cwd()/ \"outputs\" / \"omics\"])\n",
    "    S = Settings()\n",
    "    for d in [S.outputs_root, S.preprocessed_dir, S.pattern_dir, S.relationships_dir, S.anomalies_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[Sec9 bootstrap] Using fallback Settings (S).\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "telemetry_cols = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_cols = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "all_vars = telemetry_cols + radiation_cols\n",
    "\n",
    "out_dir = S.outputs_root / \"section9_global_qa\"\n",
    "tables_dir = out_dir / \"tables\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "tables_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PRE = S.preprocessed_dir\n",
    "PAT = S.pattern_dir\n",
    "REL = S.relationships_dir\n",
    "ANOM = S.anomalies_dir\n",
    "\n",
    "# ---------- Helpers: load mission data ----------\n",
    "def _load_pre(rr: str, kind: str) -> pd.DataFrame | None:\n",
    "    p = PRE / f\"{rr}_cleaned_{kind}.csv\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return pd.read_csv(p, index_col=0, parse_dates=True).sort_index()\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        if kind == \"telemetry\" and \"preprocessed_telemetry\" in globals() and rr in preprocessed_telemetry:\n",
    "            return preprocessed_telemetry[rr].copy()\n",
    "        if kind == \"radiation\" and \"cleaned_radiation\" in globals() and rr in cleaned_radiation:\n",
    "            return cleaned_radiation[rr].copy()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _z_anomaly_rate_per_day(s: pd.Series, zthr=3.0) -> float:\n",
    "    s = s.dropna()\n",
    "    if len(s) < 100 or s.std() == 0:\n",
    "        return 0.0\n",
    "    z = (s - s.mean()) / s.std()\n",
    "    count = (z.abs() > zthr).sum()\n",
    "    days = max((s.index[-1] - s.index[0]).days + 1, 1)\n",
    "    return float(count) / days\n",
    "\n",
    "# ---------- OMICS helpers (use Sec7 state, GLDS_FILES, or search) ----------\n",
    "def _standardize_deg_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d.columns = (d.columns\n",
    "                   .str.strip()\n",
    "                   .str.replace(\"\\u200b\",\"\", regex=False)\n",
    "                   .str.replace(\"\\xa0\",\"\", regex=False))\n",
    "    if \"gene\" not in d.columns:\n",
    "        gc = next((c for c in d.columns if c.lower() in {\"gene\",\"symbol\",\"genes\"}), None)\n",
    "        if gc: d = d.rename(columns={gc: \"gene\"})\n",
    "    if \"log2FoldChange\" not in d.columns:\n",
    "        l2 = next((c for c in d.columns if (\"log2\" in c.lower() and \"fold\" in c.lower()) or c.lower() in {\"log2fc\",\"logfc\"}), None)\n",
    "        if l2: d = d.rename(columns={l2: \"log2FoldChange\"})\n",
    "    if \"padj\" not in d.columns:\n",
    "        pj = next((c for c in d.columns if c.lower() in {\"padj\",\"adj.p.val\",\"fdr\",\"fdr p-value\",\"adj.p.value_(space flight)v(ground control)\"}), None)\n",
    "        if pj: d = d.rename(columns={pj: \"padj\"})\n",
    "    return d\n",
    "\n",
    "def _omics_diff_df(glds: str) -> pd.DataFrame | None:\n",
    "    # 1) From Section 7 global 'omics_data'\n",
    "    if \"omics_data\" in globals() and isinstance(omics_data, dict):\n",
    "        dd = omics_data.get(glds, {})\n",
    "        if isinstance(dd, dict) and \"diff\" in dd and isinstance(dd[\"diff\"], pd.DataFrame):\n",
    "            return _standardize_deg_cols(dd[\"diff\"])\n",
    "    # 2) From explicit GLDS_FILES mapping if present\n",
    "    if \"GLDS_FILES\" in globals():\n",
    "        path = GLDS_FILES.get(glds, {}).get(\"diff\")\n",
    "        if path and Path(path).exists():\n",
    "            try:\n",
    "                return _standardize_deg_cols(pd.read_csv(path))\n",
    "            except Exception:\n",
    "                pass\n",
    "    # 3) Search under S.omics_roots\n",
    "    candidates = [\n",
    "        f\"{glds}_rna_seq_differential_expression_GLbulkRNAseq.csv\",\n",
    "        f\"{glds}_rna_seq_differential_expression_rRNArm_GLbulkRNAseq.csv\",\n",
    "        f\"{glds}_rna_seq_differential_expression.csv\",\n",
    "    ]\n",
    "    for root in getattr(S, \"omics_roots\", []):\n",
    "        root = Path(root)\n",
    "        for name in candidates:\n",
    "            for p in [root / \"RR-1\" / glds / name, root / name]:\n",
    "                if p.exists():\n",
    "                    try:\n",
    "                        return _standardize_deg_cols(pd.read_csv(p))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "    return None\n",
    "\n",
    "# =================== 9.1 Coverage & Quality ===================\n",
    "coverage_rows, quality_rows = [], []\n",
    "for rr in S.missions:\n",
    "    tel = _load_pre(rr, \"telemetry\")\n",
    "    rad = _load_pre(rr, \"radiation\")\n",
    "\n",
    "    for kind, df, cols in [(\"telemetry\", tel, telemetry_cols), (\"radiation\", rad, radiation_cols)]:\n",
    "        if df is None:\n",
    "            continue\n",
    "        total = len(df.index)\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            present = df[c].notna().sum()\n",
    "            cov = present / total if total > 0 else 0.0\n",
    "            qcol = f\"{c}_qflag\"\n",
    "            if qcol in df.columns:\n",
    "                vc = df[qcol].value_counts(dropna=False)\n",
    "                orig = int(vc.get(\"orig\", 0)); interp = int(vc.get(\"interp\", 0)); miss = int(vc.get(\"missing\", 0))\n",
    "            else:\n",
    "                orig = int(present); interp = 0; miss = int(total - present)\n",
    "            coverage_rows.append({\n",
    "                \"RR\": rr, \"Variable\": c, \"Type\": kind,\n",
    "                \"Total_Min\": total, \"Present_Min\": int(present),\n",
    "                \"Coverage_Pct\": float(cov * 100.0),\n",
    "                \"Interp_Pct_of_Present\": float((interp / present) * 100.0) if present else 0.0\n",
    "            })\n",
    "            quality_rows.append({\n",
    "                \"RR\": rr, \"Variable\": c, \"Type\": kind,\n",
    "                \"Orig_Min\": int(orig), \"Interp_Min\": int(interp), \"Missing_Min\": int(miss)\n",
    "            })\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_rows)\n",
    "quality_df = pd.DataFrame(quality_rows)\n",
    "coverage_df.to_csv(out_dir / \"coverage_summary.csv\", index=False)\n",
    "quality_df.to_csv(out_dir / \"quality_counts.csv\", index=False)\n",
    "\n",
    "if not coverage_df.empty:\n",
    "    piv = coverage_df.pivot_table(index=\"RR\", columns=\"Variable\", values=\"Coverage_Pct\", aggfunc=\"mean\")\n",
    "    plt.figure(figsize=(12, max(4, 0.6 * len(piv))))\n",
    "    sns.heatmap(piv, vmin=0, vmax=100, cmap=\"viridis\", annot=True, fmt=\".0f\", cbar_kws={\"label\": \"Coverage (%)\"})\n",
    "    plt.title(\"Minute-level coverage by mission × variable\")\n",
    "    plt.tight_layout(); plt.savefig(out_dir / \"fig_coverage_heatmap.png\", dpi=200); plt.close()\n",
    "\n",
    "if not quality_df.empty:\n",
    "    qsum = quality_df.copy()\n",
    "    qsum[\"Total\"] = qsum[\"Orig_Min\"] + qsum[\"Interp_Min\"] + qsum[\"Missing_Min\"]\n",
    "    qsum = qsum[qsum[\"Total\"] > 0]\n",
    "    qsum[\"Orig%\"] = 100 * qsum[\"Orig_Min\"] / qsum[\"Total\"]\n",
    "    qsum[\"Interp%\"] = 100 * qsum[\"Interp_Min\"] / qsum[\"Total\"]\n",
    "    qsum[\"Miss%\"] = 100 * qsum[\"Missing_Min\"] / qsum[\"Total\"]\n",
    "    for rr in qsum[\"RR\"].unique():\n",
    "        sub = qsum[qsum[\"RR\"] == rr]\n",
    "        if sub.empty: continue\n",
    "        idx = np.arange(len(sub))\n",
    "        plt.figure(figsize=(12, 4 + 0.2 * len(sub)))\n",
    "        plt.barh(idx, sub[\"Miss%\"], label=\"Missing\")\n",
    "        plt.barh(idx, sub[\"Interp%\"], left=sub[\"Miss%\"], label=\"Interpolated\")\n",
    "        plt.barh(idx, sub[\"Orig%\"], left=sub[\"Miss%\"] + sub[\"Interp%\"], label=\"Original\")\n",
    "        plt.yticks(idx, sub[\"Variable\"])\n",
    "        plt.xlabel(\"Percent of minutes\"); plt.title(f\"{rr} · quality composition\")\n",
    "        plt.legend(loc=\"lower right\", fontsize=8)\n",
    "        plt.tight_layout(); plt.savefig(out_dir / f\"fig_quality_{rr}.png\", dpi=200); plt.close()\n",
    "\n",
    "# =================== 9.2 Seasonality distribution ===================\n",
    "season_path = PAT / \"seasonality_summary.csv\"\n",
    "seas = pd.read_csv(season_path) if season_path.exists() else None\n",
    "if seas is not None and \"Seasonality_Strength\" in seas.columns:\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.histplot(seas, x=\"Seasonality_Strength\", hue=\"Type\", bins=30, stat=\"percent\", common_norm=False)\n",
    "    plt.title(\"Distribution of seasonality strength\"); plt.xlabel(\"Strength (0..1)\")\n",
    "    plt.tight_layout(); plt.savefig(out_dir / \"fig_seasonality_strength_hist.png\", dpi=200); plt.close()\n",
    "    top_seasonal = (seas.sort_values([\"Seasonality_Strength\",\"Seasonal_Amplitude_P95_P05\"], ascending=[False, False])\n",
    "                       .head(15)[[\"Mission\",\"Type\",\"Variable\",\"Seasonality_Strength\",\"Seasonal_Amplitude_P95_P05\"]])\n",
    "    top_seasonal.to_csv(out_dir / \"seasonality_top15.csv\", index=False)\n",
    "else:\n",
    "    top_seasonal = pd.DataFrame()\n",
    "\n",
    "# =================== 9.3 Directed-causality (VAR–Granger) ===================\n",
    "gpath = REL / \"granger_var_results.csv\"\n",
    "if gpath.exists():\n",
    "    gdf = pd.read_csv(gpath)\n",
    "    gdf[\"Significant\"] = gdf[\"Significant\"].astype(bool) if \"Significant\" in gdf.columns else False\n",
    "    if \"MinusLog10_q\" not in gdf.columns and \"qvalue\" in gdf.columns:\n",
    "        gdf[\"MinusLog10_q\"] = -np.log10(gdf[\"qvalue\"].clip(lower=1e-300))\n",
    "    gdf[\"Direction\"] = np.where(gdf[\"Source\"].isin(telemetry_cols) & gdf[\"Target\"].isin(radiation_cols), \"telemetry→radiation\",\n",
    "                         np.where(gdf[\"Source\"].isin(radiation_cols) & gdf[\"Target\"].isin(telemetry_cols), \"radiation→telemetry\",\n",
    "                                  \"within-domain\"))\n",
    "    gsum = (gdf[gdf[\"Significant\"]]\n",
    "            .groupby([\"Mission\",\"Direction\"])\n",
    "            .size().rename(\"Edges\").reset_index())\n",
    "    gsum_piv = gsum.pivot_table(index=\"Mission\", columns=\"Direction\", values=\"Edges\", fill_value=0)\n",
    "    gsum_piv.to_csv(out_dir / \"granger_edge_counts.csv\")\n",
    "    if not gsum.empty:\n",
    "        plt.figure(figsize=(12,5))\n",
    "        sns.barplot(data=gsum, x=\"Mission\", y=\"Edges\", hue=\"Direction\", errorbar=None)\n",
    "        plt.title(\"Significant Granger edges by mission and direction (q≤0.05)\")\n",
    "        plt.tight_layout(); plt.savefig(out_dir / \"fig_granger_edges.png\", dpi=200); plt.close()\n",
    "else:\n",
    "    gdf = pd.DataFrame()\n",
    "    gsum = pd.DataFrame()\n",
    "    gsum_piv = pd.DataFrame()\n",
    "\n",
    "# =================== 9.4 Forecast performance distributions ===================\n",
    "def _read_scoreboard() -> pd.DataFrame | None:\n",
    "    cand = list(ANOM.glob(\"*scoreboard*.csv\"))\n",
    "    if cand:\n",
    "        try: return pd.read_csv(cand[0])\n",
    "        except Exception: pass\n",
    "    parts=[]\n",
    "    for rr in S.missions:\n",
    "        p = ANOM / f\"{rr}_forecast_summary.csv\"\n",
    "        if p.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(p); df[\"RR\"] = rr; parts.append(df)\n",
    "            except Exception: pass\n",
    "    return pd.concat(parts, ignore_index=True) if parts else None\n",
    "\n",
    "score = _read_scoreboard()\n",
    "if score is not None and not score.empty:\n",
    "    df_score = score.copy()\n",
    "    if \"Model\" not in df_score.columns: df_score[\"Model\"] = \"LSTM\"\n",
    "    df_score = df_score.dropna(subset=[\"Variable\",\"RMSE\",\"MAE\"])\n",
    "    df_score[\"ModelShort\"] = df_score[\"Model\"].astype(str).str.replace(r\"SARIMAX.*\",\"SARIMAX\", regex=True)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.boxplot(data=df_score, x=\"ModelShort\", y=\"RMSE\")\n",
    "    plt.title(\"Forecast error by model (RMSE)\"); plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"fig_rmse_by_model.png\", dpi=200); plt.close()\n",
    "    best = (df_score.sort_values(\"RMSE\")\n",
    "              .groupby([\"RR\",\"Variable\"], as_index=False)\n",
    "              .first()[[\"RR\",\"Variable\",\"ModelShort\",\"RMSE\",\"MAE\"]])\n",
    "    best.to_csv(out_dir / \"forecast_best_by_variable.csv\", index=False)\n",
    "else:\n",
    "    df_score = pd.DataFrame()\n",
    "    best = pd.DataFrame()\n",
    "\n",
    "# =================== 9.5 Mission-level Z>3 anomaly rates ===================\n",
    "anom_rows = []\n",
    "for rr in S.missions:\n",
    "    tel = _load_pre(rr, \"telemetry\"); rad = _load_pre(rr, \"radiation\")\n",
    "    for kind, df, cols in [(\"telemetry\", tel, telemetry_cols), (\"radiation\", rad, radiation_cols)]:\n",
    "        if df is None:\n",
    "            continue\n",
    "        for c in cols:\n",
    "            if c not in df.columns:\n",
    "                continue\n",
    "            s = df[c].resample(\"1min\").mean().interpolate(limit=5)\n",
    "            rate = _z_anomaly_rate_per_day(s, zthr=3.0)\n",
    "            anom_rows.append({\"RR\": rr, \"Variable\": c, \"Type\": kind, \"Anomalies_per_day_Z3\": rate})\n",
    "\n",
    "anom_df = pd.DataFrame(anom_rows)\n",
    "anom_df.to_csv(out_dir / \"anomaly_rate_per_day.csv\", index=False)\n",
    "if not anom_df.empty:\n",
    "    agg = (anom_df.groupby([\"RR\",\"Type\"])[\"Anomalies_per_day_Z3\"]\n",
    "                 .mean().rename(\"Anoms/day\").reset_index())\n",
    "    plt.figure(figsize=(12,5))\n",
    "    sns.barplot(data=agg, x=\"RR\", y=\"Anoms/day\", hue=\"Type\", errorbar=None)\n",
    "    plt.title(\"Average Z>3 anomaly rate (per day)\"); plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"fig_anomaly_rate_by_mission.png\", dpi=200); plt.close()\n",
    "\n",
    "# =================== 9.6 Omics DEG counts (GLDS-98/99/104) ===================\n",
    "def _omics_deg_counts(glds: str) -> dict | None:\n",
    "    df = _omics_diff_df(glds)\n",
    "    if df is None:\n",
    "        return None\n",
    "    df = _standardize_deg_cols(df)\n",
    "    if \"padj\" not in df.columns or \"log2FoldChange\" not in df.columns:\n",
    "        return None\n",
    "    sig = df[df[\"padj\"] < 0.05].copy()\n",
    "    up = (sig[\"log2FoldChange\"] > 0).sum()\n",
    "    down = (sig[\"log2FoldChange\"] < 0).sum()\n",
    "    # write detailed tops\n",
    "    if not sig.empty:\n",
    "        sig_up = sig.sort_values(\"log2FoldChange\", ascending=False).head(25)[[\"gene\",\"log2FoldChange\",\"padj\"]]\n",
    "        sig_dn = sig.sort_values(\"log2FoldChange\", ascending=True).head(25)[[\"gene\",\"log2FoldChange\",\"padj\"]]\n",
    "        sig_up.to_csv(tables_dir / f\"{glds}_top25_up_DEGs.csv\", index=False)\n",
    "        sig_dn.to_csv(tables_dir / f\"{glds}_top25_down_DEGs.csv\", index=False)\n",
    "    return {\"GLDS\": glds, \"Total_DEG_q<0.05\": int(len(sig)), \"Up\": int(up), \"Down\": int(down)}\n",
    "\n",
    "deg_rows = [r for g in [\"GLDS-98\",\"GLDS-99\",\"GLDS-104\"] if (r := _omics_deg_counts(g))]\n",
    "deg_df = pd.DataFrame(deg_rows)\n",
    "deg_df.to_csv(out_dir / \"omics_deg_counts.csv\", index=False)\n",
    "if not deg_df.empty:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    dd = deg_df.melt(id_vars=\"GLDS\", value_vars=[\"Up\",\"Down\"], var_name=\"Direction\", value_name=\"Count\")\n",
    "    sns.barplot(data=dd, x=\"GLDS\", y=\"Count\", hue=\"Direction\", errorbar=None)\n",
    "    plt.title(\"Significant DEGs by dataset (q<0.05)\"); plt.tight_layout()\n",
    "    plt.savefig(out_dir / \"fig_deg_counts.png\", dpi=200); plt.close()\n",
    "\n",
    "# =================== 9.7 KEY FINDINGS (MD + JSON + extra CSVs) ===================\n",
    "def _fmt(x, nd=3):\n",
    "    try:\n",
    "        return float(np.round(float(x), nd))\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "findings = {\"missions\": list(S.missions)}\n",
    "\n",
    "# Coverage/quality summaries\n",
    "if not coverage_df.empty:\n",
    "    findings[\"coverage\"] = {\n",
    "        \"avg_coverage_pct\": _fmt(coverage_df[\"Coverage_Pct\"].mean(), 2),\n",
    "        \"median_coverage_pct\": _fmt(coverage_df[\"Coverage_Pct\"].median(), 2),\n",
    "        \"worst_coverage_top10\": (coverage_df.sort_values(\"Coverage_Pct\")\n",
    "                                              .head(10)[[\"RR\",\"Variable\",\"Type\",\"Coverage_Pct\"]]\n",
    "                                              .to_dict(\"records\"))\n",
    "    }\n",
    "    coverage_df.sort_values(\"Coverage_Pct\").head(25).to_csv(tables_dir / \"worst_coverage_top25.csv\", index=False)\n",
    "\n",
    "if not quality_df.empty:\n",
    "    totals = quality_df[[\"Orig_Min\",\"Interp_Min\",\"Missing_Min\"]].sum()\n",
    "    tot = totals.sum() if totals.sum()>0 else 1\n",
    "    findings[\"quality\"] = {\n",
    "        \"orig_pct\": _fmt(100 * totals[\"Orig_Min\"] / tot, 2),\n",
    "        \"interp_pct\": _fmt(100 * totals[\"Interp_Min\"] / tot, 2),\n",
    "        \"missing_pct\": _fmt(100 * totals[\"Missing_Min\"] / tot, 2)\n",
    "    }\n",
    "\n",
    "# Seasonality\n",
    "if seas is not None and not seas.empty and \"Seasonality_Strength\" in seas.columns:\n",
    "    findings[\"seasonality\"] = {\n",
    "        \"n_series\": int(len(seas)),\n",
    "        \"median_strength\": _fmt(seas[\"Seasonality_Strength\"].median(), 3),\n",
    "        \"top5\": (seas.sort_values(\"Seasonality_Strength\", ascending=False)\n",
    "                      .head(5)[[\"Mission\",\"Type\",\"Variable\",\"Seasonality_Strength\"]]\n",
    "                      .to_dict(\"records\"))\n",
    "    }\n",
    "    top_seasonal.to_csv(tables_dir / \"seasonality_top15.csv\", index=False)\n",
    "\n",
    "# Granger\n",
    "if not gsum_piv.empty:\n",
    "    totals_by_dir = gsum.groupby(\"Direction\")[\"Edges\"].sum().to_dict()\n",
    "    findings[\"granger\"] = {\n",
    "        \"edges_total\": int(gsum[\"Edges\"].sum()),\n",
    "        \"edges_by_direction\": {k:int(v) for k,v in totals_by_dir.items()}\n",
    "    }\n",
    "    # top edges by strength if available\n",
    "    if not gdf.empty and \"MinusLog10_q\" in gdf.columns:\n",
    "        top_edges = (gdf[gdf.get(\"Significant\", False)==True]\n",
    "                       .sort_values(\"MinusLog10_q\", ascending=False)\n",
    "                       .head(20)[[\"Mission\",\"Source\",\"Target\",\"Lag\",\"qvalue\",\"MinusLog10_q\"]])\n",
    "        top_edges.to_csv(tables_dir / \"granger_top20_edges.csv\", index=False)\n",
    "        findings[\"granger\"][\"top_edge_example\"] = top_edges.head(1).to_dict(\"records\")\n",
    "\n",
    "# Forecast\n",
    "if not df_score.empty:\n",
    "    rmse_by_model = (df_score.groupby(\"ModelShort\")[\"RMSE\"].median().sort_values().to_dict())\n",
    "    findings[\"forecast\"] = {\n",
    "        \"median_rmse_by_model\": {k:_fmt(v,3) for k,v in rmse_by_model.items()}\n",
    "    }\n",
    "if not best.empty:\n",
    "    winners = (best.groupby(\"ModelShort\").size().sort_values(ascending=False).to_dict())\n",
    "    findings.setdefault(\"forecast\", {})[\"best_model_counts\"] = winners\n",
    "    best.sort_values([\"RR\",\"Variable\"]).to_csv(tables_dir / \"forecast_best_by_variable.csv\", index=False)\n",
    "\n",
    "# Anomaly rates\n",
    "if not anom_df.empty:\n",
    "    agg = (anom_df.groupby([\"RR\",\"Type\"])[\"Anomalies_per_day_Z3\"]\n",
    "                 .mean().rename(\"Anoms_per_day\").reset_index())\n",
    "    top_anoms = (anom_df.sort_values(\"Anomalies_per_day_Z3\", ascending=False)\n",
    "                       .head(20)[[\"RR\",\"Variable\",\"Type\",\"Anomalies_per_day_Z3\"]])\n",
    "    findings[\"anomalies\"] = {\n",
    "        \"avg_anoms_per_day_overall\": _fmt(anom_df[\"Anomalies_per_day_Z3\"].mean(), 3),\n",
    "        \"top10_rate_examples\": top_anoms.head(10).to_dict(\"records\")\n",
    "    }\n",
    "    top_anoms.to_csv(tables_dir / \"anomaly_rate_top20.csv\", index=False)\n",
    "\n",
    "# Omics\n",
    "if not deg_df.empty:\n",
    "    findings[\"omics\"] = {\n",
    "        \"dataset_counts\": deg_df.to_dict(\"records\")\n",
    "    }\n",
    "\n",
    "# Write JSON + Markdown summaries\n",
    "(key_md := out_dir / \"key_findings.md\").write_text(\n",
    "    \"# Key Findings (Section 9)\\n\"\n",
    "    f\"- Missions analyzed: {len(findings.get('missions', []))}\\n\"\n",
    "    + (\"\\n## Coverage\\n\"\n",
    "       f\"- Average coverage: {findings['coverage']['avg_coverage_pct']}%\\n\"\n",
    "       f\"- Median coverage: {findings['coverage']['median_coverage_pct']}%\\n\"\n",
    "       if 'coverage' in findings else \"\")\n",
    "    + (\"\\n## Quality mix\\n\"\n",
    "       f\"- Original: {findings['quality']['orig_pct']}% | \"\n",
    "       f\"Interpolated: {findings['quality']['interp_pct']}% | \"\n",
    "       f\"Missing: {findings['quality']['missing_pct']}%\\n\"\n",
    "       if 'quality' in findings else \"\")\n",
    "    + (\"\\n## Seasonality\\n\"\n",
    "       f\"- Series: {findings['seasonality']['n_series']}, \"\n",
    "       f\"median strength: {findings['seasonality']['median_strength']}\\n\"\n",
    "       if 'seasonality' in findings else \"\")\n",
    "    + (\"\\n## Granger causality\\n\"\n",
    "       f\"- Total significant edges: {findings['granger']['edges_total']}\\n\"\n",
    "       f\"- By direction: {findings['granger']['edges_by_direction']}\\n\"\n",
    "       if 'granger' in findings else \"\")\n",
    "    + (\"\\n## Forecasting\\n\"\n",
    "       f\"- Median RMSE by model: {findings['forecast'].get('median_rmse_by_model', {})}\\n\"\n",
    "       f\"- Best-model wins: {findings['forecast'].get('best_model_counts', {})}\\n\"\n",
    "       if 'forecast' in findings else \"\")\n",
    "    + (\"\\n## Anomalies\\n\"\n",
    "       f\"- Overall average Z>3 anomalies/day: {findings['anomalies']['avg_anoms_per_day_overall']}\\n\"\n",
    "       if 'anomalies' in findings else \"\")\n",
    "    + (\"\\n## Omics\\n\"\n",
    "       f\"- DEG counts (q<0.05): {findings['omics']['dataset_counts']}\\n\"\n",
    "       if 'omics' in findings else \"\")\n",
    ")\n",
    "with open(out_dir / \"key_findings.json\", \"w\") as f:\n",
    "    json.dump(findings, f, indent=2, default=lambda x: float(x) if isinstance(x, (np.floating,)) else x)\n",
    "\n",
    "# Also print a short console highlight:\n",
    "print(\"\\n[Section 9] Key highlights\")\n",
    "if 'coverage' in findings:\n",
    "    print(\" - Avg coverage:\", findings['coverage']['avg_coverage_pct'], \"%\")\n",
    "if 'forecast' in findings and 'median_rmse_by_model' in findings['forecast']:\n",
    "    print(\" - Median RMSE by model:\", findings['forecast']['median_rmse_by_model'])\n",
    "if 'granger' in findings:\n",
    "    print(\" - Granger edges (total):\", findings['granger']['edges_total'])\n",
    "if 'anomalies' in findings:\n",
    "    print(\" - Avg anomalies/day (Z>3):\", findings['anomalies']['avg_anoms_per_day_overall'])\n",
    "if 'omics' in findings:\n",
    "    print(\" - Omics DEG datasets:\", findings['omics']['dataset_counts'])\n",
    "\n",
    "print(\"\\n[Section 9] Wrote summaries/figures →\", str(out_dir))\n",
    "print(\"[Section 9] Tables →\", str(tables_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f984e7",
   "metadata": {},
   "source": [
    "### Section 10: Publication Figures & Repro Plots (GLDS-98/99/104)\n",
    "#### Purpose\n",
    "Produce clean, reproducible figures for the paper/deck: anomaly timelines, anomaly calendars, seasonal overlays, model comparison panels, and omics volcano/PCA for GLDS-98/99/104.\n",
    "\n",
    "#### What is implemented\n",
    "1. Composite anomaly timeline per mission with Z>3 markers saved under `outputs/anomaly_forecast/<RR>/fig_composite_timeline.png`.\n",
    "2. Anomaly calendar heatmap (hour × date) per mission saved under `outputs/anomaly_forecast/<RR>/fig_anomaly_calendar.png`.\n",
    "3. Cross-model scoreboard bar chart (RMSE) across variables saved as `outputs/anomaly_forecast/fig_model_scoreboard.png`.\n",
    "4. Seasonal index overlay across missions for each variable saved under `outputs/pattern_analysis/fig_seasonal_overlay_<VAR>.png`.\n",
    "5. Omics volcano plots for GLDS-98, GLDS-99, GLDS-104 saved under `outputs/omics/fig_volcano_<GLDS>.png`.\n",
    "6. Optional GLDS PCA scatter (if a PCA table is available) saved under `outputs/omics/fig_pca_<GLDS>.png`.\n",
    "\n",
    "#### Future Work\n",
    "- SARIMAX residual diagnostics for the best (mission, variable) by RMSE: residual trace, ACF, PACF, Q–Q, Ljung–Box p-values saved under `outputs/anomaly_forecast/<RR>/fig_resid_diagnostics_<VAR>.png`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aaaf052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-14 17:28:23 UTC] [Sec10] Begin figure generation\n",
      "[2025-09-14 17:28:23 UTC] [Sec10] START timeline RR-1\n",
      "[2025-09-14 17:28:26 UTC] [Sec10] DONE timeline RR-1 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-1/fig_composite_timeline.png\n",
      "[2025-09-14 17:28:26 UTC] [Sec10] START calendar RR-1\n",
      "[2025-09-14 17:28:27 UTC] [Sec10] DONE calendar RR-1 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-1/fig_anomaly_calendar.png\n",
      "[2025-09-14 17:28:27 UTC] [Sec10] START timeline RR-3\n",
      "[2025-09-14 17:28:30 UTC] [Sec10] DONE timeline RR-3 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-3/fig_composite_timeline.png\n",
      "[2025-09-14 17:28:30 UTC] [Sec10] START calendar RR-3\n",
      "[2025-09-14 17:28:31 UTC] [Sec10] DONE calendar RR-3 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-3/fig_anomaly_calendar.png\n",
      "[2025-09-14 17:28:31 UTC] [Sec10] START timeline RR-6\n",
      "[2025-09-14 17:28:35 UTC] [Sec10] DONE timeline RR-6 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-6/fig_composite_timeline.png\n",
      "[2025-09-14 17:28:35 UTC] [Sec10] START calendar RR-6\n",
      "[2025-09-14 17:28:36 UTC] [Sec10] DONE calendar RR-6 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-6/fig_anomaly_calendar.png\n",
      "[2025-09-14 17:28:36 UTC] [Sec10] START timeline RR-9\n",
      "[2025-09-14 17:28:39 UTC] [Sec10] DONE timeline RR-9 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-9/fig_composite_timeline.png\n",
      "[2025-09-14 17:28:39 UTC] [Sec10] START calendar RR-9\n",
      "[2025-09-14 17:28:40 UTC] [Sec10] DONE calendar RR-9 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-9/fig_anomaly_calendar.png\n",
      "[2025-09-14 17:28:40 UTC] [Sec10] START timeline RR-12\n",
      "[2025-09-14 17:28:43 UTC] [Sec10] DONE timeline RR-12 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-12/fig_composite_timeline.png\n",
      "[2025-09-14 17:28:43 UTC] [Sec10] START calendar RR-12\n",
      "[2025-09-14 17:28:44 UTC] [Sec10] DONE calendar RR-12 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-12/fig_anomaly_calendar.png\n",
      "[2025-09-14 17:28:44 UTC] [Sec10] START timeline RR-19\n",
      "[2025-09-14 17:28:46 UTC] [Sec10] DONE timeline RR-19 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-19/fig_composite_timeline.png\n",
      "[2025-09-14 17:28:46 UTC] [Sec10] START calendar RR-19\n",
      "[2025-09-14 17:28:47 UTC] [Sec10] DONE calendar RR-19 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/RR-19/fig_anomaly_calendar.png\n",
      "[2025-09-14 17:28:47 UTC] [Sec10] START scoreboard\n",
      "[2025-09-14 17:28:47 UTC] [Sec10] DONE scoreboard -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/anomaly_forecast/fig_model_scoreboard.png\n",
      "[2025-09-14 17:28:48 UTC] [Sec10] START seasonal overlay Temp_degC_ISS (missions=6)\n",
      "[2025-09-14 17:28:48 UTC] [Sec10] DONE seasonal overlay Temp_degC_ISS -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/fig_seasonal_overlay_Temp_degC_ISS.png\n",
      "[2025-09-14 17:28:49 UTC] [Sec10] START seasonal overlay RH_percent_ISS (missions=6)\n",
      "[2025-09-14 17:28:49 UTC] [Sec10] DONE seasonal overlay RH_percent_ISS -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/fig_seasonal_overlay_RH_percent_ISS.png\n",
      "[2025-09-14 17:28:50 UTC] [Sec10] START seasonal overlay CO2_ppm_ISS (missions=6)\n",
      "[2025-09-14 17:28:51 UTC] [Sec10] DONE seasonal overlay CO2_ppm_ISS -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/fig_seasonal_overlay_CO2_ppm_ISS.png\n",
      "[2025-09-14 17:28:51 UTC] [Sec10] START seasonal overlay GCR_Dose_mGy_d (missions=6)\n",
      "[2025-09-14 17:28:52 UTC] [Sec10] DONE seasonal overlay GCR_Dose_mGy_d -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/fig_seasonal_overlay_GCR_Dose_mGy_d.png\n",
      "[2025-09-14 17:28:52 UTC] [Sec10] START seasonal overlay SAA_Dose_mGy_d (missions=6)\n",
      "[2025-09-14 17:28:53 UTC] [Sec10] DONE seasonal overlay SAA_Dose_mGy_d -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/fig_seasonal_overlay_SAA_Dose_mGy_d.png\n",
      "[2025-09-14 17:28:53 UTC] [Sec10] START seasonal overlay Total_Dose_mGy_d (missions=6)\n",
      "[2025-09-14 17:28:54 UTC] [Sec10] DONE seasonal overlay Total_Dose_mGy_d -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/fig_seasonal_overlay_Total_Dose_mGy_d.png\n",
      "[2025-09-14 17:28:54 UTC] [Sec10] START seasonal overlay Accumulated_Dose_mGy_d (missions=6)\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] DONE seasonal overlay Accumulated_Dose_mGy_d -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/pattern_analysis/fig_seasonal_overlay_Accumulated_Dose_mGy_d.png\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] START volcano GLDS-98\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] GLDS-98 DEG cols -> gene:True, l2fc:True, padj:True\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] DONE volcano GLDS-98 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/omics/fig_volcano_GLDS-98.png\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] START PCA GLDS-98\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] DONE PCA GLDS-98 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/omics/fig_pca_GLDS-98.png\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] START volcano GLDS-99\n",
      "[2025-09-14 17:28:55 UTC] [Sec10] GLDS-99 DEG cols -> gene:True, l2fc:True, padj:True\n",
      "[2025-09-14 17:28:56 UTC] [Sec10] DONE volcano GLDS-99 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/omics/fig_volcano_GLDS-99.png\n",
      "[2025-09-14 17:28:56 UTC] [Sec10] START PCA GLDS-99\n",
      "[2025-09-14 17:28:56 UTC] [Sec10] DONE PCA GLDS-99 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/omics/fig_pca_GLDS-99.png\n",
      "[2025-09-14 17:28:56 UTC] [Sec10] START volcano GLDS-104\n",
      "[2025-09-14 17:28:56 UTC] [Sec10] GLDS-104 DEG cols -> gene:True, l2fc:True, padj:True\n",
      "[2025-09-14 17:28:57 UTC] [Sec10] DONE volcano GLDS-104 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/omics/fig_volcano_GLDS-104.png\n",
      "[2025-09-14 17:28:57 UTC] [Sec10] START PCA GLDS-104\n",
      "[2025-09-14 17:28:57 UTC] [Sec10] DONE PCA GLDS-104 -> /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs/omics/fig_pca_GLDS-104.png\n",
      "[2025-09-14 17:28:57 UTC] [Sec10] All done → /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs\n",
      "[Section 10] Figures written to: /Users/kennethjenkins/Citizen Science/Data Mining for Space Habitats/outputs\n"
     ]
    }
   ],
   "source": [
    "# ==== Section 10: Publication Figures & Repro Plots (GLDS-98/99/104) ====\n",
    "\n",
    "import os, warnings\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# -------------------- Settings fallback --------------------\n",
    "if \"S\" not in globals():\n",
    "    @dataclass\n",
    "    class Settings:\n",
    "        missions: list[str] = field(default_factory=lambda: [\"RR-1\",\"RR-3\",\"RR-6\",\"RR-9\",\"RR-12\",\"RR-19\"])\n",
    "        sampling: str = \"1min\"\n",
    "        outputs_root: Path = field(default_factory=lambda: Path.cwd() / \"outputs\")\n",
    "        preprocessed_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"preprocessed\")\n",
    "        pattern_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"pattern_analysis\")\n",
    "        anomalies_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"anomaly_forecast\")\n",
    "        relationships_dir: Path = field(default_factory=lambda: Path.cwd() / \"outputs\" / \"relationships\")\n",
    "        omics_roots: list[Path] = field(default_factory=lambda: [Path(\"/Users/kennethjenkins/data/genelab\"),\n",
    "                                                                 Path.cwd() / \"outputs\" / \"omics\"])\n",
    "    S = Settings()\n",
    "    for d in [S.outputs_root, S.preprocessed_dir, S.pattern_dir, S.anomalies_dir, S.relationships_dir, *S.omics_roots]:\n",
    "        Path(d).mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[Sec10 bootstrap] Using fallback Settings (S).\")\n",
    "\n",
    "# -------------------- Config & logging --------------------\n",
    "DO_RESID_DIAG = False  # keep False (residual diagnostics are slow; enable later if needed)\n",
    "\n",
    "telemetry_cols = [\"Temp_degC_ISS\", \"RH_percent_ISS\", \"CO2_ppm_ISS\"]\n",
    "radiation_cols  = [\"GCR_Dose_mGy_d\", \"SAA_Dose_mGy_d\", \"Total_Dose_mGy_d\", \"Accumulated_Dose_mGy_d\"]\n",
    "all_vars = telemetry_cols + radiation_cols\n",
    "GLDS_LIST = [\"GLDS-98\", \"GLDS-99\", \"GLDS-104\"]\n",
    "\n",
    "ANOM_DIR    = S.anomalies_dir\n",
    "PAT_DIR     = S.pattern_dir\n",
    "PRE_DIR     = S.preprocessed_dir\n",
    "OMICS_DIRS  = getattr(S, \"omics_roots\", [S.outputs_root / \"omics\"])\n",
    "INSIGHTS_DIR= S.outputs_root / \"insights\"\n",
    "\n",
    "LOG_PATH = S.outputs_root / \"section10_progress.log\"\n",
    "def log(msg: str):\n",
    "    ts = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    line = f\"[{ts} UTC] {msg}\"\n",
    "    print(line, flush=True)\n",
    "    try:\n",
    "        with open(LOG_PATH, \"a\") as f:\n",
    "            f.write(line + \"\\n\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "def _ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _load_pre(rr: str, kind: str) -> pd.DataFrame | None:\n",
    "    p = PRE_DIR / f\"{rr}_cleaned_{kind}.csv\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            return pd.read_csv(p, index_col=0, parse_dates=True).sort_index()\n",
    "        except Exception:\n",
    "            return None\n",
    "    try:\n",
    "        if kind == \"telemetry\" and \"preprocessed_telemetry\" in globals() and rr in preprocessed_telemetry:\n",
    "            return preprocessed_telemetry[rr].copy()\n",
    "        if kind == \"radiation\" and \"cleaned_radiation\" in globals() and rr in cleaned_radiation:\n",
    "            return cleaned_radiation[rr].copy()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def _missions_with(kind_needed=(\"telemetry\",\"radiation\")) -> list[str]:\n",
    "    ms = []\n",
    "    for rr in S.missions:\n",
    "        ok = True\n",
    "        for k in kind_needed:\n",
    "            if (PRE_DIR / f\"{rr}_cleaned_{k}.csv\").exists():\n",
    "                continue\n",
    "            if not ((k==\"telemetry\" and \"preprocessed_telemetry\" in globals() and rr in preprocessed_telemetry) or\n",
    "                    (k==\"radiation\"  and \"cleaned_radiation\" in globals() and rr in cleaned_radiation)):\n",
    "                ok = False; break\n",
    "        if ok: ms.append(rr)\n",
    "    return ms\n",
    "\n",
    "def _z_anoms(s: pd.Series, zthr=3.0) -> pd.Series:\n",
    "    s = s.dropna()\n",
    "    if len(s) < 10 or s.std() == 0:\n",
    "        return pd.Series(index=s.index, dtype=bool)\n",
    "    z = (s - s.mean()) / s.std()\n",
    "    return z.abs() > zthr\n",
    "\n",
    "def _read_scoreboard() -> pd.DataFrame | None:\n",
    "    cand = list(ANOM_DIR.glob(\"*scoreboard*.csv\"))\n",
    "    if cand:\n",
    "        try:\n",
    "            return pd.read_csv(cand[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    parts = []\n",
    "    for rr in S.missions:\n",
    "        p = ANOM_DIR / f\"{rr}_forecast_summary.csv\"\n",
    "        if p.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(p)\n",
    "                if \"RR\" not in df.columns:\n",
    "                    df[\"RR\"] = rr\n",
    "                parts.append(df)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return pd.concat(parts, ignore_index=True) if parts else None\n",
    "\n",
    "def _minute_of_day_index(idx: pd.DatetimeIndex) -> pd.Index:\n",
    "    m = (idx.hour*60 + idx.minute).astype(int)\n",
    "    return pd.Index(m, name=\"minute_of_day\")\n",
    "\n",
    "def _load_seasonal(rr: str, var: str) -> pd.Series | None:\n",
    "    p = PAT_DIR / \"seasonal_indices\" / f\"{rr}_{var}_seasonal.csv\"\n",
    "    if not p.exists(): return None\n",
    "    try:\n",
    "        s = pd.read_csv(p, parse_dates=[\"timestamp\"]).set_index(\"timestamp\")[\"seasonal\"]\n",
    "        return s.asfreq(\"1min\").interpolate(limit=5).dropna()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _safe_read_csv(p: Path | str) -> pd.DataFrame | None:\n",
    "    if p is None: return None\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(p, engine=\"python\", encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "# ----- Omics file resolvers -----\n",
    "def _omics_candidates_deg(glds: str) -> list[str]:\n",
    "    # support both classic and GLbulkRNAseq / rRNArm variants\n",
    "    base = f\"{glds}_rna_seq_differential_expression\"\n",
    "    return [\n",
    "        f\"{base}.csv\",\n",
    "        f\"{base}_GLbulkRNAseq.csv\",\n",
    "        f\"{base}_rRNArm_GLbulkRNAseq.csv\",\n",
    "    ]\n",
    "\n",
    "def _find_omics_file(glds: str, fname: str) -> Path | None:\n",
    "    for root in OMICS_DIRS:\n",
    "        root = Path(root)\n",
    "        p1 = root / \"RR-1\" / glds / fname\n",
    "        p2 = root / fname\n",
    "        if p1.exists(): return p1\n",
    "        if p2.exists(): return p2\n",
    "    return None\n",
    "\n",
    "def _find_deg_any(glds: str) -> Path | None:\n",
    "    # 1) explicit GLDS_FILES mapping (if present)\n",
    "    if \"GLDS_FILES\" in globals():\n",
    "        p = GLDS_FILES.get(glds, {}).get(\"diff\")\n",
    "        if p and Path(p).exists(): return Path(p)\n",
    "    # 2) search OMICS_DIRS for any supported DEG filename\n",
    "    for name in _omics_candidates_deg(glds):\n",
    "        p = _find_omics_file(glds, name)\n",
    "        if p is not None:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _find_pca_table(glds: str) -> Path | None:\n",
    "    # Prefer Section 7 PCA output\n",
    "    p1 = INSIGHTS_DIR / f\"{glds}_pca_table.csv\"\n",
    "    if p1.exists(): return p1\n",
    "    # Accept a few generic names under omics roots (rare)\n",
    "    for cand in [f\"{glds}_rna_seq_visualization_PCA_table.csv\",\n",
    "                 \"rna_seq_visualization_PCA_table.csv\",\n",
    "                 \"visualization_PCA_table.csv\"]:\n",
    "        p = _find_omics_file(glds, cand)\n",
    "        if p is not None:\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def _standardize_deg_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize common GeneLab DEG headers to: gene, log2FoldChange, padj.\"\"\"\n",
    "    d = df.copy()\n",
    "    d.columns = (d.columns.astype(str)\n",
    "                 .str.strip()\n",
    "                 .str.replace(\"\\u200b\",\"\", regex=False)\n",
    "                 .str.replace(\"\\xa0\",\"\",  regex=False))\n",
    "\n",
    "    cols = list(d.columns)\n",
    "\n",
    "    def pick(cond):\n",
    "        for c in cols:\n",
    "            if cond(c): return c\n",
    "        return None\n",
    "\n",
    "    # gene column\n",
    "    gene_c = pick(lambda c: c.lower() in {\"gene\",\"genes\",\"symbol\",\"gene_symbol\",\"geneid\",\"gene_id\"} \n",
    "                           or \"symbol\" in c.lower())\n",
    "    # log2 fold-change (allow substrings like \"Log2fc_(Space Flight)v(Ground Control)\")\n",
    "    l2fc_c = pick(lambda c: (\"log2\" in c.lower() and (\"fc\" in c.lower() or \"fold\" in c.lower()))\n",
    "                           or any(k in c.lower() for k in [\"log2fc\",\"logfc\",\"lfc\"]))\n",
    "    # adjusted p-value (padj/FDR variants; allow substrings)\n",
    "    padj_c = pick(lambda c: \"padj\" in c.lower() or (\"adj\" in c.lower() and \"p\" in c.lower()) or \"fdr\" in c.lower())\n",
    "    # fallback nominal p-value if padj absent\n",
    "    pval_c = pick(lambda c: c.lower() in {\"pvalue\",\"p.value\",\"pval\"} or (\"p\" in c.lower() and \"value\" in c.lower()))\n",
    "\n",
    "    if gene_c and gene_c != \"gene\":                d = d.rename(columns={gene_c: \"gene\"})\n",
    "    if l2fc_c and l2fc_c != \"log2FoldChange\":      d = d.rename(columns={l2fc_c: \"log2FoldChange\"})\n",
    "    if padj_c and padj_c != \"padj\":                d = d.rename(columns={padj_c: \"padj\"})\n",
    "    if \"padj\" not in d.columns and pval_c:         d = d.rename(columns={pval_c: \"padj\"})\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "# -------------------- Figures --------------------\n",
    "def figure_composite_timeline(rr: str):\n",
    "    log(f\"[Sec10] START timeline {rr}\")\n",
    "    tel = _load_pre(rr, \"telemetry\"); rad = _load_pre(rr, \"radiation\")\n",
    "    if tel is None and rad is None:\n",
    "        log(f\"[Sec10] SKIP timeline {rr} (no data)\")\n",
    "        return\n",
    "    cols = [c for c in telemetry_cols if tel is not None and c in tel.columns] + \\\n",
    "           [c for c in radiation_cols  if rad is not None and c in rad.columns]\n",
    "    if not cols:\n",
    "        log(f\"[Sec10] SKIP timeline {rr} (no overlapping columns)\")\n",
    "        return\n",
    "    df = None\n",
    "    if tel is not None: df = tel[telemetry_cols].copy()\n",
    "    if rad is not None:\n",
    "        df = rad[radiation_cols] if df is None else df.join(rad[radiation_cols], how=\"outer\")\n",
    "    df = df[[c for c in cols]].sort_index().resample(\"1min\").mean().interpolate(limit=5)\n",
    "\n",
    "    n = len(cols); h = max(2.5, 1.5*n)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(14, h), sharex=True)\n",
    "    if n == 1: axes = [axes]\n",
    "    for ax, c in zip(axes, cols):\n",
    "        s = df[c].dropna(); mask = _z_anoms(s, 3.0)\n",
    "        ax.plot(s.index, s.values, lw=0.6, color=\"black\")\n",
    "        ax.scatter(s.index[mask], s[mask], s=6, color=\"red\")\n",
    "        ax.set_ylabel(c)\n",
    "    axes[0].set_title(f\"{rr} · Composite anomaly timeline (Z>3)\")\n",
    "    axes[-1].set_xlabel(\"Time (UTC)\")\n",
    "    out = ANOM_DIR / rr / \"fig_composite_timeline.png\"; _ensure_dir(out.parent)\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()\n",
    "    log(f\"[Sec10] DONE timeline {rr} -> {out}\")\n",
    "\n",
    "def figure_anomaly_calendar(rr: str):\n",
    "    log(f\"[Sec10] START calendar {rr}\")\n",
    "    tel = _load_pre(rr, \"telemetry\"); rad = _load_pre(rr, \"radiation\")\n",
    "    if tel is None and rad is None:\n",
    "        log(f\"[Sec10] SKIP calendar {rr} (no data)\")\n",
    "        return\n",
    "    cols = [c for c in telemetry_cols if tel is not None and c in tel.columns] + \\\n",
    "           [c for c in radiation_cols  if rad is not None and c in rad.columns]\n",
    "    if not cols:\n",
    "        log(f\"[Sec10] SKIP calendar {rr} (no overlapping columns)\")\n",
    "        return\n",
    "    df = None\n",
    "    if tel is not None: df = tel[telemetry_cols].copy()\n",
    "    if rad is not None:\n",
    "        df = rad[radiation_cols] if df is None else df.join(rad[radiation_cols], how=\"outer\")\n",
    "    df = df[[c for c in cols]].sort_index().resample(\"1min\").mean().interpolate(limit=5)\n",
    "\n",
    "    mask_any = pd.Series(False, index=df.index)\n",
    "    for c in cols:\n",
    "        mask_any = mask_any | _z_anoms(df[c], 3.0).reindex(df.index, fill_value=False)\n",
    "    if mask_any.sum() == 0:\n",
    "        log(f\"[Sec10] SKIP calendar {rr} (no anomalies)\")\n",
    "        return\n",
    "\n",
    "    dt = df.index[mask_any]\n",
    "    cal = pd.DataFrame({\"date\": dt.date, \"hour\": dt.hour})\n",
    "    tbl = cal.value_counts().rename(\"count\").reset_index()\n",
    "    piv = tbl.pivot(index=\"date\", columns=\"hour\", values=\"count\").fillna(0)\n",
    "\n",
    "    plt.figure(figsize=(14, max(3, 0.2*len(piv))))\n",
    "    sns.heatmap(piv, cmap=\"magma\", cbar_kws={\"label\":\"anomaly count\"})\n",
    "    plt.title(f\"{rr} · Anomaly calendar (Z>3, per hour)\")\n",
    "    plt.xlabel(\"Hour\"); plt.ylabel(\"Date\")\n",
    "    out = ANOM_DIR / rr / \"fig_anomaly_calendar.png\"; _ensure_dir(out.parent)\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()\n",
    "    log(f\"[Sec10] DONE calendar {rr} -> {out}\")\n",
    "\n",
    "def figure_scoreboard(score: pd.DataFrame):\n",
    "    if score is None or score.empty:\n",
    "        log(\"[Sec10] SKIP scoreboard (no rows)\")\n",
    "        return\n",
    "    log(\"[Sec10] START scoreboard\")\n",
    "    df = score.copy()\n",
    "    if \"Model\" not in df.columns:\n",
    "        df[\"Model\"] = \"LSTM\"\n",
    "    df = df.dropna(subset=[\"Variable\",\"RMSE\"])\n",
    "    df[\"ModelShort\"] = df[\"Model\"].astype(str).str.replace(r\"SARIMAX.*\",\"SARIMAX\", regex=True)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(data=df, x=\"Variable\", y=\"RMSE\", hue=\"ModelShort\", errorbar=None)\n",
    "    plt.title(\"Model comparison (RMSE)\"); plt.tight_layout()\n",
    "    out = ANOM_DIR / \"fig_model_scoreboard.png\"; _ensure_dir(out.parent)\n",
    "    plt.savefig(out, dpi=200); plt.close()\n",
    "    log(f\"[Sec10] DONE scoreboard -> {out}\")\n",
    "\n",
    "def figure_seasonal_overlay(var: str):\n",
    "    parts = []\n",
    "    for rr in S.missions:\n",
    "        s = _load_seasonal(rr, var)\n",
    "        if s is None or len(s) < 1440:\n",
    "            continue\n",
    "        mod = pd.Series(s.values, index=_minute_of_day_index(s.index))\n",
    "        grp = mod.groupby(level=0).median()\n",
    "        grp = grp - grp.mean()\n",
    "        parts.append((rr, grp))\n",
    "    if not parts:\n",
    "        log(f\"[Sec10] SKIP seasonal overlay {var} (no series)\")\n",
    "        return\n",
    "    log(f\"[Sec10] START seasonal overlay {var} (missions={len(parts)})\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for rr, grp in parts:\n",
    "        plt.plot(grp.index, grp.values, lw=1.2, label=rr, alpha=0.9)\n",
    "    plt.title(f\"Seasonal index overlay — {var}\")\n",
    "    plt.xlabel(\"Minute of day (0–1439)\"); plt.ylabel(\"Seasonal (zero-mean)\")\n",
    "    plt.legend(ncol=3, fontsize=8)\n",
    "    out = PAT_DIR / f\"fig_seasonal_overlay_{var}.png\"; _ensure_dir(out.parent)\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()\n",
    "    log(f\"[Sec10] DONE seasonal overlay {var} -> {out}\")\n",
    "\n",
    "def figure_omics_volcano(glds: str):\n",
    "    log(f\"[Sec10] START volcano {glds}\")\n",
    "    deg_path = _find_deg_any(glds)\n",
    "    if deg_path is None:\n",
    "        log(f\"[Sec10] SKIP volcano {glds} (no DEG file)\")\n",
    "        return\n",
    "\n",
    "    df = _safe_read_csv(deg_path)\n",
    "    if df is None or df.empty:\n",
    "        log(f\"[Sec10] SKIP volcano {glds} (DEG read failed)\")\n",
    "        return\n",
    "\n",
    "    df = _standardize_deg_cols(df)\n",
    "    log(f\"[Sec10] {glds} DEG cols -> gene:{'gene' in df.columns}, l2fc:{'log2FoldChange' in df.columns}, padj:{'padj' in df.columns}\")\n",
    "\n",
    "    if \"log2FoldChange\" not in df.columns:\n",
    "        log(f\"[Sec10] SKIP volcano {glds} (no log2FoldChange after std)\")\n",
    "        return\n",
    "\n",
    "    x = df[\"log2FoldChange\"].astype(float)\n",
    "    if \"padj\" in df.columns:\n",
    "        y = -np.log10(pd.to_numeric(df[\"padj\"], errors=\"coerce\").clip(lower=1e-300))\n",
    "        sig_mask = pd.to_numeric(df[\"padj\"], errors=\"coerce\") < 0.05\n",
    "        ylab = \"-log10(padj)\"\n",
    "    else:\n",
    "        # proxy if only fold-changes exist\n",
    "        y = np.log10(1 + x.abs())\n",
    "        sig_mask = y > y.median()\n",
    "        ylab = \"proxy intensity\"\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(x, y, s=10, alpha=0.6, c=np.where(sig_mask, \"crimson\", \"gray\"))\n",
    "    plt.axvline(0, color=\"black\", lw=0.7)\n",
    "    if ylab == \"-log10(padj)\":\n",
    "        plt.axhline(-np.log10(0.05), color=\"black\", lw=0.7, ls=\"--\")\n",
    "    plt.title(f\"{glds} · Volcano\"); plt.xlabel(\"log2 fold change\"); plt.ylabel(ylab)\n",
    "    out = S.outputs_root / \"omics\" / f\"fig_volcano_{glds}.png\"; _ensure_dir(out.parent)\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()\n",
    "    log(f\"[Sec10] DONE volcano {glds} -> {out}\")\n",
    "\n",
    "def _infer_group_from_sample_names(samples: pd.Series) -> pd.Series:\n",
    "    # heuristic: look for tokens like FLT/Flight vs GC/Control\n",
    "    def lab(s):\n",
    "        s = str(s).upper()\n",
    "        if \"FLT\" in s or \"FLIGHT\" in s: return \"FLT\"\n",
    "        if \"GC\" in s or \"CONTROL\" in s: return \"GC\"\n",
    "        return \"UNK\"\n",
    "    return samples.apply(lab)\n",
    "\n",
    "def _load_sample_table_for_grouping(glds: str) -> dict | None:\n",
    "    # Try GLDS_FILES mapping first\n",
    "    if \"GLDS_FILES\" in globals():\n",
    "        sp = GLDS_FILES.get(glds, {}).get(\"sample\")\n",
    "        if sp and Path(sp).exists():\n",
    "            st = _safe_read_csv(sp)\n",
    "            if st is not None:\n",
    "                # guess id and group columns\n",
    "                id_col = next((c for c in st.columns if c.lower() in {\"sample\",\"sampleid\",\"sample_id\",\"sample_name\",\"samplename\"}), None)\n",
    "                group_col = next((c for c in st.columns if c.lower() in {\"group\",\"condition\",\"treatment\"}), None)\n",
    "                if id_col and group_col:\n",
    "                    return st.set_index(id_col)[group_col].astype(str).to_dict()\n",
    "    # else none\n",
    "    return None\n",
    "\n",
    "def figure_omics_pca_if_available(glds: str):\n",
    "    p = _find_pca_table(glds)\n",
    "    if p is None:\n",
    "        log(f\"[Sec10] SKIP PCA {glds} (no PCA table)\")\n",
    "        return\n",
    "    log(f\"[Sec10] START PCA {glds}\")\n",
    "    pc = _safe_read_csv(p)\n",
    "    if pc is None or pc.empty:\n",
    "        log(f\"[Sec10] SKIP PCA {glds} (read failed)\")\n",
    "        return\n",
    "    # Flexible PC col detection\n",
    "    x = y = None\n",
    "    for cand in [(\"PC1\",\"PC2\"), (\"PC_1\",\"PC_2\"), (\"PC 1\",\"PC 2\"), (\"C1\",\"PC2\"), (\"Dim1\",\"Dim2\")]:\n",
    "        if cand[0] in pc.columns and cand[1] in pc.columns:\n",
    "            x, y = cand; break\n",
    "    if not (x and y):\n",
    "        log(f\"[Sec10] SKIP PCA {glds} (PC columns missing)\")\n",
    "        return\n",
    "\n",
    "    # Color by group, if available\n",
    "    color_col = None\n",
    "    for c in [\"Group\",\"Condition\",\"group\",\"condition\",\"SampleGroup\",\"Treatment\"]:\n",
    "        if c in pc.columns: color_col = c; break\n",
    "\n",
    "    if color_col is None:\n",
    "        # Try to infer from Sample names\n",
    "        sample_col = next((c for c in pc.columns if c.lower() in {\"sample\",\"sampleid\",\"sample_id\",\"sample_name\",\"samplename\"}), None)\n",
    "        if sample_col:\n",
    "            pc[\"__GroupInferred\"] = _infer_group_from_sample_names(pc[sample_col])\n",
    "            color_col = \"__GroupInferred\"\n",
    "        else:\n",
    "            # last resort: try mapping from sample table\n",
    "            sample_col = \"Sample\" if \"Sample\" in pc.columns else None\n",
    "            mapping = _load_sample_table_for_grouping(glds)\n",
    "            if sample_col and mapping:\n",
    "                pc[\"__GroupFromTable\"] = pc[sample_col].map(mapping).fillna(\"UNK\")\n",
    "                color_col = \"__GroupFromTable\"\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    if color_col:\n",
    "        for g, sub in pc.groupby(color_col):\n",
    "            plt.scatter(sub[x], sub[y], s=22, alpha=0.8, label=str(g))\n",
    "        plt.legend(title=color_col, fontsize=8)\n",
    "    else:\n",
    "        plt.scatter(pc[x], pc[y], s=22, alpha=0.8)\n",
    "    plt.title(f\"{glds} · PCA\"); plt.xlabel(x); plt.ylabel(y)\n",
    "    out = S.outputs_root / \"omics\" / f\"fig_pca_{glds}.png\"; _ensure_dir(out.parent)\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=200); plt.close()\n",
    "    log(f\"[Sec10] DONE PCA {glds} -> {out}\")\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "log(\"[Sec10] Begin figure generation\")\n",
    "\n",
    "missions = _missions_with((\"telemetry\",\"radiation\"))\n",
    "scoreboard = _read_scoreboard()\n",
    "\n",
    "for rr in missions:\n",
    "    try:\n",
    "        figure_composite_timeline(rr)\n",
    "        figure_anomaly_calendar(rr)\n",
    "        if DO_RESID_DIAG:\n",
    "            log(f\"[Sec10] Residual diagnostics are disabled (DO_RESID_DIAG=False).\")\n",
    "    except Exception as e:\n",
    "        log(f\"[Sec10] ERROR mission {rr}: {e}\")\n",
    "\n",
    "figure_scoreboard(scoreboard)\n",
    "\n",
    "for v in all_vars:\n",
    "    try:\n",
    "        figure_seasonal_overlay(v)\n",
    "    except Exception as e:\n",
    "        log(f\"[Sec10] ERROR seasonal overlay {v}: {e}\")\n",
    "\n",
    "for glds in GLDS_LIST:\n",
    "    try:\n",
    "        figure_omics_volcano(glds)\n",
    "        figure_omics_pca_if_available(glds)\n",
    "    except Exception as e:\n",
    "        log(f\"[Sec10] ERROR omics {glds}: {e}\")\n",
    "\n",
    "log(f\"[Sec10] All done → {S.outputs_root}\")\n",
    "print(\"[Section 10] Figures written to:\", str(S.outputs_root))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6be2e2",
   "metadata": {},
   "source": [
    "### Section 11: Streamlit Dashboard\n",
    "#### Purpose\n",
    "Provide a compact UI to browse anomalies, forecasts, seasonality, relationships, policies, and (for RR-1) omics summaries.\n",
    "#### What is implemented\n",
    "1. Mission/type/variable selection; artifact discovery from `outputs/*` with caching.\n",
    "2. Panels for anomaly/forecast images, STL/FFT/Welch plots, and relationship visuals (correlations, directed Granger).\n",
    "3. Policy viewer (JSON) and access to key CSV exports (e.g., scoreboard, summaries).\n",
    "4. RR-1 omics tables (DEG/PCA/anomaly timestamps).\n",
    "#### Future Work\n",
    "- Publish dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4204b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 10:41:49.352 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/envs/dlclass/lib/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-14 10:41:49.352 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.354 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.355 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.355 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.355 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.356 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.357 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.358 Session state does not function when running a script without `streamlit run`\n",
      "2025-09-14 10:41:49.358 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:49.656 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:50.064 No runtime found, using MemoryCacheStorageManager\n",
      "2025-09-14 10:41:51.695 No runtime found, using MemoryCacheStorageManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== Section 11: Streamlit Dashboard (app.py) ====\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Optional, Iterable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "# --------------------------- App setup ---------------------------\n",
    "st.set_page_config(page_title=\"Space Habitat Dashboard\", layout=\"wide\")\n",
    "st.title(\"Space Habitat • Anomalies • Forecasts • Policies • Omics\")\n",
    "\n",
    "# Root detection (run from project root)\n",
    "BASE = Path.cwd()\n",
    "OUT  = BASE / \"outputs\"\n",
    "\n",
    "ANOM_DIR    = OUT / \"anomaly_forecast\"\n",
    "PATTERN_DIR = OUT / \"pattern_analysis\"\n",
    "REL_DIR     = OUT / \"relationships\"\n",
    "INS_DIR     = OUT / \"insights\"\n",
    "POLICY_DIR  = OUT / \"policy\"\n",
    "PREP_DIR    = OUT / \"preprocessed\"\n",
    "SEC9_DIR    = OUT / \"section9_global_qa\"\n",
    "OMICS_DIR   = OUT / \"omics\"\n",
    "\n",
    "# --------------------------- Helpers & cache ---------------------------\n",
    "def _exists(p: Path) -> bool: return p.exists()\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def list_missions() -> list[str]:\n",
    "    if not _exists(ANOM_DIR): return []\n",
    "    return sorted([p.name for p in ANOM_DIR.iterdir() if p.is_dir()])\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def variables_for(rr: str) -> dict:\n",
    "    out = {\"telemetry\": [], \"radiation\": []}\n",
    "    root = ANOM_DIR / rr\n",
    "    if not _exists(root): return out\n",
    "    for p in root.iterdir():\n",
    "        if not p.is_dir(): continue\n",
    "        n = p.name\n",
    "        if n.startswith(\"telemetry_\"): out[\"telemetry\"].append(n.replace(\"telemetry_\",\"\"))\n",
    "        if n.startswith(\"radiation_\"):  out[\"radiation\"].append(n.replace(\"radiation_\",\"\"))\n",
    "    out[\"telemetry\"].sort(); out[\"radiation\"].sort()\n",
    "    return out\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_csv(path: Path, nrows: Optional[int] = None) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        return pd.read_csv(path, nrows=nrows)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_csv_dtindex(path: Path) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        return pd.read_csv(path, index_col=0, parse_dates=True).sort_index()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_text(path: Path, max_chars: int = 20000) -> Optional[str]:\n",
    "    try:\n",
    "        txt = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        return txt[:max_chars]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def human_size(nbytes: int) -> str:\n",
    "    if nbytes is None: return \"?\"\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]\n",
    "    if nbytes == 0: return \"0 B\"\n",
    "    i = int(math.floor(math.log(nbytes, 1024)))\n",
    "    return f\"{nbytes/1024**i:.1f} {units[i]}\"\n",
    "\n",
    "@st.cache_data(show_spinner=False)\n",
    "def list_files_recursive(root: Path, exts: Optional[Iterable[str]]=None) -> pd.DataFrame:\n",
    "    \"\"\"Return table of all files under root with size and mtime.\"\"\"\n",
    "    rows = []\n",
    "    if not _exists(root): return pd.DataFrame(columns=[\"relpath\",\"size\",\"mtime\",\"ext\",\"abs\"])\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            ext = p.suffix.lower()\n",
    "            if exts and ext not in exts: continue\n",
    "            try:\n",
    "                stat = p.stat()\n",
    "                rows.append({\n",
    "                    \"relpath\": str(p.relative_to(root)),\n",
    "                    \"size\": stat.st_size,\n",
    "                    \"mtime\": datetime.fromtimestamp(stat.st_mtime).isoformat(sep=\" \", timespec=\"seconds\"),\n",
    "                    \"ext\": ext,\n",
    "                    \"abs\": str(p.resolve())\n",
    "                })\n",
    "            except Exception:\n",
    "                continue\n",
    "    df = pd.DataFrame(rows)\n",
    "    if len(df): df = df.sort_values([\"ext\",\"relpath\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def safe_image(path: Path, caption: str, use_col=True):\n",
    "    if _exists(path):\n",
    "        st.image(str(path), caption=caption, use_column_width=use_col)\n",
    "\n",
    "def download_button_for(path: Path, label: Optional[str]=None):\n",
    "    if _exists(path):\n",
    "        label = label or f\"Download {path.name}\"\n",
    "        st.download_button(label, data=path.read_bytes(), file_name=path.name)\n",
    "\n",
    "# --------------------------- Sidebar ---------------------------\n",
    "with st.sidebar:\n",
    "    st.header(\"Controls\")\n",
    "    missions = list_missions()\n",
    "    if missions:\n",
    "        rr = st.selectbox(\"Mission\", missions, index=0)\n",
    "        kinds = variables_for(rr)\n",
    "        vtype = st.radio(\"Type\", [\"telemetry\",\"radiation\"], horizontal=True)\n",
    "        vlist = kinds.get(vtype, [])\n",
    "        var = st.selectbox(\"Variable\", vlist) if vlist else None\n",
    "    else:\n",
    "        rr = None; vtype = \"telemetry\"; var = None\n",
    "    st.caption(f\"Outputs root: {OUT.resolve()}\")\n",
    "\n",
    "# --------------------------- Tabs ---------------------------\n",
    "tabs = st.tabs([\n",
    "    \"Overview\",\n",
    "    \"Per-Mission (Anomaly/Forecast)\",\n",
    "    \"Pattern Analysis\",\n",
    "    \"Relationships\",\n",
    "    \"Policy & Compliance\",\n",
    "    \"Insights & Narratives\",\n",
    "    \"Global QA (Sec 9)\",\n",
    "    \"Omics (GLDS-98/99/104)\",\n",
    "    \"Explorer & Downloads\"\n",
    "])\n",
    "\n",
    "# ========================= Overview =========================\n",
    "with tabs[0]:\n",
    "    st.subheader(\"What’s available\")\n",
    "    cols = st.columns(3)\n",
    "    with cols[0]:\n",
    "        st.markdown(\"**Core directories**\")\n",
    "        for name, p in [\n",
    "            (\"anomaly_forecast\", ANOM_DIR),\n",
    "            (\"pattern_analysis\", PATTERN_DIR),\n",
    "            (\"relationships\", REL_DIR),\n",
    "            (\"insights\", INS_DIR),\n",
    "            (\"policy\", POLICY_DIR),\n",
    "            (\"preprocessed\", PREP_DIR),\n",
    "            (\"section9_global_qa\", SEC9_DIR),\n",
    "            (\"omics (figures)\", OMICS_DIR),\n",
    "        ]:\n",
    "            st.write(f\"- {name}: {'✅' if _exists(p) else '❌'}\")\n",
    "    with cols[1]:\n",
    "        st.markdown(\"**Key figures**\")\n",
    "        safe_image(ANOM_DIR / \"fig_model_scoreboard.png\", \"Model scoreboard (RMSE)\")\n",
    "        safe_image(PATTERN_DIR / \"fig_seasonal_overlay_CO2_ppm_ISS.png\", \"Seasonal overlay — CO₂\")\n",
    "    with cols[2]:\n",
    "        st.markdown(\"**Key findings (Sec 9)**\")\n",
    "        kjson = SEC9_DIR / \"key_findings.json\"\n",
    "        kmd   = SEC9_DIR / \"key_findings.md\"\n",
    "        if _exists(kjson):\n",
    "            st.json(json.loads(kjson.read_text()))\n",
    "            download_button_for(kjson)\n",
    "        if _exists(kmd):\n",
    "            st.markdown(\"---\")\n",
    "            st.markdown(load_text(kmd) or \"\")\n",
    "            download_button_for(kmd)\n",
    "\n",
    "# ============== Per-Mission (Anomaly/Forecast) ==============\n",
    "with tabs[1]:\n",
    "    st.subheader(\"Anomaly timelines, calendars, forecasts\")\n",
    "    if rr is None:\n",
    "        st.info(\"No missions discovered under outputs/anomaly_forecast.\")\n",
    "    else:\n",
    "        # Mission-level images\n",
    "        c1, c2 = st.columns(2)\n",
    "        with c1: safe_image(ANOM_DIR / rr / \"fig_composite_timeline.png\", f\"{rr} • Composite anomaly timeline (Z>3)\")\n",
    "        with c2: safe_image(ANOM_DIR / rr / \"fig_anomaly_calendar.png\", f\"{rr} • Anomaly calendar (Z>3)\")\n",
    "\n",
    "        # Variable-specific artifacts\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(f\"**Artifacts for** `{rr}` · `{vtype}:{var}`\")\n",
    "        if var:\n",
    "            col_dir = ANOM_DIR / rr / f\"{vtype}_{var}\"\n",
    "            if _exists(col_dir):\n",
    "                c1, c2 = st.columns(2)\n",
    "                with c1:\n",
    "                    safe_image(col_dir/\"anoms.png\", \"Z & EWMA anomalies\")\n",
    "                    safe_image(col_dir/\"ae_error.png\", \"LSTM AE reconstruction error\")\n",
    "                    if _exists(col_dir/\"anoms.csv\"):\n",
    "                        st.caption(\"Anomaly flags CSV\"); download_button_for(col_dir/\"anoms.csv\")\n",
    "                with c2:\n",
    "                    safe_image(col_dir/\"lstm_forecast.png\", \"LSTM forecast\")\n",
    "                    safe_image(col_dir/\"sarimax_forecast.png\", \"SARIMAX forecast\")\n",
    "                    if _exists(col_dir/\"forecast.csv\"):\n",
    "                        st.caption(\"Forecast CSV\"); download_button_for(col_dir/\"forecast.csv\")\n",
    "            else:\n",
    "                st.info(\"No artifacts for the selected series yet.\")\n",
    "        else:\n",
    "            st.caption(\"Pick a variable in the sidebar to see series-level artifacts.\")\n",
    "\n",
    "        # Per-mission summary table (if present)\n",
    "        st.markdown(\"---\")\n",
    "        summary_path = ANOM_DIR / f\"{rr}_forecast_summary.csv\"\n",
    "        if _exists(summary_path):\n",
    "            st.markdown(\"**Per-mission forecast summary**\")\n",
    "            df = load_csv(summary_path)\n",
    "            st.dataframe(df)\n",
    "            download_button_for(summary_path)\n",
    "\n",
    "# ======================= Pattern Analysis =======================\n",
    "with tabs[2]:\n",
    "    st.subheader(\"STL / FFT / Welch & seasonal overlays\")\n",
    "    if rr and var:\n",
    "        if vtype == \"telemetry\":\n",
    "            base = PATTERN_DIR\n",
    "        else:\n",
    "            base = PATTERN_DIR / \"radiation\"\n",
    "        for suffix, title in [(\"Raw\",\"Raw\"), (\"STL\",\"STL\"), (\"FFT\",\"FFT\"), (\"WELCH\",\"Welch PSD\")]:\n",
    "            safe_image(base / f\"{var}_{rr}_{suffix}.png\", f\"{rr} • {var} • {title}\")\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**Seasonality summary (Section 4)**\")\n",
    "    seas_path = PATTERN_DIR / \"seasonality_summary.csv\"\n",
    "    if _exists(seas_path):\n",
    "        seas = load_csv(seas_path)\n",
    "        st.dataframe(seas.head(50))\n",
    "        download_button_for(seas_path)\n",
    "\n",
    "    st.markdown(\"**Seasonal overlay figures (Section 10)**\")\n",
    "    overlay_cols = st.columns(3)\n",
    "    overlays = [\n",
    "        \"Temp_degC_ISS\",\"RH_percent_ISS\",\"CO2_ppm_ISS\",\n",
    "        \"GCR_Dose_mGy_d\",\"SAA_Dose_mGy_d\",\"Total_Dose_mGy_d\",\"Accumulated_Dose_mGy_d\"\n",
    "    ]\n",
    "    for i, v in enumerate(overlays):\n",
    "        with overlay_cols[i % 3]:\n",
    "            safe_image(PATTERN_DIR / f\"fig_seasonal_overlay_{v}.png\", f\"Overlay — {v}\")\n",
    "\n",
    "# ========================= Relationships =========================\n",
    "with tabs[3]:\n",
    "    st.subheader(\"Correlations & Granger causality\")\n",
    "    if rr:\n",
    "        c1, c2 = st.columns(2)\n",
    "        with c1:\n",
    "            safe_image(REL_DIR / \"plots\" / f\"{rr}_pearson.png\", f\"{rr} • Pearson\")\n",
    "            safe_image(REL_DIR / \"plots\" / f\"{rr}_spearman.png\", f\"{rr} • Spearman\")\n",
    "        with c2:\n",
    "            safe_image(REL_DIR / \"plots\" / f\"{rr}_granger_directed.png\", f\"{rr} • Directed Granger (−log10 q)\")\n",
    "            safe_image(REL_DIR / \"plots\" / f\"{rr}_network.png\", f\"{rr} • Correlation network\")\n",
    "        # CSVs\n",
    "        for fname in [f\"{rr}_pearson_corr.csv\", f\"{rr}_spearman_corr.csv\", f\"{rr}_partial_corr.csv\"]:\n",
    "            p = REL_DIR / fname\n",
    "            if _exists(p):\n",
    "                st.caption(fname); download_button_for(p)\n",
    "    # Combined tables\n",
    "    st.markdown(\"---\")\n",
    "    for fname in [\"granger_var_results.csv\", \"correlations_tidy.csv\"]:\n",
    "        p = REL_DIR / fname\n",
    "        if _exists(p):\n",
    "            st.markdown(f\"**{fname}**\")\n",
    "            st.dataframe(load_csv(p).head(200))\n",
    "            download_button_for(p)\n",
    "\n",
    "# ===================== Policy & Compliance =====================\n",
    "with tabs[4]:\n",
    "    st.subheader(\"Prescriptive policy & compliance (Section 8)\")\n",
    "    # Policy JSONs\n",
    "    pol_cands = [POLICY_DIR / \"prescriptive_policy_calibrated.json\", POLICY_DIR / \"prescriptive_policy.json\"]\n",
    "    pol = None\n",
    "    for p in pol_cands:\n",
    "        if _exists(p):\n",
    "            pol = json.loads(p.read_text())\n",
    "            st.markdown(f\"**Loaded policy:** `{p.name}`\"); download_button_for(p)\n",
    "            st.json(pol.get(\"meta\", {}))\n",
    "            break\n",
    "    if pol and rr in pol.get(\"missions\", {}):\n",
    "        st.markdown(f\"**Thresholds — {rr}**\")\n",
    "        st.json(pol[\"missions\"][rr].get(\"thresholds\", {}))\n",
    "        st.markdown(\"**Radiation guidance**\")\n",
    "        st.json(pol[\"missions\"][rr].get(\"radiation\", {}))\n",
    "        st.markdown(\"**Scheduling preferences**\")\n",
    "        st.json(pol[\"missions\"][rr].get(\"scheduling\", {}))\n",
    "    # Compliance\n",
    "    comp_summ = POLICY_DIR / \"compliance_summary.csv\"\n",
    "    if _exists(comp_summ):\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"**Compliance summary (all missions)**\")\n",
    "        st.dataframe(load_csv(comp_summ))\n",
    "        download_button_for(comp_summ)\n",
    "    if rr:\n",
    "        comp_path = POLICY_DIR / \"compliance\" / f\"{rr}_compliance.csv\"\n",
    "        if _exists(comp_path):\n",
    "            st.markdown(f\"**Compliance series — {rr}**\")\n",
    "            df = load_csv(comp_path, nrows=100000)\n",
    "            st.dataframe(df.head(5000))  # show a slice to keep UI snappy\n",
    "            download_button_for(comp_path)\n",
    "\n",
    "# ===================== Insights & Narratives =====================\n",
    "with tabs[5]:\n",
    "    st.subheader(\"Cross-mission insights (Section 7)\")\n",
    "    # Core CSVs\n",
    "    for fname in [\n",
    "        \"all_missions_metrics_raw.csv\",\n",
    "        \"volatility_by_variable.csv\",\n",
    "        \"anomaly_calendar_all_missions.csv\",\n",
    "        \"mission_anomaly_rates.csv\",\n",
    "        \"group_comparison_metrics.csv\",\n",
    "        \"top_forecastable_by_mission.csv\",\n",
    "        \"top_anomalous_by_mission.csv\",\n",
    "        \"top_seasonality_by_mission.csv\",\n",
    "        \"omics_datasets_summary.csv\",\n",
    "    ]:\n",
    "        p = INS_DIR / fname\n",
    "        if _exists(p):\n",
    "            st.markdown(f\"**{fname}**\")\n",
    "            st.dataframe(load_csv(p).head(1000))\n",
    "            download_button_for(p)\n",
    "\n",
    "    # Narratives\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**Mission narratives**\")\n",
    "    if rr and _exists(INS_DIR / f\"{rr}_narrative.md\"):\n",
    "        st.markdown(load_text(INS_DIR / f\"{rr}_narrative.md\") or \"\")\n",
    "        download_button_for(INS_DIR / f\"{rr}_narrative.md\")\n",
    "    # Plots\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**Insight plots**\")\n",
    "    safe_image(INS_DIR / \"plots\" / \"anomaly_intensity_by_type.png\", \"Anomaly intensity by type (AE)\")\n",
    "    safe_image(INS_DIR / \"plots\" / \"top_forecastable_by_mission.png\", \"Top forecastable by mission\")\n",
    "\n",
    "# ======================= Global QA (Section 9) =======================\n",
    "with tabs[6]:\n",
    "    st.subheader(\"Global QA & summary (Section 9)\")\n",
    "    # Main tables/figures\n",
    "    for fname in [\n",
    "        \"coverage_summary.csv\",\"quality_counts.csv\",\"anomaly_rate_per_day.csv\",\n",
    "        \"forecast_best_by_variable.csv\",\"granger_edge_counts.csv\",\"seasonality_top15.csv\",\n",
    "    ]:\n",
    "        p = SEC9_DIR / fname\n",
    "        if _exists(p):\n",
    "            st.markdown(f\"**{fname}**\")\n",
    "            st.dataframe(load_csv(p).head(2000))\n",
    "            download_button_for(p)\n",
    "    # Figures\n",
    "    fig_names = [\n",
    "        \"fig_coverage_heatmap.png\",\"fig_rmse_by_model.png\",\"fig_anomaly_rate_by_mission.png\",\n",
    "        \"fig_granger_edges.png\",\"fig_seasonality_strength_hist.png\"\n",
    "    ]\n",
    "    grid = st.columns(3)\n",
    "    for i, fname in enumerate(fig_names):\n",
    "        with grid[i % 3]:\n",
    "            safe_image(SEC9_DIR / fname, fname.replace(\"_\",\" \"))\n",
    "    # Per-mission quality figs\n",
    "    st.markdown(\"---\")\n",
    "    st.markdown(\"**Per-mission quality composition**\")\n",
    "    if rr:\n",
    "        safe_image(SEC9_DIR / f\"fig_quality_{rr}.png\", f\"{rr} • quality composition\")\n",
    "    # Section 9 tables subdir\n",
    "    tbl_dir = SEC9_DIR / \"tables\"\n",
    "    if _exists(tbl_dir):\n",
    "        st.markdown(\"---\")\n",
    "        st.markdown(\"**Tables (top-k digests)**\")\n",
    "        files = list_files_recursive(tbl_dir, exts={\".csv\"})\n",
    "        if len(files):\n",
    "            st.dataframe(files[[\"relpath\",\"size\",\"mtime\"]])\n",
    "            for _, row in files.iterrows():\n",
    "                p = tbl_dir / row[\"relpath\"]\n",
    "                with st.expander(row[\"relpath\"]):\n",
    "                    st.dataframe(load_csv(p))\n",
    "                    download_button_for(p)\n",
    "\n",
    "# =================== Omics (GLDS-98/99/104) ===================\n",
    "with tabs[7]:\n",
    "    st.subheader(\"Omics summaries & figures\")\n",
    "    # Section 7 top genes & PCA tables\n",
    "    for glds in [\"GLDS-98\",\"GLDS-99\",\"GLDS-104\"]:\n",
    "        st.markdown(f\"### {glds}\")\n",
    "        # Top up/down from insights\n",
    "        up   = INS_DIR / f\"{glds}_top_upregulated.csv\"\n",
    "        down = INS_DIR / f\"{glds}_top_downregulated.csv\"\n",
    "        pca  = INS_DIR / f\"{glds}_pca_table.csv\"\n",
    "        if _exists(up):\n",
    "            st.caption(\"Top upregulated\"); st.dataframe(load_csv(up)); download_button_for(up)\n",
    "        if _exists(down):\n",
    "            st.caption(\"Top downregulated\"); st.dataframe(load_csv(down)); download_button_for(down)\n",
    "        if _exists(pca):\n",
    "            st.caption(\"PCA 2D table\"); st.dataframe(load_csv(pca).head(200)); download_button_for(pca)\n",
    "        # Section 10 figures\n",
    "        safe_image(OMICS_DIR / f\"fig_volcano_{glds}.png\", f\"{glds} • volcano\")\n",
    "        safe_image(OMICS_DIR / f\"fig_pca_{glds}.png\",      f\"{glds} • PCA\")\n",
    "\n",
    "# =================== Explorer & Downloads ===================\n",
    "with tabs[8]:\n",
    "    st.subheader(\"Full artifact browser\")\n",
    "    colA, colB = st.columns([2,1])\n",
    "    with colA:\n",
    "        root_choice = st.selectbox(\"Pick a root directory to browse\", [\n",
    "            str(OUT), str(ANOM_DIR), str(PATTERN_DIR), str(REL_DIR),\n",
    "            str(INS_DIR), str(POLICY_DIR), str(PREP_DIR), str(SEC9_DIR), str(OMICS_DIR)\n",
    "        ], index=0)\n",
    "        root = Path(root_choice)\n",
    "        ext_filter = st.multiselect(\n",
    "            \"Filter by extension\",\n",
    "            [\".csv\",\".png\",\".json\",\".md\",\".log\"],\n",
    "            default=[\".csv\",\".png\",\".json\",\".md\"]\n",
    "        )\n",
    "        files = list_files_recursive(root, exts=set(ext_filter) if ext_filter else None)\n",
    "        q = st.text_input(\"Search filename contains\", \"\")\n",
    "        if q:\n",
    "            files = files[files[\"relpath\"].str.contains(q, case=False, na=False)]\n",
    "        if len(files):\n",
    "            show = files.copy()\n",
    "            show[\"size\"] = show[\"size\"].apply(human_size)\n",
    "            st.dataframe(show[[\"relpath\",\"ext\",\"size\",\"mtime\"]], use_container_width=True, height=400)\n",
    "        else:\n",
    "            st.info(\"No files found with current filters.\")\n",
    "    with colB:\n",
    "        st.markdown(\"**Preview / Download**\")\n",
    "        if len(files):\n",
    "            pick = st.selectbox(\"Choose a file\", files[\"relpath\"].tolist())\n",
    "            fpath = root / pick\n",
    "            st.caption(pick)\n",
    "            if fpath.suffix.lower() in {\".csv\"}:\n",
    "                n = st.slider(\"Rows to preview\", 5, 2000, 50, 5)\n",
    "                df = load_csv(fpath, nrows=n)\n",
    "                if df is not None: st.dataframe(df)\n",
    "            elif fpath.suffix.lower() in {\".json\",\".md\",\".log\"}:\n",
    "                st.code(load_text(fpath) or \"\", language=\"markdown\" if fpath.suffix.lower()==\".md\" else \"json\")\n",
    "            elif fpath.suffix.lower() in {\".png\",\".jpg\",\".jpeg\"}:\n",
    "                safe_image(fpath, fpath.name)\n",
    "            download_button_for(fpath)\n",
    "\n",
    "st.caption(\"Data Mining for Space Habitats — all artifacts are preliminary pending SARIMAX benchmarking & label-based calibration.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc587a7",
   "metadata": {},
   "source": [
    "# In Terminal only \n",
    "# create & activate a venv\n",
    "python -m venv .venv\n",
    "# macOS/Linux:\n",
    "source .venv/bin/activate\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# launch the dashboard\n",
    "streamlit run app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7cbf3-26dd-4bb5-b8e0-74c933e9d18f",
   "metadata": {},
   "source": [
    "### Section 11: Evaluation and Insights\n",
    "\n",
    "#### 11.1 Forecasting and Anomaly Detection Performance\n",
    "\n",
    "**LSTM Forecasting Performance**\n",
    "\n",
    "**Key Observations:**\n",
    "\n",
    "**Anomaly Detection Effectiveness**\n",
    "\n",
    "#### 11.2 Biological Linkage via Omics Data (RR-1)\n",
    "\n",
    "**Findings:**\n",
    "\n",
    "#### 11.3 Pattern and Relationship Insights\n",
    "\n",
    "**STL and FFT Observations:**\n",
    "\n",
    "**Correlation and Causality Analysis:**\n",
    "\n",
    "#### 11.4 Dashboard Usability\n",
    "\n",
    "#### 11.5 Key Findings and Future Applications\n",
    "\n",
    "**Future Work Recommendations:**\n",
    "\n",
    "### 11.6 Descriptive Statistical Summary\n",
    "\n",
    "#### Telemetry Variables (RR-1)\n",
    "\n",
    "#### Radiation Variables (RR-1)\n",
    "\n",
    "\n",
    "### 11.7 Prescriptive Statistical Insights\n",
    "\n",
    "#### Environmental Monitoring Recommendations\n",
    "\n",
    "#### Radiation-Aware Activity Scheduling\n",
    "\n",
    "#### Sampling and Health Monitoring Triggers\n",
    "\n",
    "#### Onboard Predictive Infrastructure\n",
    "\n",
    "#### Habitat Design and Sensor Deployment\n",
    "\n",
    "#### Recommendations for Artemis and Future Missions\n",
    "\n",
    "---\n",
    "\n",
    "### Section 12: Conclusion and Lessons Learned\n",
    "\n",
    "#### Summary of Insights\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "#### Project Challenges and Solutions\n",
    "\n",
    "#### Recommendations and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea80afc",
   "metadata": {},
   "source": [
    "# End of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3604f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlclass)",
   "language": "python",
   "name": "dlclass"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0105d7c8abe44456b948142e8623c03b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "05412d96764c43a5b050868121839d39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "055f81b493fe466c95ec2f41e4bd7f67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "056fe106e4f2478e9ff3672f4db5765b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "0900a3226ed44e0093aaff9843b84c23": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0a715919607e4b4cb39477c58b5d8489": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0af197ab626e40de88b47900ac5670a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f7c532e2c0b741ef99375798df72dd4f",
       "style": "IPY_MODEL_91f56b39007045be8318b12b6d79aeb4",
       "value": " 3/3 [11:06&lt;00:00, 222.07s/it]"
      }
     },
     "0cb650448d8544318f421b01d80d9a8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_1c5715b5fed84df18157d755d4f73873",
       "max": 3,
       "style": "IPY_MODEL_97c39a9719bf4943a62a66aa03ab1183",
       "value": 3
      }
     },
     "0ccb3a8c92eb45f5979fab8b5a65f788": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0d56f9b2cc964992bbf3b119b48930d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_15b17fdaeef3409ca6e90d0cd320f80e",
       "max": 4,
       "style": "IPY_MODEL_e6b2418a51934977a8082ed20861a45c",
       "value": 4
      }
     },
     "10acd205ec454c31a841f7cfd2d755b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1c3c01b207534ff8b9f7569005fb84e5",
       "style": "IPY_MODEL_48043d86d37847a6bf216f2a3aa6f45b",
       "value": "RR-12 Telemetry: 100%"
      }
     },
     "11248b09fd9d4c1a85f3d5a989ac1ea5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f6c7f24f409245b0aef62cad42e1d591",
       "style": "IPY_MODEL_cbda992a714f449db286acee26f072a9",
       "value": "RR-3 Telemetry: 100%"
      }
     },
     "12b03d4116674a6fbf8a79e1af5bbb00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_723325f882b946e0b1955b4091b4cd96",
       "style": "IPY_MODEL_7a823eb46e824b5793566d700e2f5d93",
       "value": "RR-19 Telemetry: 100%"
      }
     },
     "1371f2e616ce4e31bb24db807f5beb04": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "13cf77d916ce4e8d889dadc9f2f9a69e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "14bb949239804689aa1d0f8ddb5154da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_6d02e1c759944b48aff854cf9b8d57fd",
       "style": "IPY_MODEL_ff914fb81c4e4dc69f38c7773f1af6eb",
       "value": " 4/4 [13:00&lt;00:00, 195.25s/it]"
      }
     },
     "15b17fdaeef3409ca6e90d0cd320f80e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1786ef92417144f095c748162f894f4b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "17bea4318e7d439c82b5c3e213a1d63b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "186adaf2d0da4d1eab9424c121d67133": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "1abc260f1e6b46dbabfdba5200ef666b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_cb312b7eb19a4b308870424323b75d40",
       "max": 4,
       "style": "IPY_MODEL_b00e10213e8b409b823007b730523d16",
       "value": 4
      }
     },
     "1c3c01b207534ff8b9f7569005fb84e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1c5715b5fed84df18157d755d4f73873": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1cad01a5ece04e1d87f58570779d0f54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "1cc53e06de7e4b71a63e4571580c47c1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1ffd29006378488ea8536f98f556f673": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2052a67a5ca341cbba670dfcb3245901": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "23cf2f5c10f14be08fe176ff703bba7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1ffd29006378488ea8536f98f556f673",
       "style": "IPY_MODEL_1371f2e616ce4e31bb24db807f5beb04",
       "value": "RR-1 Telemetry: 100%"
      }
     },
     "2e8b82397f83496b987a6a844a810190": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30e8f08a98774c1ca50c51c47e0bc174": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "312bebc091274fcbb8b4502b83bd46f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "35f2c27af36d4244ba0fce500e67f706": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "381de98e1d76401bab6fce89c854634a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b5590886c8ba46f1b01e4e7beb1c6e9c",
       "style": "IPY_MODEL_0900a3226ed44e0093aaff9843b84c23",
       "value": " 4/4 [15:08&lt;00:00, 227.30s/it]"
      }
     },
     "39062a94f95c4ffeaecb849a39fc77d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "48043d86d37847a6bf216f2a3aa6f45b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "48a6cdd0136f45f7a0a386662d709369": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_58dd4a6bb5a04c07a10d730c5fb2803b",
       "max": 4,
       "style": "IPY_MODEL_69f44ca764334216a6af1a7c4837b98b",
       "value": 4
      }
     },
     "4a78f03a54a94f80944e688f552037c8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4b79b19cfcd447e98843fc9a9ed51fea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7a2f7decad5f4d3cb8013dacb941c246",
       "style": "IPY_MODEL_53e4ae9228ed4a4c8762cca2737ad945",
       "value": " 4/4 [17:22&lt;00:00, 260.49s/it]"
      }
     },
     "4cf86187267643c7bb0a35354a24a598": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d098cc28c5d4c1c9701646b266c8dce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "519de2b170c6484d81f08be8ce5ef658": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_53a09848271c45e9a924b5ce40838701",
       "style": "IPY_MODEL_b026cb411c014b409f5889d0dfdba75e",
       "value": " 3/3 [11:27&lt;00:00, 228.52s/it]"
      }
     },
     "53a09848271c45e9a924b5ce40838701": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53e4ae9228ed4a4c8762cca2737ad945": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "549dfeb0c78b47308f65aa4af133b999": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c50926b1af3c4e0792bac244d9f2ac99",
        "IPY_MODEL_5eeb43891fb94883ae11e7c3f2f9578d",
        "IPY_MODEL_9e6b9917a4c24b95930cf31969c2bf41"
       ],
       "layout": "IPY_MODEL_8f0057d60f1f47299a7e91353706a0c4"
      }
     },
     "56ae1130a1204d749a1dc97a51eee6d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_89d0f76767d3451282e00d7a1e119c38",
       "style": "IPY_MODEL_312bebc091274fcbb8b4502b83bd46f0",
       "value": " 3/3 [16:33&lt;00:00, 330.85s/it]"
      }
     },
     "58dd4a6bb5a04c07a10d730c5fb2803b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5bb46903844245beb7b810b6d6684918": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_ef4476c85b964a4b988976c7c8449b61",
       "max": 4,
       "style": "IPY_MODEL_ca686b0f9a90413b9b840a666175de4c",
       "value": 4
      }
     },
     "5eeb43891fb94883ae11e7c3f2f9578d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_a9044765612543a7a9908a174eaeb035",
       "max": 6,
       "style": "IPY_MODEL_056fe106e4f2478e9ff3672f4db5765b",
       "value": 6
      }
     },
     "6220b205807b4e9586b010162aad3c5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "62f7d4f971af4188a9b04eb5b8d773b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1cc53e06de7e4b71a63e4571580c47c1",
       "style": "IPY_MODEL_c99a2a3f459041479b20073e92abd094",
       "value": "RR-6 Radiation: 100%"
      }
     },
     "651d57e1c93943beaed8ef1d30932887": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4a78f03a54a94f80944e688f552037c8",
       "style": "IPY_MODEL_a1dd42b9310b43fc9850f23d2024866f",
       "value": " 3/3 [09:35&lt;00:00, 191.83s/it]"
      }
     },
     "66ed3bfb60684456a90d110d472a60a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "67e581514e594e9d8a0b4270bf401fab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b1b1797d002c43578c4ae160b4944c0f",
       "style": "IPY_MODEL_faf41b0cb1734fbea8fed4b49ac56ef6",
       "value": " 3/3 [09:31&lt;00:00, 190.32s/it]"
      }
     },
     "69f44ca764334216a6af1a7c4837b98b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6d02e1c759944b48aff854cf9b8d57fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6edc876eb207481b9039bb009cfae109": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9f893d1f90104ddf8c24d002be608673",
       "style": "IPY_MODEL_8d20bf4aaad64588bcbc3279cc4d5890",
       "value": "RR-12 Radiation: 100%"
      }
     },
     "723325f882b946e0b1955b4091b4cd96": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "72c8f80374b14b19bf20ac2afff676a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "75d2190d284741d7b1b523e272728f38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "790182fefe7846c68c03e03aff79aa6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2e8b82397f83496b987a6a844a810190",
       "style": "IPY_MODEL_9b2a80260afe4d5a83f1a26567a35e2f",
       "value": " 3/3 [13:33&lt;00:00, 271.15s/it]"
      }
     },
     "7a2f7decad5f4d3cb8013dacb941c246": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7a823eb46e824b5793566d700e2f5d93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7c3733078cc245819ee36287f0019c9c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "7e7734fff3114147af8e8f4ae047c1a2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "800e4a0fa81949f5accdad5285417efc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "874112f47b4847248a2e5e45944daa60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "89d0f76767d3451282e00d7a1e119c38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8d20bf4aaad64588bcbc3279cc4d5890": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f0057d60f1f47299a7e91353706a0c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8f749a27a4f94ec7ab2f6a2386ffa697": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0a715919607e4b4cb39477c58b5d8489",
       "style": "IPY_MODEL_fe2bdf91e6aa4886b9a42cfdaee8232e",
       "value": " 4/4 [12:18&lt;00:00, 185.02s/it]"
      }
     },
     "91f56b39007045be8318b12b6d79aeb4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9283198c21534365b59fd58bcb489eb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "97c39a9719bf4943a62a66aa03ab1183": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9a29522fe51d407580dae245d3657a17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b2a80260afe4d5a83f1a26567a35e2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b799086c969408591a9f6c603f85ccf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9c2b7bbeddf649a0908795a6b5a33a7e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_0105d7c8abe44456b948142e8623c03b",
       "max": 3,
       "style": "IPY_MODEL_13cf77d916ce4e8d889dadc9f2f9a69e",
       "value": 3
      }
     },
     "9c6f0ffea60649ddbdc1a1aaacf7582e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_05412d96764c43a5b050868121839d39",
       "max": 4,
       "style": "IPY_MODEL_b29cb6ffd4ab444b912b838a561843b0",
       "value": 4
      }
     },
     "9e6b9917a4c24b95930cf31969c2bf41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e125557750af4eb18b7687f98e6b3377",
       "style": "IPY_MODEL_9b799086c969408591a9f6c603f85ccf",
       "value": " 6/6 [2:34:36&lt;00:00, 1463.01s/it]"
      }
     },
     "9f893d1f90104ddf8c24d002be608673": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a0ba287f546242029f44ed728f811b92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a17c18f68f074242acb7440edd49cf34": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "a1dd42b9310b43fc9850f23d2024866f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a270c8559aa5484f971b22d4d8527d30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d523f7f36b31446ca98f7180eb90cb6a",
       "style": "IPY_MODEL_a0ba287f546242029f44ed728f811b92",
       "value": "RR-9 Radiation: 100%"
      }
     },
     "a2968afe73b54c3d8b2aeff7dcb4e8a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a4dccffe0f954425ad739be10a8f50b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a5cc695395e04ae38926b55668df74fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7e7734fff3114147af8e8f4ae047c1a2",
       "style": "IPY_MODEL_1786ef92417144f095c748162f894f4b",
       "value": "RR-1 Radiation: 100%"
      }
     },
     "a81ec47ab4274cbebf4d5e9ea6889036": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_72c8f80374b14b19bf20ac2afff676a2",
       "style": "IPY_MODEL_2052a67a5ca341cbba670dfcb3245901",
       "value": "RR-3 Radiation: 100%"
      }
     },
     "a9044765612543a7a9908a174eaeb035": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b00e10213e8b409b823007b730523d16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b026cb411c014b409f5889d0dfdba75e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b02cf6a7f0e34da09d57671a34467bff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "b1b1797d002c43578c4ae160b4944c0f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b29cb6ffd4ab444b912b838a561843b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b5590886c8ba46f1b01e4e7beb1c6e9c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b7c508af0f704e669e63371b87952157": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fa4675d6d72e4668a80be47e84eb9c40",
       "style": "IPY_MODEL_874112f47b4847248a2e5e45944daa60",
       "value": " 4/4 [11:12&lt;00:00, 168.03s/it]"
      }
     },
     "be54054d39d346c7aa0cf20ce4ec4ca8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bed2d10a52034ae9b7e240afe0e121be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c50926b1af3c4e0792bac244d9f2ac99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a4dccffe0f954425ad739be10a8f50b5",
       "style": "IPY_MODEL_bed2d10a52034ae9b7e240afe0e121be",
       "value": "Missions: 100%"
      }
     },
     "c59078b3158144da85605f63b9dd2f0d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "c5a04953716a47b8b3d9c13c0a43ceb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c8fb272d4f6c4c65a1c2ba4d3369a94a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_fee513a412534d40aa2ccb961acfb28c",
       "max": 3,
       "style": "IPY_MODEL_35f2c27af36d4244ba0fce500e67f706",
       "value": 3
      }
     },
     "c99a2a3f459041479b20073e92abd094": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ca686b0f9a90413b9b840a666175de4c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "cb312b7eb19a4b308870424323b75d40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cbda992a714f449db286acee26f072a9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cd77598119c64848afc9dffd2fec8744": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "ce1e8b04873141c3ab0aa2e64a9fb997": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4d098cc28c5d4c1c9701646b266c8dce",
       "style": "IPY_MODEL_c5a04953716a47b8b3d9c13c0a43ceb0",
       "value": " 4/4 [13:45&lt;00:00, 206.09s/it]"
      }
     },
     "cfbd4ddcec9c403bbcbf585a0642dad8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_4cf86187267643c7bb0a35354a24a598",
       "max": 3,
       "style": "IPY_MODEL_6220b205807b4e9586b010162aad3c5d",
       "value": 3
      }
     },
     "d523f7f36b31446ca98f7180eb90cb6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d68cdb32cd744d39b1a092881a733fba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_a2968afe73b54c3d8b2aeff7dcb4e8a6",
       "max": 3,
       "style": "IPY_MODEL_e55d937b10b14a538f9e1178e3286df8",
       "value": 3
      }
     },
     "d7bf50854c664df2a6dde153043df6b3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dc019df29bf14b3aa197492891dcd861": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "visibility": "hidden"
      }
     },
     "de0b4c58f18a4ac9a7b02ba6a5614250": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_be54054d39d346c7aa0cf20ce4ec4ca8",
       "max": 3,
       "style": "IPY_MODEL_f8b31d99dd9847f292e96a3a402cc884",
       "value": 3
      }
     },
     "e125557750af4eb18b7687f98e6b3377": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e3da2403031a47c7a31d3642072f8fb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_055f81b493fe466c95ec2f41e4bd7f67",
       "style": "IPY_MODEL_9a29522fe51d407580dae245d3657a17",
       "value": "RR-9 Telemetry: 100%"
      }
     },
     "e55d937b10b14a538f9e1178e3286df8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e5be7ceb10084d898810badc25aa5558": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_75d2190d284741d7b1b523e272728f38",
       "max": 4,
       "style": "IPY_MODEL_9283198c21534365b59fd58bcb489eb7",
       "value": 4
      }
     },
     "e6b2418a51934977a8082ed20861a45c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "eab526f00e2947ff987800b8436653e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ef4476c85b964a4b988976c7c8449b61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f2d56fc75a1e47dbb0d5b2364925fd6f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_d7bf50854c664df2a6dde153043df6b3",
       "style": "IPY_MODEL_30e8f08a98774c1ca50c51c47e0bc174",
       "value": "RR-19 Radiation: 100%"
      }
     },
     "f6c7f24f409245b0aef62cad42e1d591": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f7c532e2c0b741ef99375798df72dd4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f8415d35cb42422780ab171fb12d8878": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0ccb3a8c92eb45f5979fab8b5a65f788",
       "style": "IPY_MODEL_eab526f00e2947ff987800b8436653e6",
       "value": "RR-6 Telemetry: 100%"
      }
     },
     "f8b31d99dd9847f292e96a3a402cc884": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "fa4675d6d72e4668a80be47e84eb9c40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "faf41b0cb1734fbea8fed4b49ac56ef6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fe2bdf91e6aa4886b9a42cfdaee8232e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fee513a412534d40aa2ccb961acfb28c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ff914fb81c4e4dc69f38c7773f1af6eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
